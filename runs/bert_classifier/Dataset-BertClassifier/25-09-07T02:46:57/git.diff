diff --git a/TODO.md b/TODO.md
index e69de29..d2a04dd 100644
--- a/TODO.md
+++ b/TODO.md
@@ -0,0 +1,80 @@
+- [ ] CapPa
+- [ ] Classification Done Right for Vision-Language
+Pre-Training
+
+
+- [ ] CLIP
+- [ ] SigLip
+- [ ] MAE
+- [ ] DINOv2
+- [ ] BERT
+- [ ] ColBert
+- [ ] Mamba
+- [ ] SWIN
+- [ ] DeiT
+- [ ] CoAtNet
+- [ ] AIM v2
+
+- [ ] CV Modules
+  - [ ] convnext
+  - [ ] NFNet
+  - [ ] FastViT
+  - [ ] mobileone
+  - [ ] SE Net
+
+https://github.com/LeapLabTHU/MLLA/blob/master/models/mlla.py
+
+- [ ] Diff transformer
+- [ ] Sigmoid Attention
+
+- [ ] duckdb dataset
+- [ ] hugging face datasets support (streaming)
+
+
+
+
+- [ ] Attention Zoo
+  - [ ] Sigmoid Attentio
+  - [ ] Diff Attention
+  - [ ] TokenFormer
+  - [ ] GKA
+
+- [ ] SSM Zoo / Hybrids
+
+
+- [ ] VLM Zoo
+  - [ ] fast VLM
+  - LLAVA
+
+- [ ] ViT Zoo
+  - dense former
+  - sparse former
+  - ViTamin
+  - mixture of depth
+  - MOE
+  - PEFT
+    - https://github.com/GraphPKU/PiSSA
+- [ ] schedule free 
+- - [ ] shampoo https://x.com/cloneofsimo/status/1836003682141577418
+  - [ ] soap
+  - [ ] muon
+  - [ ] adeamix
+  - [ ] https://github.com/ClashLuke/HeavyBall
+  - https://github.com/iShohei220/adopt/tree/main
+- [ ] bits and bytes
+
+- [ ] liger kernels
+- [ ] triton
+- [ ] torchao
+- [ ] distributed training
+
+https://arxiv.org/abs/2404.0p5196
+
+
+- [ ] optuna support
+
+
+
+
+- [ ] uv
+- [ ] upgrade docs
\ No newline at end of file
diff --git a/examples/benchmark.py b/examples/benchmark.py
index b749a31..ec7f94e 100644
--- a/examples/benchmark.py
+++ b/examples/benchmark.py
@@ -1,10 +1,12 @@
+from itertools import repeat
+
 import timm
 import torch
-from itertools import repeat
-from torch.cuda.amp import autocast, GradScaler
+from torch.cuda.amp import GradScaler, autocast
+
 import yann
-from yann.callbacks import ProgressBar
 import yann.transforms
+from yann.callbacks import ProgressBar
 
 
 class Params(yann.params.HyperParams):
@@ -21,7 +23,6 @@ class Params(yann.params.HyperParams):
   pin_memory = False
   prefetch_factor = 2
 
-
   device = yann.default.device
   memory_format: str = 'contiguous_format'
   dtype = 'float32'
@@ -36,21 +37,20 @@ class Params(yann.params.HyperParams):
 
 
 def simple_train_loop(
-    model: torch.nn.Module,
-    loader,
-    loss,
-    optimizer,
-    device=None,
-    memory_format=None,
-    dtype=None,
-    non_blocking=False,
-    amp=False,
-    progress: ProgressBar = None
+  model: torch.nn.Module,
+  loader,
+  loss,
+  optimizer,
+  device=None,
+  memory_format=None,
+  dtype=None,
+  non_blocking=False,
+  amp=False,
+  progress: ProgressBar = None,
 ):
   model.train()
   progress.on_epoch_start()
   for i, (inputs, targets) in enumerate(loader):
-
     # print(i)
     if device or memory_format:
       inputs, targets = (
@@ -58,12 +58,9 @@ def simple_train_loop(
           device,
           memory_format=memory_format,
           dtype=dtype,
-          non_blocking=non_blocking
+          non_blocking=non_blocking,
         ),
-        targets.to(
-          device=device,
-          non_blocking=non_blocking
-        )
+        targets.to(device=device, non_blocking=non_blocking),
       )
 
     with autocast(enabled=amp):
@@ -78,7 +75,6 @@ def simple_train_loop(
   progress.on_epoch_end()
 
 
-
 def benchmark_train(params: Params):
   print(params)
 
@@ -92,7 +88,7 @@ def benchmark_train(params: Params):
   transform = yann.transforms.ImageTransformer(
     resize=params.size,
     crop=params.size,
-    color_space='RGB'
+    color_space='RGB',
   )
 
   dataset = yann.resolve.dataset(params.dataset, download=True)
@@ -118,7 +114,7 @@ def benchmark_train(params: Params):
     batch_size=params.batch_size,
     num_workers=params.num_workers,
     pin_memory=params.pin_memory,
-    prefetch_factor=params.prefetch_factor
+    prefetch_factor=params.prefetch_factor,
   )
 
   if params.skip_loader:
@@ -141,7 +137,7 @@ def benchmark_train(params: Params):
       memory_format=memory_format,
       non_blocking=params.non_blocking,
       amp=params.amp,
-      progress=ProgressBar(length=len(dataset))
+      progress=ProgressBar(length=len(dataset)),
     )
   else:
     train = yann.train.Trainer(
@@ -153,7 +149,7 @@ def benchmark_train(params: Params):
       # dtype=dtype,
       # memory_format=memory_format,
       amp=params.amp,
-      callbacks=[ProgressBar(length=len(dataset))]
+      callbacks=[ProgressBar(length=len(dataset))],
     )
     train(1)
 
@@ -161,4 +157,3 @@ def benchmark_train(params: Params):
 if __name__ == '__main__':
   params = Params.from_command()
   benchmark_train(params)
-
diff --git a/examples/explore_losses.ipynb b/examples/explore_losses.ipynb
index ee61bcd..f36bd4c 100644
--- a/examples/explore_losses.ipynb
+++ b/examples/explore_losses.ipynb
@@ -19,43 +19,46 @@
    "source": [
     "%matplotlib inline\n",
     "import torch\n",
+    "from matplotlib import pyplot as plt\n",
     "from torch.nn import functional as F\n",
-    "from yann.viz import plot_line\n",
+    "\n",
     "from yann.modules.loss import binary_focal_loss\n",
-    "from matplotlib import pyplot as plt\n",
+    "from yann.viz import plot_line\n",
     "\n",
     "logits = [x / 100 for x in range(-500, 500, 1)]\n",
     "\n",
     "\n",
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[1.0]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]),\n",
+    "      torch.Tensor([[1.0]]),\n",
+    "      reduction='none',\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f'binary cross entropy',\n",
+    "  show=False,\n",
+    "  title='Focal Loss',\n",
+    "  xlabel='probability of ground truth class',\n",
+    "  ylabel='loss',\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[1.0]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]),\n",
+    "        targets=torch.Tensor([[1.0]]),\n",
+    "        gamma=g,\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f'gamma = {g}',\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -79,34 +82,36 @@
    ],
    "source": [
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[0]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]),\n",
+    "      torch.Tensor([[0]]),\n",
+    "      reduction='none',\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f'binary cross entropy',\n",
+    "  show=False,\n",
+    "  title='Focal Loss',\n",
+    "  xlabel='probability of ground truth class',\n",
+    "  ylabel='loss',\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[0]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]),\n",
+    "        targets=torch.Tensor([[0]]),\n",
+    "        gamma=g,\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f'gamma = {g}',\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -130,34 +135,36 @@
    ],
    "source": [
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[.8]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss with smooth target (.8)',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]),\n",
+    "      torch.Tensor([[0.8]]),\n",
+    "      reduction='none',\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f'binary cross entropy',\n",
+    "  show=False,\n",
+    "  title='Focal Loss with smooth target (.8)',\n",
+    "  xlabel='probability of ground truth class',\n",
+    "  ylabel='loss',\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[.8]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]),\n",
+    "        targets=torch.Tensor([[0.8]]),\n",
+    "        gamma=g,\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f'gamma = {g}',\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -233,33 +240,35 @@
    ],
    "source": [
     "for g in [2]:\n",
-    " for alpha in [None, .1, .3, .5, .8, 1]:\n",
+    "  for alpha in [None, 0.1, 0.3, 0.5, 0.8, 1]:\n",
     "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[1]]),\n",
-    "            gamma=g,\n",
-    "            alpha=alpha\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma={g}, alpha={alpha}, target=1',\n",
-    "        show=False,\n",
+    "      y=[\n",
+    "        binary_focal_loss(\n",
+    "          torch.Tensor([[logit]]),\n",
+    "          torch.Tensor([[1]]),\n",
+    "          gamma=g,\n",
+    "          alpha=alpha,\n",
+    "        ).item()\n",
+    "        for logit in logits\n",
+    "      ],\n",
+    "      x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "      name=f'gamma={g}, alpha={alpha}, target=1',\n",
+    "      show=False,\n",
     "    )\n",
-    "    \n",
+    "\n",
     "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[0]]),\n",
-    "            gamma=g,\n",
-    "            alpha=alpha\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma={g}, alpha={alpha}, target=0',\n",
-    "        show=False,\n",
+    "      y=[\n",
+    "        binary_focal_loss(\n",
+    "          torch.Tensor([[logit]]),\n",
+    "          torch.Tensor([[0]]),\n",
+    "          gamma=g,\n",
+    "          alpha=alpha,\n",
+    "        ).item()\n",
+    "        for logit in logits\n",
+    "      ],\n",
+    "      x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "      name=f'gamma={g}, alpha={alpha}, target=0',\n",
+    "      show=False,\n",
     "    )\n",
     "\n",
     "    plt.grid()\n",
diff --git a/examples/train_mnist.py b/examples/train_mnist.py
index 26f6d94..1c67258 100644
--- a/examples/train_mnist.py
+++ b/examples/train_mnist.py
@@ -1,10 +1,13 @@
 import torch
 from torch import nn
 from torchvision import transforms
+from torchvision.datasets import MNIST
 
 import yann
-from yann.modules import Stack, Flatten, Infer
-from yann.params import HyperParams, Choice, Range
+from yann.callbacks import History
+from yann.callbacks.rich_progress import RichProgress
+from yann.modules import Flatten, Infer, Stack
+from yann.params import Choice, HyperParams, Range
 from yann.train import Trainer
 
 
@@ -13,66 +16,108 @@ class Params(HyperParams):
   batch_size = 32
   epochs = 10
   optimizer: Choice(('SGD', 'Adam')) = 'SGD'
-  learning_rate: Range(.01, .0001) = .01
+  learning_rate: Range(0.01, 0.0001) = 0.01
   momentum = 0
 
   seed = 1
 
-# parse command line arguments
-params = Params.from_command()
-params.validate()
 
-print(params)
-
-# set random, numpy and pytorch seeds in one call
-yann.seed(params.seed)
-
-
-lenet = Stack(
-  Infer(nn.Conv2d, 10, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
-
-  Infer(nn.Conv2d, 20, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
-
-  Flatten(),
-
-  Infer(nn.Linear, 50),
-  nn.ReLU(inplace=True),
-  Infer(nn.Linear, 10),
-  activation=nn.LogSoftmax(dim=1)
-)
-
-# run a forward pass to infer input shapes using `Infer` modules
-lenet(torch.rand(1, 1, 28, 28))
-
-# use the registry to resolve optimizer name to an optimizer class
-optimizer = yann.resolve.optimizer(
-  params.optimizer,
-  yann.trainable(lenet.parameters()),
-  momentum=params.momentum,
-  lr=params.learning_rate
-)
-
-train = Trainer(
-  model=lenet,
-  optimizer=optimizer,
-  dataset=params.dataset,
-  batch_size=params.batch_size,
-  transform=transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.1307,), (0.3081,))
-  ]),
-  loss='nll_loss',
-  metrics=('accuracy',)
-)
-
-train(params.epochs)
-
-# save checkpoint
-train.checkpoint()
-
-# plot the loss curve
-train.history.plot()
\ No newline at end of file
+#
+#
+# class Vit(nn.Module):
+#   def __init__(self):
+#     self.conv = nn.Sequential(
+#       nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
+#     )
+#
+#     self.transformers = nn.Sequential(
+#       nn.TransformerEncoder()
+#
+#     )
+#
+
+
+class BoundedLeakyReLU(nn.Module):
+  def __init__(self, negative_slope=0.01, peak=1):
+    super(BoundedLeakyReLU, self).__init__()
+    self.negative_slope = negative_slope
+    self.peak = peak
+
+  def forward(self, x):
+    # Create the mask for the different regions
+    positive_mask = (x > self.peak).float()  # x > 1 -> apply -x
+    relu_mask = (x > 0).float() * (x <= self.peak).float()  # 0 <= x <= 1 -> apply ReLU
+    leaky_relu_mask = (x <= 0).float()  # x <= 0 -> apply Leaky ReLU
+
+    # Compute the outputs for each region
+    negative_part = leaky_relu_mask * self.negative_slope * x
+    relu_part = relu_mask * x
+    inverted_part = positive_mask * (-x)
+
+    # Combine the outputs
+    return negative_part + relu_part + inverted_part
+
+
+if __name__ == '__main__':
+  # parse command line arguments
+  params = Params.from_command()
+  params.validate()
+
+  print(params)
+
+  # set random, numpy and pytorch seeds in one call
+  # yann.seed(params.seed)
+
+  Activation = BoundedLeakyReLU
+
+  lenet = Stack(
+    Infer(nn.Conv2d, 10, kernel_size=5),
+    nn.MaxPool2d(2),
+    Activation(),
+    Infer(nn.Conv2d, 20, kernel_size=5),
+    nn.MaxPool2d(2),
+    Activation(),
+    Flatten(),
+    Infer(nn.Linear, 50),
+    Activation(),
+    Infer(nn.Linear, 10),
+    activation=nn.LogSoftmax(dim=1),
+  )
+
+  # run a forward pass to infer input shapes using `Infer` modules
+  lenet(torch.rand(1, 1, 28, 28))
+
+  # use the registry to resolve optimizer name to an optimizer class
+  optimizer = yann.resolve.optimizer(
+    params.optimizer,
+    yann.trainable(lenet.parameters()),
+    momentum=params.momentum,
+    lr=params.learning_rate,
+  )
+
+  train = Trainer(
+    params=params,
+    model=lenet,
+    optimizer=optimizer,
+    dataset=params.dataset,
+    val_dataset=(params.dataset, {'train': False}),
+    batch_size=params.batch_size,
+    transform=transforms.Compose(
+      [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))],
+    ),
+    loss='nll_loss',
+    metrics=('accuracy',),
+    callbacks=[
+      # History("accuracy"),
+      RichProgress(),
+    ],
+    amp=False,
+  )
+
+  train(params.epochs)
+
+  # save checkpoint
+  train.checkpoint()
+
+  # plot the loss curve
+  # train.history.plot()
diff --git a/pyproject.toml b/pyproject.toml
index d87e365..e9cbd2f 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -9,10 +9,13 @@ authors = [
 ]
 requires-python = ">=3.7"
 dependencies = [
+    "datasets>=2.13.2",
     "pyyaml>=6.0.1",
     "rich>=13.8.1",
+    "tokenizers>=0.13.3",
     "torch",
     "torchvision",
+    "transformers>=4.30.2",
 ]
 
 [project.optional-dependencies]
@@ -35,6 +38,7 @@ exclude = ["tests*"]  # Example: Exclude tests directory if you have one
 dev = [
     "add-trailing-comma>=2.4.0",
     "ipython>=7.34.0",
+    "pytest>=7.4.4",
 ]
 
 
diff --git a/requirements-dev.txt b/requirements-dev.txt
deleted file mode 100644
index df0b7de..0000000
--- a/requirements-dev.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-pytest==3.9.3
-pytest-asyncio==0.9.0
-pytest-cov==2.7.1
-pytest-forked==0.2
-pytest-mocha==0.3.0
-pytest-testmon==0.9.18
-pytest-watch==4.2.0
-pytest-xdist==1.24.0
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 56d239d..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-click
-numpy
-scipy
-matplotlib
-requests
-scikit-learn
-torch==1.2.0
-torchvision==0.4.0
diff --git a/tests/contrib/test_pretrained.py b/tests/contrib/test_pretrained.py
index d5c8042..c8fddf9 100644
--- a/tests/contrib/test_pretrained.py
+++ b/tests/contrib/test_pretrained.py
@@ -1,5 +1,5 @@
-import torch
 import pytest
+import torch
 from torch import nn
 
 import yann
@@ -15,7 +15,8 @@ except ImportError:
 
 @pytest.mark.skipif(
   not PRETRAINED_INSTALLED,
-  reason='pretrainedmodels not instaled')
+  reason='pretrainedmodels not instaled',
+)
 def test_pretrained():
   timer = Timer(log=True)
 
diff --git a/tests/data/io/test_download.py b/tests/data/io/test_download.py
index 2564443..66aa92f 100644
--- a/tests/data/io/test_download.py
+++ b/tests/data/io/test_download.py
@@ -54,4 +54,4 @@
 #
 #
 #   dataset_downloader.get_all(['1', '2', '3'])
-#   dataset_downloader.stream(['1', '2', '3'])
\ No newline at end of file
+#   dataset_downloader.stream(['1', '2', '3'])
diff --git a/tests/data/io/test_io.py b/tests/data/io/test_io.py
index 7524b0d..94b710e 100644
--- a/tests/data/io/test_io.py
+++ b/tests/data/io/test_io.py
@@ -1,11 +1,10 @@
-import yann
 import torch
 
+import yann
+
 
 def test_io(tmpdir):
-  x = {
-    'a': 2
-  }
+  x = {'a': 2}
 
   yann.save(x, tmpdir / 'x.json')
   y = yann.load(tmpdir / 'x.json')
@@ -17,4 +16,4 @@ def test_io(tmpdir):
 
   yann.save(x, tmpdir / 'x.pickle')
   y = yann.load(tmpdir / 'x.pickle')
-  assert x == y
\ No newline at end of file
+  assert x == y
diff --git a/tests/data/test_classes.py b/tests/data/test_classes.py
index 0c6dda0..fef391a 100644
--- a/tests/data/test_classes.py
+++ b/tests/data/test_classes.py
@@ -1,7 +1,6 @@
 from yann.data.classes import Classes, get_class_weights
 
 
-
 def test_classes():
   classes = Classes.ordered(10)
 
@@ -16,15 +15,9 @@ def test_classes():
   assert classes.index_encode('a') == 0
   assert classes.one_hot_encode('a')[0] == 1
 
-
   classes = Classes(counts={'a': 10, 'b': 20})
-  assert classes.weights() == [30/10, 30/20]
+  assert classes.weights(normalize=False) == [30 / 10, 30 / 20]
 
 
 def test_class_weights():
-  counts = {
-    0: 4,
-    1: 5,
-    2: 10
-  }
-
+  counts = {0: 4, 1: 5, 2: 10}
diff --git a/tests/data/test_place.py b/tests/data/test_place.py
index a9aa870..c86dbca 100644
--- a/tests/data/test_place.py
+++ b/tests/data/test_place.py
@@ -1,18 +1,18 @@
-import torch
 import pytest
+import torch
 
 from yann.data.place import Place
 
-
 devices = ['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']
 
 
 @pytest.mark.parametrize('device', devices)
 def test_place(device):
   import torch
-  tuple_batch = (torch.rand(3,3), torch.rand(3,1), 'foo')
+
+  tuple_batch = (torch.rand(3, 3), torch.rand(3, 1), 'foo')
 
   place = Place(('cpu', device))
   b = place(tuple_batch)
   # assert b[0].device == 'cpu'
-  assert b[2] == 'foo'
\ No newline at end of file
+  assert b[2] == 'foo'
diff --git a/tests/modules/conv/test_mixconv.py b/tests/modules/conv/test_mixconv.py
index dd14d97..210f418 100644
--- a/tests/modules/conv/test_mixconv.py
+++ b/tests/modules/conv/test_mixconv.py
@@ -1,11 +1,11 @@
-from yann.modules.conv.mixconv import MixConv
+import pytest
 import torch
 
-import pytest
+from yann.modules.conv.mixconv import MixConv
 
 
 def test_mixconv():
-  c = MixConv(10, 20, (3,5,7))
+  c = MixConv(10, 20, (3, 5, 7))
 
   assert c.input_channel_counts == [4, 3, 3]
   assert c.output_channel_counts == [8, 6, 6]
@@ -16,6 +16,7 @@ def test_mixconv():
 
   assert x.shape == torch.Size([8, 20, 32, 32])
 
+
 def test_not_depthwise():
   c = MixConv(10, 20, (3, 5), depthwise=False)
   t = torch.rand((8, 10, 32, 32))
@@ -34,10 +35,12 @@ def test_1_group():
   t = torch.rand((10, 4, 32, 32))
   c(t)
 
+
 def test_auto_groups():
-  c = MixConv((2,2,2), (2,2,2))
+  c = MixConv((2, 2, 2), (2, 2, 2))
+
+  assert c.kernel_sizes == [3, 5, 7]
 
-  assert c.kernel_sizes == [3,5,7]
 
 def test_variable_input_channel_counts():
   c = MixConv((16, 8, 4, 4), (16, 8, 4, 4), (3, 5, 7, 9))
@@ -56,4 +59,4 @@ def test_variable_input_channel_counts():
   c(t)
 
   c = MixConv((16, 8, 4, 4), (16, 16, 4, 4), (3, 5, 7))
-  c(t)
\ No newline at end of file
+  c(t)
diff --git a/tests/modules/conv/test_utils.py b/tests/modules/conv/test_utils.py
index 747c579..cf2ebe1 100644
--- a/tests/modules/conv/test_utils.py
+++ b/tests/modules/conv/test_utils.py
@@ -5,4 +5,3 @@ def test_get_same_padding():
   assert get_same_padding(3) == 1
   assert get_same_padding(5) == 2
   assert get_same_padding(7) == 3
-
diff --git a/tests/modules/test_loss.py b/tests/modules/test_loss.py
index dbb4ab1..519c773 100644
--- a/tests/modules/test_loss.py
+++ b/tests/modules/test_loss.py
@@ -1,107 +1,84 @@
-from yann.modules.loss import binary_focal_loss
 import torch
 from torch.nn.functional import binary_cross_entropy_with_logits
 
+from yann.modules.loss import binary_focal_loss
 from yann.testing import check_tensor
 
 T = torch.tensor
 
 
 def test_focal_loss():
-  logits = torch.Tensor([
-      [-10, 2.3],
-      [-10, 5.5]
-    ])
+  logits = torch.Tensor([[-10, 2.3], [-10, 5.5]])
 
-  targets = torch.Tensor([
-      [1.0, 0.0],
-      [.0, 1.0]
-    ])
+  targets = torch.Tensor([[1.0, 0.0], [0.0, 1.0]])
 
   assert torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='none', gamma=0, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='none')
-  ), "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+    binary_focal_loss(logits, targets, reduction='none', gamma=0, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction='none'),
+  ), (
+    'focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss'
+  )
 
   assert torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='mean', gamma=0, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='mean')
-  ), "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+    binary_focal_loss(logits, targets, reduction='mean', gamma=0, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction='mean'),
+  ), (
+    'focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss'
+  )
 
   assert not torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='none', gamma=.4, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='none')
-  ), "focal loss with gamma > 0 should not be same as binary cross entropy loss"
-
+    binary_focal_loss(logits, targets, reduction='none', gamma=0.4, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction='none'),
+  ), 'focal loss with gamma > 0 should not be same as binary cross entropy loss'
 
   assert binary_focal_loss(
     T([[100.0]]),
     T([[1.0]]),
-    gamma=4
-  ) == binary_focal_loss(
-    T([[-100.0]]),
-    T([[0.0]]),
-    gamma=4
-  ), "loss should be symmetrical"
+    gamma=4,
+  ) == binary_focal_loss(T([[-100.0]]), T([[0.0]]), gamma=4), (
+    'loss should be symmetrical'
+  )
 
-  assert binary_focal_loss(
-    T([[100.0]]),
-    T([[0.0]])
-  ) == binary_focal_loss(
+  assert binary_focal_loss(T([[100.0]]), T([[0.0]])) == binary_focal_loss(
     T([[-100.0]]),
-    T([[1.0]])
+    T([[1.0]]),
   )
 
-  assert binary_focal_loss(
+  assert binary_focal_loss(T([[2.0]]), T([[1.0]]), gamma=1) > binary_focal_loss(
     T([[2.0]]),
     T([[1.0]]),
-    gamma=1
-  ) > binary_focal_loss(
-    T([[2.0]]),
-    T([[1.0]]),
-    gamma=2
-  ), "larger gamma should reduce loss for well classified examples"
+    gamma=2,
+  ), 'larger gamma should reduce loss for well classified examples'
 
   assert binary_focal_loss(
     T([[2.0]]),
     T([[1.0]]),
-    gamma=1
+    gamma=1,
   ) < binary_cross_entropy_with_logits(
     T([[2.0]]),
     T([[1.0]]),
-  ), "focal loss should penalize well classified examples less than binary cross entropy"
+  ), (
+    'focal loss should penalize well classified examples less than binary cross entropy'
+  )
 
   for g in range(0, 10):
     for logit in range(-1_000, 1_000, 100):
       check_tensor(
-        binary_focal_loss(
-          T([[float(logit)]]),
-          T([[1.0]]),
-          gamma=g
-        ),
+        binary_focal_loss(T([[float(logit)]]), T([[1.0]]), gamma=g),
         anomalies=True,
         gte=0,
       )
   check_tensor(
-    binary_focal_loss(
-      T([[-.3]]),
-      T([[.2]]),
-      gamma=2
-    ),
+    binary_focal_loss(T([[-0.3]]), T([[0.2]]), gamma=2),
     anomalies=True,
     gte=0,
   )
 
+
 def test_focal_loss_gradients():
   logits = T([[0.0], [0.0]], requires_grad=True)
   targets = T([[0.0], [0.0]])
 
   loss = binary_focal_loss(logits, targets)
   loss.backward()
-  check_tensor(logits.grad, anomalies=True)
\ No newline at end of file
+  check_tensor(logits.grad, anomalies=True)
diff --git a/tests/test_callbacks.py b/tests/test_callbacks.py
index 41e5b9d..8d4310a 100644
--- a/tests/test_callbacks.py
+++ b/tests/test_callbacks.py
@@ -1,12 +1,11 @@
 from yann import callbacks
 
 
-
 def test():
   cbs = callbacks.get_callbacks(
     checkpoint={'freq': 5},
     plot=True,
-    progress=False
+    progress=False,
   )
 
   assert not any(isinstance(x, callbacks.ProgressBar) for x in cbs)
@@ -15,4 +14,4 @@ def test():
 
   for cb in cbs:
     if isinstance(cb, callbacks.Checkpoint):
-      assert cb.freq == 5
\ No newline at end of file
+      assert cb.freq == 5
diff --git a/tests/test_export.py b/tests/test_export.py
index ddcd89a..5f40f9c 100644
--- a/tests/test_export.py
+++ b/tests/test_export.py
@@ -35,10 +35,9 @@ def test_export_traced(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))],
+  )
 
   path = tmpdir
 
@@ -52,20 +51,20 @@ def test_export_traced(tmpdir):
     model=model,
     trace=torch.rand(1, 1, 32, 32),
     preprocess=preprocess,
-    classes=classes
-
+    classes=classes,
   )
 
   expected_files = [
     'model.traced.th',
     'preprocess.pkl',
     'classes.json',
-    'requirements.txt',
-    'env.yml'
   ]
 
   for name in expected_files:
     assert (path / name).exists()
+  
+  # These files may or may not exist depending on available tools
+  optional_files = ['requirements.txt', 'env.yml']
 
   loaded = load(path)
 
@@ -90,10 +89,9 @@ def test_export_state_dict(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))],
+  )
 
   path = tmpdir
 
@@ -105,7 +103,7 @@ def test_export_state_dict(tmpdir):
     model=model,
     state_dict=True,
     preprocess=preprocess,
-    classes=[str(n) for n in range(NUM_CLASSES)]
+    classes=[str(n) for n in range(NUM_CLASSES)],
   )
 
   expected_files = [
@@ -124,10 +122,9 @@ def test_export_pickled(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))],
+  )
 
   path = tmpdir
 
@@ -139,8 +136,7 @@ def test_export_pickled(tmpdir):
     model=model,
     state_dict=False,
     preprocess=preprocess,
-    classes=[str(n) for n in range(NUM_CLASSES)]
-
+    classes=[str(n) for n in range(NUM_CLASSES)],
   )
 
   expected_files = [
diff --git a/tests/test_params.py b/tests/test_params.py
index 1b93df4..dabfdb9 100644
--- a/tests/test_params.py
+++ b/tests/test_params.py
@@ -6,7 +6,6 @@ def test():
     a = 4
     b = 'b'
 
-
   p = Params()
   assert p.a == 4
   assert p.b == 'b'
@@ -27,7 +26,6 @@ def test():
 
 
 def test_serialization(tmpdir):
-
   class Params(params.HyperParams):
     a = 4
     b = 'b'
@@ -47,4 +45,4 @@ def test_serialization(tmpdir):
   p.save(tmpdir / 'params.pkl')
   p2 = p.load(tmpdir / 'params.pkl')
   assert (tmpdir / 'params.pkl').exists()
-  assert p == p2
\ No newline at end of file
+  assert p == p2
diff --git a/tests/test_train.py b/tests/test_train.py
index ed21cd9..45aa8f5 100644
--- a/tests/test_train.py
+++ b/tests/test_train.py
@@ -4,14 +4,17 @@ from torch import nn
 from torch.optim import SGD
 
 from yann.callbacks import (
-  History, HistoryPlotter, HistoryWriter, Logger, Checkpoint
+  Checkpoint,
+  History,
+  HistoryPlotter,
+  HistoryWriter,
+  Logger,
 )
 from yann.datasets import TinyDigits
 from yann.datasets.wrappers import Slice
 from yann.modules import Flatten
 from yann.train import Trainer
 
-
 devices = ['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']
 
 
@@ -19,6 +22,8 @@ devices = ['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']
 @pytest.mark.parametrize('device', devices)
 def test_train(tmpdir, device):
   """Sanity check train run"""
+  
+  pytest.importorskip("sklearn", reason="scikit-learn not installed")
 
   model = nn.Sequential(
     nn.Conv2d(1, 20, 3),
@@ -26,7 +31,7 @@ def test_train(tmpdir, device):
     nn.Conv2d(20, 20, 3),
     nn.ReLU(inplace=True),
     Flatten(),
-    nn.Linear(320, 10)
+    nn.Linear(320, 10),
   )
 
   train = Trainer(
@@ -34,15 +39,20 @@ def test_train(tmpdir, device):
     model=model,
     dataset=Slice(TinyDigits(), 0, 256),
     device=device,
-    optimizer=SGD(model.parameters(), lr=.01, momentum=0.9, weight_decay=.001),
+    optimizer=SGD(
+      model.parameters(),
+      lr=0.01,
+      momentum=0.9,
+      weight_decay=0.001,
+    ),
     loss=nn.CrossEntropyLoss(),
     callbacks=[
       History(),
       HistoryPlotter(save=True),
       HistoryWriter(),
       Logger(batch_freq=20),
-      Checkpoint()
-    ]
+      Checkpoint(),
+    ],
   )
 
   train(2)
@@ -56,7 +66,6 @@ def test_train(tmpdir, device):
   assert export_path.is_dir()
 
 
-
 def test_interface():
   train = Trainer()
 
@@ -65,7 +74,6 @@ def test_interface():
   train.paths
 
 
-
 # @pytest.mark.slow
 # @pytest.mark.parametrize('device', devices)
 # def test_train_resolved(tmpdir, device):
@@ -90,4 +98,4 @@ def test_interface():
 #       'mask': 'foo',
 #       'label': 'foo'
 #     }
-#   )
\ No newline at end of file
+#   )
diff --git a/tests/utils/test_registry.py b/tests/utils/test_registry.py
index 1374bdb..dacc202 100644
--- a/tests/utils/test_registry.py
+++ b/tests/utils/test_registry.py
@@ -54,7 +54,9 @@ def test():
 
   yann.register.optimizers(
     SGD,
-    init=lambda SGD, *args, parameters=None, **kwargs: SGD(parameters, )
+    init=lambda SGD, *args, parameters=None, **kwargs: SGD(
+      parameters,
+    ),
   )
 
 
@@ -109,7 +111,6 @@ def test_yann_registry():
   yann.resolve('MNIST', required=True)
 
 
-
 def test_tuple_arg():
   import yann
 
@@ -119,4 +120,4 @@ def test_tuple_arg():
       self.kwargs = kwargs
 
   foo = yann.resolve(('Foo', {'test': True}))
-  assert foo.kwargs['test'] == True
\ No newline at end of file
+  assert foo.kwargs['test'] == True
diff --git a/yann/__init__.py b/yann/__init__.py
index 62c1604..a69dd7d 100644
--- a/yann/__init__.py
+++ b/yann/__init__.py
@@ -3,32 +3,25 @@ from contextlib import contextmanager
 
 __version__ = '0.0.40'
 
+from pathlib import Path
 from typing import Union
 
+import numpy as np
 import torch
 from torch import nn
-import numpy as np
-
-from .config.defaults import default
-from .config.setup import registry
-
-register = registry
-resolve = registry.resolve
 
-from pathlib import Path
-
-from yann.utils import to_numpy, repeat, counter, is_notebook, timeout
-from yann.data import batches, shuffle, chunk
+from yann.data import batches, chunk, shuffle
 from yann.data.io import load, save
 from yann.data.io.download import download
+from yann.data.loaders import loader
 from yann.data.utils import pad, pad_to_largest
-# from yann.data import datasets
-from yann.viz import show, plot
+from yann.testing import Checker
+from yann.utils import counter, is_notebook, repeat, timeout, to_numpy
+from yann.utils.profile import param_count, profile
 from yann.utils.timer import time
-from yann.utils.profile import profile, param_count
 
-from yann.testing import Checker
-from yann.data.loaders import loader
+# from yann.data import datasets
+from yann.viz import plot, show
 
 # T = torch.Tensor
 #
@@ -80,27 +73,32 @@ context = object()
 memory_formats = dict(
   contiguous_format=torch.contiguous_format,
   channels_last=torch.channels_last,
-  preserve_format=torch.preserve_format
+  preserve_format=torch.preserve_format,
 )
 
+
 def to_tensor(
-    x: Union[list, tuple, np.ndarray, torch.Tensor, 'PIL.Image.Image']
+  x: Union[list, tuple, np.ndarray, torch.Tensor, 'PIL.Image.Image'],
 ) -> torch.Tensor:
   if torch.is_tensor(x):
     return x
   if isinstance(x, np.ndarray):
     return torch.from_numpy(x)
   import PIL.Image
+
   if isinstance(x, PIL.Image.Image):
     from torchvision.transforms import functional as F
+
     return F.to_tensor(x)
   return torch.Tensor(x)
 
 
 def seed(val=1, deterministic=False):
+  import random
+
   import numpy as np
   import torch
-  import random
+
   random.seed(val)
   np.random.seed(val)
   torch.manual_seed(val)
@@ -127,11 +125,13 @@ def get_item(x: Union[torch.Tensor, np.ndarray]):
 
 def benchmark():
   from torch.backends import cudnn
+
   cudnn.benchmark = True
 
 
 def detect_anomalies(val=True):
   import torch.autograd
+
   torch.autograd.set_detect_anomaly(val)
 
 
@@ -191,15 +191,16 @@ def group_params(model, get_key):
     splits[get_key(name, param)] = param
   return splits
 
+
 from torch.nn.modules.batchnorm import _BatchNorm
 
 
 def split_regularization_params(
-    module: nn.Module,
-    excluded_modules=(_BatchNorm,),
-    excluded_names=('bias',),
-    param_groups=True,
-    weight_decay=1e-4
+  module: nn.Module,
+  excluded_modules=(_BatchNorm,),
+  excluded_names=('bias',),
+  param_groups=True,
+  weight_decay=1e-4,
 ):
   """
   filter out parameters which should not be regularized
@@ -217,7 +218,10 @@ def split_regularization_params(
           else:
             reg.append(param)
   if param_groups:
-    return [dict(params=reg, weight_decay=weight_decay), dict(params=no_reg, weight_decay=0)]
+    return [
+      dict(params=reg, weight_decay=weight_decay),
+      dict(params=no_reg, weight_decay=0),
+    ]
   else:
     return reg, no_reg
 
@@ -240,8 +244,9 @@ def freeze(x, exclude=None):
   elif exclude:
     raise ValueError(
       "can't exclude modules if parameters are passed, "
-      "pass an instance of nn.Module if you need to "
-      "exclude certain modules")
+      'pass an instance of nn.Module if you need to '
+      'exclude certain modules',
+    )
   else:
     for p in x:
       p.requires_grad = False
@@ -285,10 +290,9 @@ def replace_linear(model, num_outputs, layer_name=None):
       raise ValueError(
         f'Multiple linear layers found and layer name was not provided, '
         f'provide a valid layer_name, '
-        f'(valid names: {", ".join([n for n, m in linear_layers])})'
+        f'(valid names: {", ".join([n for n, m in linear_layers])})',
       )
 
-
   if '.' in layer_name:
     *path, layer_name = list(layer_name.split('.'))
     for p in path:
@@ -298,11 +302,7 @@ def replace_linear(model, num_outputs, layer_name=None):
   new_linear = nn.Linear(old_linear.in_features, num_outputs)
   new_linear.to(old_linear.weight.device)
 
-  setattr(
-    model,
-    layer_name,
-    new_linear
-  )
+  setattr(model, layer_name, new_linear)
 
   return layer_name
 
@@ -331,7 +331,8 @@ def eval_mode(*modules, grad=False):
   if grad:
     training = (m.training for m in modules)
     try:
-      for m in modules: m.eval()
+      for m in modules:
+        m.eval()
       yield
     finally:
       for m, train in zip(modules, training):
@@ -341,7 +342,8 @@ def eval_mode(*modules, grad=False):
     with torch.no_grad():
       training = (m.training for m in modules)
       try:
-        for m in modules: m.eval()
+        for m in modules:
+          m.eval()
         yield
       finally:
         for m, train in zip(modules, training):
@@ -399,11 +401,15 @@ def get_device(module):
 
 def get_trainer(params=None, **kwargs):
   from yann.train import Trainer
+
   return Trainer(params=params, **kwargs)
 
 
 def get_model_name(model):
-  if isinstance(model, (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel)):
+  if isinstance(
+    model,
+    (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel),
+  ):
     model = model.module
 
   if hasattr(model, 'name'):
@@ -413,9 +419,9 @@ def get_model_name(model):
 
 
 def load_state_dict(
-    x,
-    state_dict: Union[str, 'pathlib.Path', dict],
-    strict: bool = True
+  x,
+  state_dict: Union[str, 'pathlib.Path', dict],
+  strict: bool = True,
 ):
   if not isinstance(state_dict, dict):
     state_dict = yann.load(state_dict)
@@ -423,7 +429,6 @@ def load_state_dict(
   return x.load_state_dict(state_dict, strict=strict)
 
 
-
 def grad_norm(parameters, norm_type: float = 2.0):
   """
 
@@ -444,7 +449,7 @@ def grad_norm(parameters, norm_type: float = 2.0):
 
   norm_type = float(norm_type)
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   device = parameters[0].grad.device
   try:
     from torch import inf
@@ -456,10 +461,10 @@ def grad_norm(parameters, norm_type: float = 2.0):
     norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
   else:
     norm = torch.norm(
-      torch.stack([
-        torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters
-      ]),
-      norm_type
+      torch.stack(
+        [torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters],
+      ),
+      norm_type,
     )
   return norm
 
@@ -473,7 +478,7 @@ def param_norm(parameters, norm_type: float = 2.0):
   parameters = list(parameters)
   norm_type = float(norm_type)
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   device = parameters[0].device
   try:
     from torch import inf
@@ -484,10 +489,10 @@ def param_norm(parameters, norm_type: float = 2.0):
     norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
   else:
     norm = torch.norm(
-      torch.stack([
-        torch.norm(p.detach(), norm_type).to(device) for p in parameters
-      ]),
-      norm_type
+      torch.stack(
+        [torch.norm(p.detach(), norm_type).to(device) for p in parameters],
+      ),
+      norm_type,
     )
   return norm
 
@@ -507,8 +512,14 @@ def nested_lookup(obj, key):
   return obj
 
 
-import yann.params
+from yann.config.defaults import default
+from yann.config.setup import registry
+
+register = registry
+resolve = registry.resolve
+
+import yann.callbacks
 import yann.metrics
+import yann.optim
+import yann.params
 import yann.train
-import yann.callbacks
-import yann.optim
\ No newline at end of file
diff --git a/yann/callbacks/__init__.py b/yann/callbacks/__init__.py
index 0aa232c..1e704bc 100644
--- a/yann/callbacks/__init__.py
+++ b/yann/callbacks/__init__.py
@@ -1,19 +1,20 @@
-from .base import FunctionCallback, Callback
+from yann.utils import is_notebook
+
+from .base import Callback, FunctionCallback
+from .callbacks import Callbacks
 from .checkpoint import Checkpoint
 from .eval import MulticlassEval
 from .history import History, HistoryPlotter, HistoryWriter
 from .logging import Logger
-from .stop import StopOnNaN
-from .timing import Timing
-from .progbar import ProgressBar
-from .wandb import Wandb
+
 # from .ema import EMA
 # from .swa import SWA
 from .lr import LRRangeTest
+from .progbar import ProgressBar
+from .stop import StopOnNaN
+from .timing import Timing
+from .wandb import Wandb
 
-from .callbacks import Callbacks
-
-from yann.utils import is_notebook
 
 def _maybe_init(value, cls, **kwargs):
   if value is None or value is False:
@@ -24,26 +25,29 @@ def _maybe_init(value, cls, **kwargs):
 
 
 def get_callbacks(
-    interactive=None,
-    plot=True,
-    write=True,
-    log=True,
-    checkpoint=True,
-    time=False,
-    progress=True,
-    tensorboard=True,
+  interactive=None,
+  plot=True,
+  write=True,
+  log=True,
+  checkpoint=True,
+  time=False,
+  progress=True,
+  tensorboard=True,
 ):
-
   if interactive is None:
     interactive = is_notebook()
 
   if tensorboard:
-    from .tensorboard import Tensorboard
-    tb = _maybe_init(tensorboard, Tensorboard)
+    try:
+      from .tensorboard import Tensorboard
+      tb = _maybe_init(tensorboard, Tensorboard)
+    except ImportError:
+      tb = None
   else:
     tb = None
   return [
-    x for x in (
+    x
+    for x in (
       # History(),
       _maybe_init(progress, ProgressBar, notebook=interactive),
       _maybe_init(plot, HistoryPlotter, save=not interactive),
@@ -51,5 +55,7 @@ def get_callbacks(
       _maybe_init(checkpoint, Checkpoint),
       _maybe_init(log, Logger),
       _maybe_init(time, Timing),
-      tb
-    ) if x]
+      tb,
+    )
+    if x
+  ]
diff --git a/yann/callbacks/base.py b/yann/callbacks/base.py
index b9900a7..c645cc9 100644
--- a/yann/callbacks/base.py
+++ b/yann/callbacks/base.py
@@ -21,8 +21,15 @@ class Callback:
   def on_step_start(self, index=None, inputs=None, targets=None, trainer=None):
     pass
 
-  def on_step_end(self, index=None, inputs=None, targets=None, outputs=None, loss=None,
-                   trainer=None):
+  def on_step_end(
+    self,
+    index=None,
+    inputs=None,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     pass
 
   def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
@@ -31,11 +38,22 @@ class Callback:
   def on_validation_start(self, trainer=None):
     pass
 
-  def on_validation_batch(self, inputs=None, targets=None, outputs=None, trainer=None):
+  def on_validation_batch(
+    self,
+    inputs=None,
+    targets=None,
+    outputs=None,
+    trainer=None,
+  ):
     pass
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     pass
 
   def on_train_end(self, trainer=None):
@@ -69,8 +87,8 @@ class FunctionCallback(Callback):
   def on(self, event, callback):
     if event not in self._valid_names:
       raise ValueError(
-        f'{event} is not a valid option, '
-        f'must be one of {self._valid_names}')
+        f'{event} is not a valid option, must be one of {self._valid_names}',
+      )
     callbacks = self.callbacks[event]
     if callback not in callbacks:
       callbacks.append(callback)
@@ -78,6 +96,7 @@ class FunctionCallback(Callback):
   def on_init(self, *args, **kwargs):
     for f in self.callbacks['init']:
       f(*args, **kwargs)
+
   def on_train_start(self, *args, **kwargs):
     for f in self.callbacks['train_start']:
       f(*args, **kwargs)
@@ -134,4 +153,4 @@ class TempCallback(Callback):
 
   def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
     if self.epochs and self.epochs < epoch:
-      self.unregister()
\ No newline at end of file
+      self.unregister()
diff --git a/yann/callbacks/callbacks.py b/yann/callbacks/callbacks.py
index 4b718a0..8a80c61 100644
--- a/yann/callbacks/callbacks.py
+++ b/yann/callbacks/callbacks.py
@@ -1,8 +1,8 @@
 from collections import OrderedDict
 
 import yann.callbacks
-from yann.utils import camel_to_snake
 from yann.callbacks.base import Callback
+from yann.utils import camel_to_snake
 
 
 class Events:
@@ -80,7 +80,7 @@ class Callbacks(Callback):
     return len(self._callbacks) > 0
 
   def __str__(self):
-    return f"Callbacks({', '.join(f'{k}={v}' for k,v in self._callbacks.items())}"
+    return f'Callbacks({", ".join(f"{k}={v}" for k, v in self._callbacks.items())}'
 
   def append(self, callback):
     name = self._get_name(callback)
@@ -97,6 +97,7 @@ class Callbacks(Callback):
       self.function_callback.on(event, callback)
       return self
     else:
+
       def decorated(func):
         self.function_callback.on(event, func)
         return func
@@ -120,7 +121,15 @@ class Callbacks(Callback):
     pass
 
   @callback
-  def on_step_end(self, index: int, inputs, targets, outputs, loss, trainer=None):
+  def on_step_end(
+    self,
+    index: int,
+    inputs,
+    targets,
+    outputs,
+    loss,
+    trainer=None,
+  ):
     pass
 
   @callback
@@ -144,10 +153,15 @@ class Callbacks(Callback):
     pass
 
   @callback
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     pass
 
   @callback
   def on_train_end(self, trainer=None):
     pass
-
diff --git a/yann/callbacks/checkpoint.py b/yann/callbacks/checkpoint.py
index 0852b68..e4ad017 100644
--- a/yann/callbacks/checkpoint.py
+++ b/yann/callbacks/checkpoint.py
@@ -1,4 +1,5 @@
 from collections import OrderedDict
+
 from .base import Callback
 
 
@@ -12,9 +13,13 @@ class Checkpoint(Callback):
     self.save_on_end = save_on_end
 
   def on_epoch_end(self, epoch, loss=None, metrics=None, trainer=None):
-    if epoch % self.freq == 0 :
+    if epoch % self.freq == 0:
       self.paths[trainer.num_steps] = trainer.checkpoint()
 
   def on_train_end(self, trainer=None):
-    if self.save_on_end and trainer.num_steps > 10 and trainer.num_steps not in self.paths:
+    if (
+      self.save_on_end
+      and trainer.num_steps > 10
+      and trainer.num_steps not in self.paths
+    ):
       self.paths[trainer.num_steps] = trainer.checkpoint()
diff --git a/yann/callbacks/eval.py b/yann/callbacks/eval.py
index 1cd768b..ab70fea 100644
--- a/yann/callbacks/eval.py
+++ b/yann/callbacks/eval.py
@@ -1,8 +1,7 @@
 import sys
-from sklearn.metrics import classification_report, accuracy_score
-from .base import Callback
 
 from ..metrics import get_preds
+from .base import Callback
 
 
 class MulticlassEval(Callback):
@@ -19,13 +18,23 @@ class MulticlassEval(Callback):
 
     preds, targets = preds.to('cpu').numpy(), targets.to('cpu').numpy()
 
-    print(classification_report(targets, preds, target_names=classes),
-          file=self.dest)
+    from sklearn.metrics import accuracy_score, classification_report
+
+    print(
+      classification_report(targets, preds, target_names=classes),
+      file=self.dest,
+    )
 
     print('Accuracy: ', accuracy_score(targets, preds))
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None, **kwargs):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+    **kwargs,
+  ):
     self(targets=targets, outputs=outputs, trainer=trainer)
 
 
diff --git a/yann/callbacks/history.py b/yann/callbacks/history.py
index f39e27c..bdfc408 100644
--- a/yann/callbacks/history.py
+++ b/yann/callbacks/history.py
@@ -1,17 +1,18 @@
-from time import time as get_time
 from pathlib import Path
+from time import time as get_time
+
 import torch
 
-from ..utils.decorators import lazy
+from .. import get_item, resolve
 from ..callbacks.base import Callback
-from ..viz import plot_line
-from .. import resolve, get_item
-from ..data.metrics import MetricStore, EventStore, Event
+from ..data.metrics import Event, EventStore, MetricStore
 from ..evaluation import evaluate_metrics
+from ..utils.decorators import lazy
+from ..viz import plot_line
 
 
 class History(Callback):
-  def __init__(self, *metrics, **named_metrics):
+  def __init__(self, *metrics, eval_freq: int = 1, **named_metrics):
     super(History, self).__init__()
     if len(metrics) == 1 and not named_metrics:
       if isinstance(metrics[0], dict):
@@ -31,30 +32,49 @@ class History(Callback):
       else:
         raise ValueError(f'Unknown metric {m}')
 
-    self.metrics = MetricStore(
-      self.metric_funcs.keys(),
-      cast_value=get_item
-    )
+    self.metrics = MetricStore(self.metric_funcs.keys(), cast_value=get_item)
     self.val_metrics = MetricStore(
       self.metric_funcs.keys(),
-      cast_value=get_item
+      cast_value=get_item,
     )
     self.events = EventStore()
+    self.eval_freq = eval_freq
 
     self.trainer = None
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
+    step = trainer.num_steps
+    metrics_to_update = {'loss': loss}
+
+    # Evaluate other metrics only at the specified frequency
+    if step % self.eval_freq == 0:
+      evaluated_metrics = evaluate_metrics(
+        targets=targets,
+        outputs=outputs,
+        metrics=self.metric_funcs,
+      )
+      metrics_to_update.update(evaluated_metrics)
+
     self.metrics.update(
-      step=trainer.num_steps,
-      loss=loss,
-      **evaluate_metrics(targets=targets, outputs=outputs, metrics=self.metric_funcs)
+      step=step,
+      **metrics_to_update,
     )
 
-  def on_validation_end(self, loss=None, outputs=None, targets=None, trainer=None):
+  def on_validation_end(
+    self,
+    loss=None,
+    outputs=None,
+    targets=None,
+    trainer=None,
+  ):
     self.val_metrics.update(
       step=trainer.num_epochs,
       loss=loss,
-      **evaluate_metrics(targets=targets, outputs=outputs, metrics=self.metric_funcs)
+      **evaluate_metrics(
+        targets=targets,
+        outputs=outputs,
+        metrics=self.metric_funcs,
+      ),
     )
 
   @lazy
@@ -62,10 +82,16 @@ class History(Callback):
     return HistoryPlotter(history=self)
 
 
-
 class HistoryPlotter(Callback):
-  def __init__(self, freq=500, window=50, metrics=None,
-               clear=False, save=False, history: History = None):
+  def __init__(
+    self,
+    freq=500,
+    window=50,
+    metrics=None,
+    clear=False,
+    save=False,
+    history: History = None,
+  ):
     super().__init__()
     self.history: History = history
     self.freq = freq
@@ -85,11 +111,11 @@ class HistoryPlotter(Callback):
     if self.clear:
       try:
         from IPython.display import clear_output
+
         clear_output(wait=True)
       except:
         pass
 
-
     if validation:
       metrics = self.history.val_metrics
     else:
@@ -108,11 +134,10 @@ class HistoryPlotter(Callback):
         ylabel=f'validation {name}' if validation else name,
         name=f'validation {name}' if validation else name,
         window=1 if validation else self.window,
-        save=self.save and
-             self.root / (f'validation {name}' if validation else name),
+        save=self.save and self.root / (f'validation {name}' if validation else name),
         show=not self.save,
         figsize=self.figsize,
-        **kwargs
+        **kwargs,
       )
 
   def on_train_start(self, trainer=None):
@@ -122,20 +147,25 @@ class HistoryPlotter(Callback):
 
   def on_step_end(self, *args, trainer=None, **kwargs):
     if trainer.num_steps % self.freq == 0:
-      self.plot(
-        title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}'
-      )
+      self.plot(title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}')
 
   def on_validation_end(self, *args, trainer=None, **kwargs):
     self.plot(
       validation=True,
-      title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}'
+      title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}',
     )
 
 
 class HistoryWriter(Callback):
-  def __init__(self, root=None, train=True, val=True, mode='a+',
-               write_freq=1, flush_freq=500):
+  def __init__(
+    self,
+    root=None,
+    train=True,
+    val=True,
+    mode='a+',
+    write_freq=1,
+    flush_freq=500,
+  ):
     self.root = root
     self.mode = mode
     self.train = train
@@ -181,35 +211,51 @@ class HistoryWriter(Callback):
       return
 
     if self.header is None:
-      self.header = ['timestamp', 'epoch', 'step', *trainer.history.metrics.keys()]
+      self.header = [
+        'timestamp',
+        'epoch',
+        'step',
+        *trainer.history.metrics.keys(),
+      ]
       self.train_file.write('\t'.join(self.header) + '\n')
 
     self.train_file.write(
-      f"{trainer.history.metrics.times[-1]}\t"
-      f"{trainer.num_epochs}\t"
-      f"{len(trainer.history.metrics.times)}\t"
-      + '\t'.join((f"{trainer.history.metrics[m][-1]:.4f}"
-                   for m in self.header[3:]))
-      + '\n'
+      f'{trainer.history.metrics.times[-1]}\t'
+      f'{trainer.num_epochs}\t'
+      f'{len(trainer.history.metrics.times)}\t'
+      + '\t'.join(
+        (f'{trainer.history.metrics[m][-1]:.4f}' for m in self.header[3:]),
+      )
+      + '\n',
     )
 
     if index % self.flush_freq == 0:
       self.train_file.flush()
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     if self.val_header is None:
-      self.val_header = ['timestamp', 'epoch', 'step',
-                         *trainer.history.val_metrics.keys()]
+      self.val_header = [
+        'timestamp',
+        'epoch',
+        'step',
+        *trainer.history.val_metrics.keys(),
+      ]
       self.val_file.write('\t'.join(self.header) + '\n')
 
     self.val_file.write(
-      f"{trainer.history.val_metrics.times[-1]}\t"
-      f"{trainer.num_epochs}\t"
-      f"{len(trainer.history.val_metrics.times)}\t"
-      + '\t'.join((f"{trainer.history.val_metrics[m][-1]:.4f}"
-                   for m in self.val_header[3:]))
-      + '\n'
+      f'{trainer.history.val_metrics.times[-1]}\t'
+      f'{trainer.num_epochs}\t'
+      f'{len(trainer.history.val_metrics.times)}\t'
+      + '\t'.join(
+        (f'{trainer.history.val_metrics[m][-1]:.4f}' for m in self.val_header[3:]),
+      )
+      + '\n',
     )
 
     self.val_file.flush()
diff --git a/yann/callbacks/logging.py b/yann/callbacks/logging.py
index 67ad419..60dd270 100644
--- a/yann/callbacks/logging.py
+++ b/yann/callbacks/logging.py
@@ -1,9 +1,11 @@
-import sys
 import logging
+import sys
 
-from .base import Callback
 from yann.utils.tensor import describe
 
+from .base import Callback
+
+
 class Logger(Callback):
   dist_placement = 0
 
@@ -21,7 +23,8 @@ class Logger(Callback):
       print(
         *args,
         sep.join(f'{k}: {v}' for k, v in kwargs.items()),
-        file=self.dest)
+        file=self.dest,
+      )
     else:
       print(*args, file=self.dest)
 
@@ -35,26 +38,36 @@ class Logger(Callback):
     if index % self.batch_freq == 0:
       if not self.logged_batch_shapes:
         try:
-          self.log("\ninputs:", describe(inputs),
-                   "\ntargets:", describe(targets),
-                   "\noutputs:", describe(outputs), '\n')
+          self.log(
+            '\ninputs:',
+            describe(inputs),
+            '\ntargets:',
+            describe(targets),
+            '\noutputs:',
+            describe(outputs),
+            '\n',
+          )
         except Exception as e:
           raise e
         self.logged_batch_shapes = True
 
       if self.batch_string:
-        self.log(self.batch_string.format(
-          batch=index,
-          **({m: v[-1] for m, v in trainer.history.metrics.items()})))
+        self.log(
+          self.batch_string.format(
+            batch=index,
+            **({m: v[-1] for m, v in trainer.history.metrics.items()}),
+          ),
+        )
       else:
         self.log(
           batch=f'{index:>8}',
-          **({m: f'{v[-1]:.4f}' for m, v in trainer.history.metrics.items()}))
+          **({m: f'{v[-1]:.4f}' for m, v in trainer.history.metrics.items()}),
+        )
 
   def on_epoch_start(self, epoch, trainer=None):
     self.log('\nStarting epoch', epoch)
 
-    self.log(f'''
+    self.log(f"""
 OPTIMIZER
 =========
 
@@ -65,24 +78,30 @@ PROGRESS
 ========
 epochs: {trainer.num_epochs}
 steps: {trainer.num_steps}
-samples: {trainer.num_samples}\n''')
+samples: {trainer.num_samples}\n""")
 
   def on_validation_start(self, trainer=None):
     self.log('\nStarting Validation')
 
-  def on_validation_end(self, loss=None, outputs=None, targets=None,
-    trainer=None):
+  def on_validation_end(
+    self,
+    loss=None,
+    outputs=None,
+    targets=None,
+    trainer=None,
+  ):
     self.log('\nCompleted Validation')
     self.log(
       epoch=trainer.num_epochs,
       steps=len(trainer.history.val_metrics),
-      **{m: f'{vals[-1]:.4f}' for m, vals in trainer.history.val_metrics.items()}
+      **{m: f'{vals[-1]:.4f}' for m, vals in trainer.history.val_metrics.items()},
     )
 
   def on_epoch_end(self, epoch, loss=None, metrics=None, trainer=None):
-    self.log('Completed epoch', epoch,)
+    self.log(
+      'Completed epoch',
+      epoch,
+    )
 
   def on_train_end(self, trainer=None):
     self.log('Completed training run. \n\n')
-
-
diff --git a/yann/callbacks/lr.py b/yann/callbacks/lr.py
index 5f71f1c..45e10a7 100644
--- a/yann/callbacks/lr.py
+++ b/yann/callbacks/lr.py
@@ -1,10 +1,11 @@
 import logging
 import os
 from math import cos, pi
+
 import numpy as np
 
-from ..callbacks.base import Callback
 from .. import set_param
+from ..callbacks.base import Callback
 from ..metrics import exp_moving_avg
 from ..viz import plot_line
 
@@ -15,15 +16,15 @@ def cosine_anneal(min_lr, max_lr, cur_step, num_steps):
 
 class SGDR(Callback):
   def __init__(
-      self,
-      optimizer=None,
-      max_lr=None,
-      min_lr=0,
-      cycle_len=10,
-      cycle_mult=1,
-      lr_mult=1,
-      verbose=True,
-      checkpoint=False
+    self,
+    optimizer=None,
+    max_lr=None,
+    min_lr=0,
+    cycle_len=10,
+    cycle_mult=1,
+    lr_mult=1,
+    verbose=True,
+    checkpoint=False,
   ):
     self.optimizer = optimizer
     self.max_lr = max_lr
@@ -52,10 +53,13 @@ class SGDR(Callback):
 
   def restart(self):
     if self.save_checkpoints:
-      self.checkpoints.append(self.trainer.checkpoint(
-        f'cycle-{self.completed_cycles}'
-        f'-epochs-{self.trainer.num_epochs}'
-        f'-steps-{self.trainer.num_steps}'))
+      self.checkpoints.append(
+        self.trainer.checkpoint(
+          f'cycle-{self.completed_cycles}'
+          f'-epochs-{self.trainer.num_epochs}'
+          f'-steps-{self.trainer.num_steps}',
+        ),
+      )
 
     self.cur_cycle_len *= self.cycle_mult
     self.cur_max_lr *= self.lr_mult
@@ -75,23 +79,27 @@ class SGDR(Callback):
     if self.cur_step >= self.cur_cycle_len:
       self.restart()
     else:
-      new_lr = cosine_anneal(self.cur_min_lr, self.cur_max_lr, self.cur_step,
-                             self.cur_cycle_len)
+      new_lr = cosine_anneal(
+        self.cur_min_lr,
+        self.cur_max_lr,
+        self.cur_step,
+        self.cur_cycle_len,
+      )
       self.update_lr(new_lr)
       self.cur_step += 1
 
 
 class LRRangeTest(Callback):
   def __init__(
-      self,
-      start_lr=.00001,
-      end_lr=1,
-      steps=500,
-      step=None,
-      log_freq=10,
-      plot_freq=100,
-      divergence_multiplier=4,
-      plot_path=None,
+    self,
+    start_lr=0.00001,
+    end_lr=1,
+    steps=500,
+    step=None,
+    log_freq=10,
+    plot_freq=100,
+    divergence_multiplier=4,
+    plot_path=None,
   ):
     super(LRRangeTest, self).__init__()
     self.checkpoint_path = None
@@ -115,17 +123,16 @@ class LRRangeTest(Callback):
     self.plot_freq = plot_freq
     self.log_freq = log_freq
 
-
     self.divergence_multiplier = divergence_multiplier
 
   def __repr__(self):
     return (
-      f"LRRangeTest (\n"
-      f"  min_lr: {self.start_lr}\n"
-      f"  max_lr: {self.end_lr}\n"
-      f"  step: {self.step}\n"
-      f"  steps: {self.steps}\n"
-      ")"
+      f'LRRangeTest (\n'
+      f'  min_lr: {self.start_lr}\n'
+      f'  max_lr: {self.end_lr}\n'
+      f'  step: {self.step}\n'
+      f'  steps: {self.steps}\n'
+      ')'
     )
 
   def on_train_start(self, trainer=None):
@@ -137,19 +144,23 @@ class LRRangeTest(Callback):
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
     self.losses.append(loss.item())
-    self.avg_loss = exp_moving_avg(self.losses[-1], self.avg_loss,
-                                   steps=len(self.losses))
+    self.avg_loss = exp_moving_avg(
+      self.losses[-1],
+      self.avg_loss,
+      steps=len(self.losses),
+    )
 
     if self.log_freq and len(self.lrs) % self.log_freq == 0:
-      print(f"lr: {self.lrs[-1]:.5f}  loss: {self.avg_loss:.5f}")
+      print(f'lr: {self.lrs[-1]:.5f}  loss: {self.avg_loss:.5f}')
 
     if self.plot_freq and len(self.lrs) % self.plot_freq == 0:
       self.plot()
 
     if self.min_loss is None:
       self.min_loss = self.avg_loss
-    elif (self.avg_loss > self.divergence_multiplier * self.min_loss) and \
-        len(self.lrs) > 50:
+    elif (self.avg_loss > self.divergence_multiplier * self.min_loss) and len(
+      self.lrs,
+    ) > 50:
       logging.info('Loss diverged, stopping LR Range Test')
       trainer.stop()
       return
@@ -163,7 +174,6 @@ class LRRangeTest(Callback):
     self.lrs.append(self.lrs[-1] + self.step)
     set_param(trainer.optimizer, 'lr', self.lrs[-1])
 
-
   def plot(self, **kwargs):
     plot_line(
       x=self.lrs,
@@ -172,7 +182,7 @@ class LRRangeTest(Callback):
       ylabel='loss',
       save=self.plot_path,
       show=not self.plot_path,
-      **kwargs
+      **kwargs,
     )
 
 
diff --git a/yann/callbacks/profile.py b/yann/callbacks/profile.py
index c35c6ea..3f1938b 100644
--- a/yann/callbacks/profile.py
+++ b/yann/callbacks/profile.py
@@ -2,12 +2,7 @@ from . import Callback
 
 
 class Profile(Callback):
-  def __init__(
-      self,
-      start_step=16,
-      stop_step=80,
-      **kwargs
-  ):
+  def __init__(self, start_step=16, stop_step=80, **kwargs):
     self.profiler = None
 
     self.start_step = start_step
@@ -16,10 +11,10 @@ class Profile(Callback):
 
     self.profiler_args = kwargs
 
-
   def on_step_start(self, index=None, **kwargs):
     if index == self.start_step:
       from torch.profiler import profile
+
       self.profiler = profile(**kwargs)
       self.profiler.start()
 
@@ -31,27 +26,26 @@ class Profile(Callback):
       self.save(root=trainer.paths.profile)
       self.disable()
 
-
   def save(self, root: 'pathlib.Path' = None):
-     self.profiler.export_chrome_trace(str(root / 'chrome_trace.json'))
+    self.profiler.export_chrome_trace(str(root / 'chrome_trace.json'))
 
-     try:
+    try:
       self.profiler.tensorboard_trace_handler(str(root / 'tensorboard'))
-     except:
-       pass
-
-     try:
-       self.profiler.export_stacks(
-         str(root / 'cpu.stacks'),
-         metric='self_cpu_time_total'
-       )
-     except:
-       pass
-
-     try:
-       self.profiler.export_stacks(
-         str(root / 'cuda.stacks'),
-         metric='self_cuda_time_total'
-       )
-     except:
-       pass
+    except:
+      pass
+
+    try:
+      self.profiler.export_stacks(
+        str(root / 'cpu.stacks'),
+        metric='self_cpu_time_total',
+      )
+    except:
+      pass
+
+    try:
+      self.profiler.export_stacks(
+        str(root / 'cuda.stacks'),
+        metric='self_cuda_time_total',
+      )
+    except:
+      pass
diff --git a/yann/callbacks/progbar.py b/yann/callbacks/progbar.py
index a14ab24..0411d73 100644
--- a/yann/callbacks/progbar.py
+++ b/yann/callbacks/progbar.py
@@ -1,4 +1,5 @@
 import yann.utils
+
 from .base import Callback
 
 
@@ -24,24 +25,21 @@ class ProgressBar(Callback):
     else:
       from tqdm import tqdm
 
-
     if trainer:
       try:
-        total = (len(trainer.dataset) if self.samples else len(trainer.loader))
+        total = len(trainer.dataset) if self.samples else len(trainer.loader)
       except:
         total = None
     else:
       total = None
 
     self.bar = tqdm(
-      desc=f"Epoch {epoch}",
+      desc=f'Epoch {epoch}',
       total=self.length or total,
-      unit='samples' if self.samples else 'batches'
+      unit='samples' if self.samples else 'batches',
     )
 
-  def on_epoch_end(
-    self, epoch=None, loss=None, metrics=None, trainer=None
-  ):
+  def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
     self.bar.close()
 
   def on_step_end(
@@ -51,7 +49,7 @@ class ProgressBar(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
     if self.samples:
       self.bar.update(len(inputs))
diff --git a/yann/callbacks/stop.py b/yann/callbacks/stop.py
index 824ca55..5a732b4 100644
--- a/yann/callbacks/stop.py
+++ b/yann/callbacks/stop.py
@@ -1,7 +1,7 @@
-from .base import Callback
-
 import torch
 
+from .base import Callback
+
 
 class EarlyStopping(Callback):
   def __init__(self):
@@ -10,7 +10,6 @@ class EarlyStopping(Callback):
 
 class StopOnNaN(Callback):
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
-    if torch.isnan(loss).any() \
-       or torch.isinf(loss).any():
+    if torch.isnan(loss).any() or torch.isinf(loss).any():
       print('NaN or Inf detected, stopping training')
-      trainer.stop()
\ No newline at end of file
+      trainer.stop()
diff --git a/yann/callbacks/tensorboard.py b/yann/callbacks/tensorboard.py
index e485537..c595d9c 100644
--- a/yann/callbacks/tensorboard.py
+++ b/yann/callbacks/tensorboard.py
@@ -1,15 +1,17 @@
-from pathlib import Path
 import logging
-from torch.utils.tensorboard import SummaryWriter
+from pathlib import Path
+
 from torch import nn
+from torch.utils.tensorboard import SummaryWriter
 
 from .base import Callback
 
-
 log = logging.getLogger(__name__)
 
+
 class Tensorboard(Callback):
   writer: SummaryWriter
+
   def __init__(self, root=None, trainer=None, writer=None):
     self.root = root
     self.trainer = trainer
@@ -28,11 +30,12 @@ class Tensorboard(Callback):
       self.writer.add_graph(
         val,
         kwargs.get('input'),
-        verbose=kwargs.get('verbose', False)
+        verbose=kwargs.get('verbose', False),
       )
 
   def show(self, root=None):
     from IPython import get_ipython
+
     ipython = get_ipython()
     ipython.magic('load_ext tensorboard')
 
@@ -68,7 +71,7 @@ class Tensorboard(Callback):
       try:
         self.writer.add_hparams(
           dict(self.trainer.params),
-          self.trainer.history.val_metrics.summary()
+          self.trainer.history.val_metrics.summary(),
         )
       except ValueError as e:
         log.error(e)
@@ -85,17 +88,31 @@ class Tensorboard(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
     self.writer.add_scalar('train/loss', loss, global_step=trainer.num_steps)
     for metric, values in trainer.history.metrics.items():
-      self.writer.add_scalar(f'train/{metric}', values[-1], global_step=len(values) - 1)
+      self.writer.add_scalar(
+        f'train/{metric}',
+        values[-1],
+        global_step=len(values) - 1,
+      )
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     for metric, values in trainer.history.val_metrics.items():
-      self.writer.add_scalar(f'validation/{metric}', values[-1], global_step=len(values) - 1)
+      self.writer.add_scalar(
+        f'validation/{metric}',
+        values[-1],
+        global_step=len(values) - 1,
+      )
 
   def sanitize_model(self, model):
     if isinstance(model, nn.DataParallel):
       return model.module
-    return model
\ No newline at end of file
+    return model
diff --git a/yann/callbacks/timing.py b/yann/callbacks/timing.py
index 25562de..d90f108 100644
--- a/yann/callbacks/timing.py
+++ b/yann/callbacks/timing.py
@@ -1,5 +1,4 @@
 import time
-from matplotlib import pylab as plt
 
 from .base import Callback
 
@@ -29,6 +28,8 @@ class Timing(Callback):
     return [s - e for (s, e) in zip(self.starts, [self.start_time, *self.ends])]
 
   def plot(self, start=0, end=None, scatter=False):
+    from matplotlib import pylab as plt
+
     end = end or len(self.starts)
 
     fig = plt.figure(figsize=(12, 4))
@@ -38,11 +39,15 @@ class Timing(Callback):
       plt.scatter(range(start, end), list(self.waits)[start:end], label='prep')
     else:
       plt.bar(range(start, end), list(self.waits)[start:end], label='prep')
-      plt.bar(range(start, end), list(self.times)[start:end],
-              bottom=list(self.waits)[start:end], label='step')
+      plt.bar(
+        range(start, end),
+        list(self.times)[start:end],
+        bottom=list(self.waits)[start:end],
+        label='step',
+      )
 
     plt.xlabel('step')
     plt.ylabel('seconds')
     plt.title('Train Run Timings')
     plt.legend()
-    plt.show()
\ No newline at end of file
+    plt.show()
diff --git a/yann/callbacks/unfreeze.py b/yann/callbacks/unfreeze.py
index 5310dd2..8e16ee6 100644
--- a/yann/callbacks/unfreeze.py
+++ b/yann/callbacks/unfreeze.py
@@ -1,14 +1,21 @@
 from typing import Dict
 
 import torch
-from yann.train import Trainer
+
 import yann
+from yann.train import Trainer
+
 from . import Callback
 
 
 class GradualUnfreezing(Callback):
   trainer: Trainer
-  def __init__(self, modules: Dict[int, torch.nn.Module], unfreeze=yann.unfreeze):
+
+  def __init__(
+    self,
+    modules: Dict[int, torch.nn.Module],
+    unfreeze=yann.unfreeze,
+  ):
     self.modules = modules
     self.unfreeze = unfreeze
 
@@ -25,10 +32,8 @@ class GradualUnfreezing(Callback):
 
     # clone param group variables to avoid missing keys (used by things like lr_schedulers)
     param_group = {
-      k: v for (k, v)
-      in self.trainer.optimizer.param_groups[0].items()
-      if k != 'params'
+      k: v for (k, v) in self.trainer.optimizer.param_groups[0].items() if k != 'params'
     }
     param_group['params'] = parameters
 
-    self.trainer.optimizer.add_param_group(param_group)
\ No newline at end of file
+    self.trainer.optimizer.add_param_group(param_group)
diff --git a/yann/callbacks/wandb.py b/yann/callbacks/wandb.py
index a594f7b..7c61873 100644
--- a/yann/callbacks/wandb.py
+++ b/yann/callbacks/wandb.py
@@ -14,14 +14,14 @@ class Wandb(Callback):
   run: Optional['wandb.wandb_sdk.wandb_run.Run']
 
   def __init__(
-      self,
-      project=None,
-      entity=None,
-      name=None,
-      watch_freq=0,
-      log_code=True,
-      batch_log_freq=10,
-      trackers=None,
+    self,
+    project=None,
+    entity=None,
+    name=None,
+    watch_freq=0,
+    log_code=True,
+    batch_log_freq=10,
+    trackers=None,
   ):
     self.client = wandb
     self.run = None
@@ -46,7 +46,7 @@ class Wandb(Callback):
         project=self.project,
         entity=self.entity,
         name=self.name or trainer.name,
-        config=dict(trainer.params) if trainer.params else {}
+        config=dict(trainer.params) if trainer.params else {},
       )
 
       if self.log_code is True:
@@ -61,7 +61,7 @@ class Wandb(Callback):
         models=trainer.model,
         log='all',
         log_graph=True,
-        log_freq=self.watch_freq
+        log_freq=self.watch_freq,
       )
 
     if self.trackers is None:
@@ -79,7 +79,7 @@ class Wandb(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
     if trainer.num_steps % self.batch_log_freq == 0:
       self.run.log({'train/loss': loss}, step=trainer.num_steps)
@@ -90,7 +90,13 @@ class Wandb(Callback):
         for track in self.trackers:
           self.run.log(track(trainer), step=trainer.num_steps)
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     for metric, values in trainer.history.val_metrics.items():
       self.run.log({f'validation/{metric}': values[-1]}, step=trainer.num_steps)
 
@@ -98,12 +104,9 @@ class Wandb(Callback):
     self.run.summary.update(trainer.summary)
     self.run.log({'epoch': trainer.num_epochs}, step=trainer.num_steps)
 
-
   def get_default_trackers(self, trainer=None):
-    if not trainer: return None
+    if not trainer:
+      return None
     import yann.train.track
 
-    return [
-      yann.train.track.OptimizerState()
-    ]
-
+    return [yann.train.track.OptimizerState()]
diff --git a/yann/cli.py b/yann/cli.py
index 89308ee..f3e6b10 100644
--- a/yann/cli.py
+++ b/yann/cli.py
@@ -18,20 +18,20 @@ def cli():
 @click.option('-cp', '--checkpoint')
 @click.option('-c', '--continue')
 def train(
-    name,
-    model,
-    dataset,
-    loss=None,
-    transform=None,
-    optimizer='SGD',
-    checkpoint=None,
-    lr=0.01,
-    momentum=.9,
-    epochs=10
+  name,
+  model,
+  dataset,
+  loss=None,
+  transform=None,
+  optimizer='SGD',
+  checkpoint=None,
+  lr=0.01,
+  momentum=0.9,
+  epochs=10,
 ):
   """Train model"""
-  from .train import Trainer
   from .callbacks import get_callbacks
+  from .train import Trainer
 
   t = Trainer(
     name=name,
@@ -40,7 +40,7 @@ def train(
     dataset=dataset,
     transform=transform,
     loss=loss,
-    callbacks=get_callbacks(interactive=False)
+    callbacks=get_callbacks(interactive=False),
   )
 
   if checkpoint:
@@ -137,17 +137,17 @@ def scaffold():
   raise NotImplementedError()
 
 
-
-
 @cli.command()
 @click.argument('src')
 @click.argument('dst')
 def convert(src: str, dst: str):
   import yann
+
   data = yann.load(src)
   print(f'loaded {type(data)}')
   yann.save(data, dst)
 
+
 def main():
   cli()
 
@@ -170,9 +170,11 @@ def preview(name: str):
     x = ds[i]
     print(x)
 
+
 @dataset.command()
 def list():
   print(yann.registry.dataset.print_tree())
 
+
 if __name__ == '__main__':
   cli()
diff --git a/yann/config/defaults.py b/yann/config/defaults.py
index 451103d..ecb38f3 100644
--- a/yann/config/defaults.py
+++ b/yann/config/defaults.py
@@ -1,6 +1,7 @@
-import torch
 from pathlib import Path
 
+import torch
+
 
 class default:
   root = Path('~/.yann/').expanduser()
@@ -30,4 +31,4 @@ class default:
   def dataset_root(cls, dataset):
     if hasattr(dataset, 'root'):
       return dataset.root
-    return str(cls.datasets_root / dataset.__name__)
\ No newline at end of file
+    return str(cls.datasets_root / dataset.__name__)
diff --git a/yann/config/registry.py b/yann/config/registry.py
index ed5d778..2686b0e 100644
--- a/yann/config/registry.py
+++ b/yann/config/registry.py
@@ -1,9 +1,8 @@
 import typing
-from typing import Union, Tuple, Dict, Any
-from collections import defaultdict, OrderedDict
-
+from collections import OrderedDict, defaultdict
 from functools import partial
 from itertools import chain
+from typing import Any, Dict, Tuple, Union
 
 
 def dedupe(items):
@@ -19,15 +18,11 @@ def pass_args(x, *args, **kwargs):
 
 
 def is_public(x):
-  return (hasattr(x, '__name__') and not x.__name__.startswith('_'))
+  return hasattr(x, '__name__') and not x.__name__.startswith('_')
 
 
 def is_public_callable(x):
-  return (
-      hasattr(x, '__name__')
-      and not x.__name__.startswith('_')
-      and callable(x)
-  )
+  return hasattr(x, '__name__') and not x.__name__.startswith('_') and callable(x)
 
 
 class default:
@@ -35,7 +30,7 @@ class default:
 
   @staticmethod
   def get_names(x):
-    return x.__name__,
+    return (x.__name__,)
 
 
 class RegistryError(Exception):
@@ -72,15 +67,15 @@ class Resolver:
     self.registry = registry
 
   def resolve(
-      self,
-      x: Union[Any, Tuple[Any, Dict]],
-      required=False,
-      validate=None,
-      instance=True,
-      types=None,
-      init=None,
-      args=None,
-      kwargs=None
+    self,
+    x: Union[Any, Tuple[Any, Dict]],
+    required=False,
+    validate=None,
+    instance=True,
+    types=None,
+    init=None,
+    args=None,
+    kwargs=None,
   ):
     """
 
@@ -123,13 +118,13 @@ class Resolver:
     if not required and x is None:
       return x
     elif required and x is None:
-      raise ResolutionError("Could not resolve to a value and was required")
+      raise ResolutionError('Could not resolve to a value and was required')
 
     if types and not isinstance(x, types):
       raise ResolutionError(
-        f"Failed to resolve {initial} to one of "
-        f"{' '.join(str(t) for t in types)}, "
-        f"got {x} instead of type {type(x)}"
+        f'Failed to resolve {initial} to one of '
+        f'{" ".join(str(t) for t in types)}, '
+        f'got {x} instead of type {type(x)}',
       )
 
     if validate and not validate(x):
@@ -138,9 +133,18 @@ class Resolver:
     return x
 
   def __call__(
-      self, x, *_args, required=False, validate=None,
-      instance=True, types=None, args=None, kwargs=None, init=None,
-      **_kwargs, ):
+    self,
+    x,
+    *_args,
+    required=False,
+    validate=None,
+    instance=True,
+    types=None,
+    args=None,
+    kwargs=None,
+    init=None,
+    **_kwargs,
+  ):
     return self.resolve(
       x,
       required=required,
@@ -149,7 +153,7 @@ class Resolver:
       types=types,
       args=args or _args,
       kwargs=kwargs or _kwargs,
-      init=init
+      init=init,
     )
 
   def __getattr__(self, name):
@@ -184,11 +188,12 @@ class Registry:
       return partial(self.register, name=x)
 
     if self.types and not (
-        issubclass(x, self.types) if isinstance(x, type)
-        else isinstance(x, self.types)):
+      issubclass(x, self.types) if isinstance(x, type) else isinstance(x, self.types)
+    ):
       raise RegistryError(
         f"Can't register an object of type {type(x)} in "
-        f"typed registry which expects one of {self.types}")
+        f'typed registry which expects one of {self.types}',
+      )
 
     r = Record(x, init=init)
 
@@ -256,18 +261,21 @@ class Registry:
 
     raise KeyError(
       f"Couldn't find key: '{item}', "
-      f"valid options include: {', '.join(self._records.keys())}")
+      f'valid options include: {", ".join(self._records.keys())}',
+    )
 
   def values(self):
-    return dedupe((
-      *(r.x for r in self._records.values()),
-      *chain(*(c.values() for c in self.public_subregistries()))
-    ))
+    return dedupe(
+      (
+        *(r.x for r in self._records.values()),
+        *chain(*(c.values() for c in self.public_subregistries())),
+      ),
+    )
 
   def items(self):
     return (
       *((k, r.x) for k, r in self._records.items()),
-      *chain(*(c.items() for c in self.public_subregistries()))
+      *chain(*(c.items() for c in self.public_subregistries())),
     )
 
   def keys(self):
@@ -277,14 +285,14 @@ class Registry:
     return len(self.values())
 
   def index(
-      self,
-      modules,
-      types=None,
-      get_names=None,
-      include=None,
-      exclude=None,
-      init=None,
-      include_private=False
+    self,
+    modules,
+    types=None,
+    get_names=None,
+    include=None,
+    exclude=None,
+    init=None,
+    include_private=False,
   ):
     """
     Indexes a module. If types are specified will only include entries of
@@ -306,8 +314,10 @@ class Registry:
           continue
         if types:
           if not (
-              isinstance(item, types) or
-              (isinstance(item, type)) and issubclass(item, types)):
+            isinstance(item, types)
+            or (isinstance(item, type))
+            and issubclass(item, types)
+          ):
             continue
 
         if include and not include(item):
@@ -335,11 +345,13 @@ class Registry:
   def print_tree(self, contents=True, indent=0):
     if not indent:
       print(
-        f'registry{" (Private - not resolvable from higher scopes)" if self.is_private else ""}')
+        f'registry{" (Private - not resolvable from higher scopes)" if self.is_private else ""}',
+      )
       indent += 2
     for name, registry in self._subregistries.items():
       print(
-        f"{' ' * indent}.{name} {' (Private - not resolvable from higher scopes)' if registry.is_private else ''}")
+        f'{" " * indent}.{name} {" (Private - not resolvable from higher scopes)" if registry.is_private else ""}',
+      )
       registry.print_tree(indent=indent + 2, contents=contents)
 
     if contents:
@@ -347,8 +359,8 @@ class Registry:
         if isinstance(record.x, partial) or not hasattr(record.x, '__module__'):
           details = str(record.x)
         else:
-          details = f"{record.x.__module__}.{record.x.__name__ if hasattr(record.x, '__name__') else record.x}"
-        print(f"{' ' * (indent + 2)}- {name}\t\t({details})")
+          details = f'{record.x.__module__}.{record.x.__name__ if hasattr(record.x, "__name__") else record.x}'
+        print(f'{" " * (indent + 2)}- {name}\t\t({details})')
 
   def __str__(self):
     return f"<Registry '{self.name}' ({len(self)} entries)>"
diff --git a/yann/config/setup.py b/yann/config/setup.py
index 7318801..4b1236c 100644
--- a/yann/config/setup.py
+++ b/yann/config/setup.py
@@ -1,10 +1,7 @@
 import torch
 
 from .defaults import default
-from .registry import Registry, pass_args, is_public_callable
-
-
-
+from .registry import Registry, is_public_callable, pass_args
 
 ## Configure Registry
 
@@ -12,16 +9,18 @@ registry = Registry()
 
 # Datasets
 import torchvision.datasets
-
 from torch.utils.data import Dataset
-from ..datasets import imagenette, voc, coco, food101
+
+from ..datasets import coco, food101, imagenette, voc
 
 registry.dataset.index(
   [torchvision.datasets, imagenette, voc, coco, food101],
   types=(Dataset,),
-  init=lambda D, root=None, download=True, **kwargs: \
-    D(root=str(root or default.dataset_root(D)),
-      download=download, **kwargs)
+  init=lambda D, root=None, download=True, **kwargs: D(
+    root=str(root or default.dataset_root(D)),
+    download=download,
+    **kwargs,
+  ),
 )
 
 
@@ -29,9 +28,9 @@ registry.dataset.index(
 registry.loss.index(
   torch.nn.modules.loss,
   types=(torch.nn.modules.loss._Loss,),
-  get_names=lambda x:
-  (x.__name__, x.__name__[:-len('Loss')]) if x.__name__.endswith('Loss') else (
-    x.__name__,)
+  get_names=lambda x: (x.__name__, x.__name__[: -len('Loss')])
+  if x.__name__.endswith('Loss')
+  else (x.__name__,),
 )
 
 import torch.nn.functional as F
@@ -39,50 +38,51 @@ import torch.nn.functional as F
 registry.loss.index(
   F,
   include=lambda x: hasattr(x, '__name__') and 'loss' in x.__name__.lower(),
-  get_names=lambda x: (x.__name__, x.__name__[:-len('_loss')]) if
-  x.__name__.endswith('_loss') else (x.__name__,)
+  get_names=lambda x: (x.__name__, x.__name__[: -len('_loss')])
+  if x.__name__.endswith('_loss')
+  else (x.__name__,),
 )
 
 from ..modules import loss
 
-registry.loss.update((
-  F.cross_entropy,
-  F.binary_cross_entropy,
-  F.binary_cross_entropy_with_logits,
-  loss.soft_target_cross_entropy,
-  loss.SoftTargetCrossEntropyLoss
-))
+registry.loss.update(
+  (
+    F.cross_entropy,
+    F.binary_cross_entropy,
+    F.binary_cross_entropy_with_logits,
+    loss.soft_target_cross_entropy,
+    loss.SoftTargetCrossEntropyLoss,
+  ),
+)
 
 # Optimizers
 from torch.optim import optimizer
 
-registry.optimizer.index(
-  torch.optim,
-  types=(optimizer.Optimizer,)
-)
+registry.optimizer.index(torch.optim, types=(optimizer.Optimizer,))
 
-registry.optimizer['SGD'].init = \
-  lambda SGD, params, lr=.01, momentum=0, weight_decay=0: \
-    SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)
+registry.optimizer['SGD'].init = (
+  lambda SGD, params, lr=0.01, momentum=0, weight_decay=0: SGD(
+    params,
+    lr=lr,
+    momentum=momentum,
+    weight_decay=weight_decay,
+  )
+)
 
 # LR Schedulers
 from torch.optim import lr_scheduler
 
-registry.lr_scheduler.index(
-  lr_scheduler,
-  types=(lr_scheduler._LRScheduler,)
-)
+registry.lr_scheduler.index(lr_scheduler, types=(lr_scheduler._LRScheduler,))
 # ReduceLROnPlateau subclasses object
 registry.lr_scheduler.register(lr_scheduler.ReduceLROnPlateau)
 
 # Models
 from torchvision import models
 
-
 registry.model.torchvision.index(
   models,
   init=pass_args,
-  include=is_public_callable
+  include=is_public_callable,
 )
 
 # NOTE: moved to yann.contrib.pretrainedmodels
@@ -99,24 +99,18 @@ registry.model.torchvision.index(
 #   )
 
 from .. import metrics
-registry.metric.update((
-  metrics.accuracy,
-  metrics.average_precision,
-  metrics.average_precision_at_k,
-  metrics.coverage_error,
-))
-
-registry.metric.register(
-  metrics.top_3_accuracy,
-  name='top_3_accuracy'
-)
 
-registry.metric.register(
-  metrics.top_5_accuracy,
-  name='top_5_accuracy'
+registry.metric.update(
+  (
+    metrics.accuracy,
+    metrics.average_precision,
+    metrics.average_precision_at_k,
+    metrics.coverage_error,
+  ),
 )
 
-registry.metric.register(
-  metrics.top_10_accuracy,
-  name='top_10_accuracy'
-)
\ No newline at end of file
+registry.metric.register(metrics.top_3_accuracy, name='top_3_accuracy')
+
+registry.metric.register(metrics.top_5_accuracy, name='top_5_accuracy')
+
+registry.metric.register(metrics.top_10_accuracy, name='top_10_accuracy')
diff --git a/yann/contrib/gcp.py b/yann/contrib/gcp.py
index fac206f..8b2ed6a 100644
--- a/yann/contrib/gcp.py
+++ b/yann/contrib/gcp.py
@@ -4,7 +4,6 @@ from ..callbacks.base import Callback
 from ..utils.bash import run
 
 
-
 def gcloud(command):
   return run(['gcloud', command])
 
@@ -14,38 +13,42 @@ def gsutil(command):
 
 
 def args(*flags, hyphenate=True, **kwargs):
-  return ' \ \n'.join((
-    *(f'--{str(n).replace("_", "-") if hyphenate else n}' for n in flags),
-    *(x for x in (
-      f'--{str(k).replace("_", "-") if hyphenate else k}={v}'
-      if not (v is True or v is False)
-      else (f'--{str(k).replace("_", "-") if hyphenate else k}' if v else '')
-      for k, v in kwargs.items() if v is not None)
-      if x)
-  ))
-
-
-def start_instance(
-    name,
-    zone=None,
-    preemptible=True):
-  command = (
-    f"""gcloud compute instances create {name} \
+  return ' \ \n'.join(
+    (
+      *(f'--{str(n).replace("_", "-") if hyphenate else n}' for n in flags),
+      *(
+        x
+        for x in (
+          f'--{str(k).replace("_", "-") if hyphenate else k}={v}'
+          if not (v is True or v is False)
+          else (f'--{str(k).replace("_", "-") if hyphenate else k}' if v else '')
+          for k, v in kwargs.items()
+          if v is not None
+        )
+        if x
+      ),
+    ),
+  )
+
+
+def start_instance(name, zone=None, preemptible=True):
+  command = f"""gcloud compute instances create {name} \
           {
     args(
       zone=zone,
       preemptible=preemptible,
       maintenance_policy='foo',
     )
-    }
+  }
 
     """
-  )
 
   return run(command)
 
 
-def start_dl_instance(name, ):
+def start_dl_instance(
+  name,
+):
   pass
 
 
@@ -64,27 +67,14 @@ def kill_instance():
 def shutdown():
   return run('sudo shutdown -h now')
 
+
 def gcp_sync(src, dst, exclude=None):
   if exclude:
-    return subprocess.call([
-      'gsutil',
-      '-m',
-      'rsync',
-      '-r',
-      '-x',
-      exclude,
-      src,
-      dst
-    ])
+    return subprocess.call(
+      ['gsutil', '-m', 'rsync', '-r', '-x', exclude, src, dst],
+    )
   else:
-    return subprocess.call([
-      'gsutil',
-      '-m',
-      'rsync',
-      '-r',
-      src,
-      dst
-    ])
+    return subprocess.call(['gsutil', '-m', 'rsync', '-r', src, dst])
 
 
 class SyncCallback(Callback):
diff --git a/yann/contrib/pretrained.py b/yann/contrib/pretrained.py
index 08a41fd..b8f11de 100644
--- a/yann/contrib/pretrained.py
+++ b/yann/contrib/pretrained.py
@@ -1,27 +1,30 @@
 from torch.nn import AdaptiveAvgPool2d
-from ..data.containers import Outputs, Inputs
+
+from ..data.containers import Inputs, Outputs
 from ..models import Model
 
 
 def register_models():
   import logging
+
   from .. import registry
-  from ..config.registry import pass_args, is_public_callable
+  from ..config.registry import is_public_callable, pass_args
 
   try:
     import pretrainedmodels.models
   except ImportError:
     logging.warn(
       "Couldn't register pretrainedmodels models because it's not "
-      "installed\n to install run `pip install pretrainedmodels`"
+      'installed\n to install run `pip install pretrainedmodels`',
     )
   else:
     registry.model.pretrainedmodels.index(
       pretrainedmodels.models,
       init=pass_args,
-      include=is_public_callable
+      include=is_public_callable,
     )
 
+
 # auto register the models on import of this module
 # NOTE:
 #   prerainedmodels has some top level model instantiation
@@ -41,7 +44,7 @@ class PretrainedModel(Model):
     return Outputs(
       embeddings=embeddings,
       logits=logits,
-      activations=activations
+      activations=activations,
     )
 
 
@@ -80,7 +83,7 @@ class PretrainedModelWrapper(Model):
     return Outputs(
       embeddings=embeddings,
       logits=logits,
-      activations=activations
+      activations=activations,
     )
 
 
diff --git a/yann/contrib/slack.py b/yann/contrib/slack.py
index 663d4c7..d18d957 100644
--- a/yann/contrib/slack.py
+++ b/yann/contrib/slack.py
@@ -1,28 +1,42 @@
+import json
 import urllib
 import urllib.request
-import json
 
 from ..callbacks.base import Callback
 
+
 def post(url, data):
   data = json.dumps(data).encode('utf8')
   req = urllib.request.Request(
-    url, data=data, headers={'content-type': 'application/json'})
+    url,
+    data=data,
+    headers={'content-type': 'application/json'},
+  )
   return urllib.request.urlopen(req)
 
 
 DEFAULT_CHANNEL = '#training'
 
-def send(text, attachments=None, channel=None, username=None, icon=None,
-         url=None):
+
+def send(
+  text,
+  attachments=None,
+  channel=None,
+  username=None,
+  icon=None,
+  url=None,
+):
   """https://api.slack.com/docs/message-attachments"""
-  return post(url, {
-    'text': text,
-    'channel': channel or DEFAULT_CHANNEL,
-    'username': username,
-    'icon_emoji': icon and (icon if icon.startswith(':') else f':{icon}:'),
-    'attachments': attachments,
-  })
+  return post(
+    url,
+    {
+      'text': text,
+      'channel': channel or DEFAULT_CHANNEL,
+      'username': username,
+      'icon_emoji': icon and (icon if icon.startswith(':') else f':{icon}:'),
+      'attachments': attachments,
+    },
+  )
 
 
 def atch(title=None, text=None, fields=None, color=None, **kwargs):
@@ -31,10 +45,10 @@ def atch(title=None, text=None, fields=None, color=None, **kwargs):
     text=text,
     fields=[
       *(fields or {}),
-      *({'title': k, 'value': v} for k, v in kwargs.items())],
-    color=color)
-
-
+      *({'title': k, 'value': v} for k, v in kwargs.items()),
+    ],
+    color=color,
+  )
 
 
 class Slack(Callback):
@@ -52,7 +66,7 @@ class Slack(Callback):
       channel=self.channel,
       username=self.username,
       url=self.url,
-      **kwargs
+      **kwargs,
     )
 
   def on_train_start(self, trainer=None):
@@ -60,15 +74,20 @@ class Slack(Callback):
       text='Starting train run',
       attachments=[
         atch(experiment=trainer.name, text=trainer.description),
-        atch('Configuration', f"```{trainer}```", color='good'),
-      ]
+        atch('Configuration', f'```{trainer}```', color='good'),
+      ],
     )
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     if self.validation:
       self.send(
-        text=f'Completed epoch {trainer.num_epochs} with loss: {loss.item()}'
+        text=f'Completed epoch {trainer.num_epochs} with loss: {loss.item()}',
       )
 
   def on_error(self, error, trainer=None):
@@ -77,9 +96,9 @@ class Slack(Callback):
       attachments=[
         atch(experiment=trainer.name),
         atch(epoch=trainer.num_epochs + 1),
-        atch('Exception', f"```{str(error)}```", color='danger'),
-      ]
+        atch('Exception', f'```{str(error)}```', color='danger'),
+      ],
     )
 
   def on_train_end(self, trainer=None):
-    self.send(text='Train run completed')
\ No newline at end of file
+    self.send(text='Train run completed')
diff --git a/yann/cuda.py b/yann/cuda.py
index 5981060..e258786 100644
--- a/yann/cuda.py
+++ b/yann/cuda.py
@@ -1,7 +1,6 @@
 import torch
 
 
-
 def device_info(d=None):
   d = d or torch.cuda.current_device()
   return {
@@ -12,4 +11,4 @@ def device_info(d=None):
   }
 
 
-sync = torch.cuda.synchronize
\ No newline at end of file
+sync = torch.cuda.synchronize
diff --git a/yann/data/__init__.py b/yann/data/__init__.py
index 7ec6aa3..69e32ce 100644
--- a/yann/data/__init__.py
+++ b/yann/data/__init__.py
@@ -1,10 +1,12 @@
-import torch
 import types
 
+import torch
+
+from . import place
 from .classes import Classes
 from .loaders import TransformLoader
 from .transform import Transformer
-from . import place
+
 
 def get_name(x):
   if hasattr(x, 'name'):
@@ -27,6 +29,7 @@ def batches(*tensors, size=32, shuffle=False, order=None):
   if len(tensors) == 1 and isinstance(tensors[0], str):
     # assume a registered dataset name was passed (like batches('MNIST'))
     import yann
+
     tensors = (yann.resolve.dataset(tensors[0]),)
   if shuffle:
     order = torch.randperm(len(tensors[0]))
@@ -34,17 +37,17 @@ def batches(*tensors, size=32, shuffle=False, order=None):
   if len(tensors) == 1:
     for i in range(0, len(tensors[0]), size):
       if order is not None:
-        indices = order[i:i+size]
+        indices = order[i : i + size]
         yield tensors[0][indices]
       else:
-        yield tensors[0][i:i+size]
+        yield tensors[0][i : i + size]
   else:
     for i in range(0, len(tensors[0]), size):
       if order is not None:
-        indices = order[i:i+size]
+        indices = order[i : i + size]
         yield tuple(t[indices] for t in tensors)
       else:
-        yield tuple(t[i:i+size] for t in tensors)
+        yield tuple(t[i : i + size] for t in tensors)
 
 
 def unbatch(batches):
@@ -61,7 +64,7 @@ def chunk(sequence, size=32):
         batch = []
   else:
     for i in range(0, len(sequence), size):
-      yield sequence[i:i+size]
+      yield sequence[i : i + size]
 
 
 def loop(items):
@@ -72,10 +75,11 @@ def loop(items):
 def shuffle(*sequences):
   order = torch.randperm(len(sequences[0]))
   return (
-     [s[i] for i in order] if isinstance(s, (tuple, list)) else s[order]
-     for s in sequences
+    [s[i] for i in order] if isinstance(s, (tuple, list)) else s[order]
+    for s in sequences
   )
 
+
 def flatten(x, out=None, prefix='', sep='.'):
   """
   Flatten nested dict
@@ -84,10 +88,15 @@ def flatten(x, out=None, prefix='', sep='.'):
 
   if isinstance(x, dict):
     for k in x:
-      flatten(x[k], out=out, prefix=f"{prefix}{sep if prefix else ''}{k}", sep=sep)
+      flatten(
+        x[k],
+        out=out,
+        prefix=f'{prefix}{sep if prefix else ""}{k}',
+        sep=sep,
+      )
   elif isinstance(x, (list, tuple)):
     for k, v in enumerate(x):
-      flatten(k, out=out, prefix=f"{prefix}{sep if prefix else ''}{k}", sep=sep)
+      flatten(k, out=out, prefix=f'{prefix}{sep if prefix else ""}{k}', sep=sep)
   else:
     out[prefix] = x
 
@@ -96,8 +105,9 @@ def flatten(x, out=None, prefix='', sep='.'):
 
 def print_tree(root, indent=4):
   from pathlib import Path
+
   root = Path(root)
   print(f'{root}')
   for path in sorted(root.rglob('*')):
     depth = len(path.relative_to(root).parts)
-    print(f'{" " * (depth * indent)} {path.name}')
\ No newline at end of file
+    print(f'{" " * (depth * indent)} {path.name}')
diff --git a/yann/data/batch.py b/yann/data/batch.py
index e8d482d..e1eb1fe 100644
--- a/yann/data/batch.py
+++ b/yann/data/batch.py
@@ -29,7 +29,7 @@ class Batch:
       self.outputs,
       self.losses,
       device=device,
-      **kwargs
+      **kwargs,
     )
     return self
 
diff --git a/yann/data/classes.py b/yann/data/classes.py
index f60ed3c..c37ddd4 100644
--- a/yann/data/classes.py
+++ b/yann/data/classes.py
@@ -1,6 +1,7 @@
-import numpy as np
 from collections import Counter
 
+import numpy as np
+
 # from enum import Enum
 #
 # class Encoding(Enum):
@@ -8,6 +9,7 @@ from collections import Counter
 #   one_hot = 'one_hot'
 #   normalized_one_hot = 'normalized_one_hot'
 
+
 class TargetTransformer:
   def encode(self, x, many=True):
     pass
@@ -30,18 +32,14 @@ class TargetTransformer:
 
 
 class Classes(TargetTransformer):
-  valid_encodings = {
-    'index',
-    'one_hot',
-    'normalized_one_hot'
-  }
+  valid_encodings = {'index', 'one_hot', 'normalized_one_hot'}
 
   def __init__(
-      self,
-      names=None,
-      meta=None,
-      counts=None,
-      default_encoding='index',
+    self,
+    names=None,
+    meta=None,
+    counts=None,
+    default_encoding='index',
   ):
     if names:
       self.names = list(names)
@@ -53,13 +51,14 @@ class Classes(TargetTransformer):
       raise ValueError('At least one of names, counts or meta must be defined')
     self.indices = {c: i for i, c in enumerate(self.names)}
     self.meta = meta
-    
+
     self.counts = counts
 
     self.dtype = 'float32'
 
-    assert default_encoding in self.valid_encodings, \
+    assert default_encoding in self.valid_encodings, (
       f'default_encoding must be one of {self.valid_encodings}, got {default_encoding}'
+    )
     self.default_encoding = default_encoding
 
   def weights(self, list=True, mode='multiclass', normalize=True):
@@ -71,7 +70,7 @@ class Classes(TargetTransformer):
         return weights
     else:
       raise NotImplementedError(
-        'Weights can not be determined unless `counts` are set'
+        'Weights can not be determined unless `counts` are set',
       )
 
   @classmethod
@@ -82,10 +81,7 @@ class Classes(TargetTransformer):
         counts[l] += 1
       else:
         counts.update(l)
-    return cls(
-      counts=counts,
-      **kwargs
-    )
+    return cls(counts=counts, **kwargs)
 
   @classmethod
   def ordered(cls, num, **kwargs):
@@ -95,12 +91,12 @@ class Classes(TargetTransformer):
     c = min(len(self.names) // 2, 3)
 
     return (
-      f"Classes(\n" 
-      f"  count={len(self)},\n" 
-      f"  default_encoding={self.default_encoding}\n"
-      f"  names=[{', '.join([str(x) for x in self.names[:c]])}, ..., {', '.join([str(x) for x in self.names[-c:]])}]\n"
+      f'Classes(\n'
+      f'  count={len(self)},\n'
+      f'  default_encoding={self.default_encoding}\n'
+      f'  names=[{", ".join([str(x) for x in self.names[:c]])}, ..., {", ".join([str(x) for x in self.names[-c:]])}]\n'
       # f"  encoded={self.encode(self.names[:c])}, ..., {self.encode(self.names[-c:])}\n"
-      f")"
+      f')'
     )
 
   def state_dict(self):
@@ -108,7 +104,7 @@ class Classes(TargetTransformer):
       'names': self.names,
       'meta': self.meta,
       'default_encoding': self.default_encoding,
-      'counts': self.counts
+      'counts': self.counts,
     }
 
   def load_state_dict(self, data):
@@ -142,7 +138,8 @@ class Classes(TargetTransformer):
 
   def decode(self, encoded, encoding=None):
     return getattr(self, (encoding or self.default_encoding) + '_decode')(
-      encoded)
+      encoded,
+    )
 
   def index_encode(self, classes):
     if isinstance(classes, (str, int)):
@@ -178,7 +175,8 @@ class Classes(TargetTransformer):
   #     raise NotImplementedError("truncate not supported without counts")
   #   pass
 
-def smooth(y, eps=.1, num_classes=None):
+
+def smooth(y, eps=0.1, num_classes=None):
   if not num_classes:
     if len(y.shape) == 1:
       num_classes = len(y)
@@ -187,7 +185,12 @@ def smooth(y, eps=.1, num_classes=None):
   return y * (1 - eps) + eps * (1.0 / num_classes)
 
 
-def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num_samples=None):
+def get_class_weights(
+  class_counts: dict,
+  mode='multiclass',
+  normalize=True,
+  num_samples=None,
+):
   """
   Args:
     class_counts: dict mapping from class to count
@@ -199,9 +202,7 @@ def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num
   """
   if mode == 'multiclass':
     num_samples = num_samples or sum(class_counts.values())
-    weights = {
-      k: num_samples / count for k, count in class_counts.items()
-    }
+    weights = {k: num_samples / count for k, count in class_counts.items()}
     if normalize:
       scale = len(weights) / sum(weights.values())
       return {k: w * scale for k, w in weights.items()}
@@ -221,5 +222,5 @@ def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num
   else:
     raise ValueError(
       f'''Unsupported mode, got "{mode}", expected one of '''
-      '''multiclass, multilabel, binary'''
-    )
\ No newline at end of file
+      """multiclass, multilabel, binary""",
+    )
diff --git a/yann/data/collate.py b/yann/data/collate.py
index d21de64..ae483f2 100644
--- a/yann/data/collate.py
+++ b/yann/data/collate.py
@@ -25,13 +25,11 @@ class PadCollate:
 class FilterCollate:
   def __init__(
     self,
-      filter=None,
-      value=None,
-      collate=default_collate,
+    filter=None,
+    value=None,
+    collate=default_collate,
   ):
-    self.filter = filter or (
-      lambda items: [it for it in items if it is not value]
-    )
+    self.filter = filter or (lambda items: [it for it in items if it is not value])
     self.collate = collate
 
   def __call__(self, batch):
@@ -61,21 +59,16 @@ def image_collate(batch, memory_format=torch.contiguous_format):
 
   for i, img in enumerate(images):
     nump_array = np.asarray(img, dtype=np.uint8)
-    if (nump_array.ndim < 3):
+    if nump_array.ndim < 3:
       nump_array = np.expand_dims(nump_array, axis=-1)
     nump_array = np.rollaxis(nump_array, 2)
     tensor[i] += torch.from_numpy(nump_array)
   return tensor, targets
 
 
-
 class KeyCollate:
   def __init__(self, *keys):
     self.keys = keys
 
   def __call__(self, samples):
-    return tuple(
-      torch.stack(
-        [s[k] for s in samples]
-      ) for k in self.keys
-    )
\ No newline at end of file
+    return tuple(torch.stack([s[k] for s in samples]) for k in self.keys)
diff --git a/yann/data/collection.py b/yann/data/collection.py
index 1628a97..15e3944 100644
--- a/yann/data/collection.py
+++ b/yann/data/collection.py
@@ -1,4 +1,4 @@
-from collections import defaultdict, Counter
+from collections import Counter, defaultdict
 from itertools import chain
 
 
@@ -25,7 +25,6 @@ def count(*args):
   return Counter(chain(*args))
 
 
-
 class Collection:
   def __init__(self, items):
     self.items = items
@@ -47,9 +46,7 @@ class Collection:
       yield from (tuple(getattr(x, a) for a in attrs) for x in self.items)
 
   def filter(self, condition):
-    return Collection(
-      x for x in self if condition(x)
-    )
+    return Collection(x for x in self if condition(x))
 
   def map(self, f):
     return Collection(f(x) for x in self)
@@ -59,18 +56,14 @@ class Collection:
       return sorted(
         self.items,
         key=lambda x: tuple(getattr(x, p) for p in props),
-        reverse=reverse
+        reverse=reverse,
       )
 
-    return sorted(
-      self.items,
-      key=key,
-      reverse=reverse
-    )
+    return sorted(self.items, key=key, reverse=reverse)
 
   def __getattr__(self, name: str):
     if name.startswith('by_unique_'):
-      x = by(self.items, name[len('by_unique_'):], unique=True)
+      x = by(self.items, name[len('by_unique_') :], unique=True)
       setattr(self, name, x)
       return x
 
@@ -80,13 +73,13 @@ class Collection:
       return x
 
     if name.endswith('_counts'):
-      attr = name[:-len('_counts')]
+      attr = name[: -len('_counts')]
       x = count(getattr(x, attr) for x in self.items)
       setattr(self, name, x)
       return x
 
     if name.endswith('_set'):
-      attr = name[:-len('_set')]
+      attr = name[: -len('_set')]
       x = set(getattr(x, attr) for x in self.items)
       setattr(self, name, x)
       return x
@@ -98,4 +91,3 @@ class Collection:
       return x
 
     raise AttributeError(name)
-
diff --git a/yann/data/containers.py b/yann/data/containers.py
index 2045cda..35f8e4b 100644
--- a/yann/data/containers.py
+++ b/yann/data/containers.py
@@ -1,13 +1,10 @@
-from collections import OrderedDict
-from collections import abc
+from collections import OrderedDict, abc
 from typing import List
 
 
 class Container(abc.MutableMapping):
   def __init__(self, *args, **kwargs):
-    items = OrderedDict(
-      ('_arg' + str(n), v) for n, v in enumerate(args)
-    )
+    items = OrderedDict(('_arg' + str(n), v) for n, v in enumerate(args))
     items.update(kwargs)
 
     self.__dict__.update(items)
@@ -62,4 +59,3 @@ class Samples:
 
   def __iter__(self):
     return (*self.inputs, *self.targets)
-
diff --git a/yann/data/images.py b/yann/data/images.py
index 81373ae..04f9240 100644
--- a/yann/data/images.py
+++ b/yann/data/images.py
@@ -1,6 +1,7 @@
-from PIL import Image
 import io
 
+from PIL import Image
+
 
 def image_to_bytes(image: Image.Image, format='jpeg'):
   buff = io.BytesIO()
@@ -14,4 +15,5 @@ def image_from_bytes(buffer):
 
 def enable_loading_truncated_images():
   from PIL import ImageFile
-  ImageFile.LOAD_TRUNCATED_IMAGES = True
\ No newline at end of file
+
+  ImageFile.LOAD_TRUNCATED_IMAGES = True
diff --git a/yann/data/io/__init__.py b/yann/data/io/__init__.py
index b84d094..5f90d5b 100644
--- a/yann/data/io/__init__.py
+++ b/yann/data/io/__init__.py
@@ -1,10 +1,10 @@
+import csv
+import gzip
 import json
 import os
 import pickle as pkl
 import tarfile
 from collections import namedtuple
-import csv
-import gzip
 from pathlib import Path
 from typing import Union
 
@@ -13,11 +13,18 @@ import torch
 
 class Loader:
   """
-    gs://bucket/file.th
-    ./foo/**/*.jpg
+  gs://bucket/file.th
+  ./foo/**/*.jpg
   """
 
-  def __call__(self, path, format=None, deserialize=None, filesystem=None, **kwargs):
+  def __call__(
+    self,
+    path,
+    format=None,
+    deserialize=None,
+    filesystem=None,
+    **kwargs,
+  ):
     path = Path(path)
     format = format or path.suffix[1:]
     if hasattr(self, format):
@@ -35,23 +42,28 @@ class Loader:
 
   def parquet(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_parquet(path, **kwargs)
 
   def csv(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_csv(path, **kwargs)
 
   def tsv(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_csv(path, **kwargs)
 
   def yaml(self, path, **kwargs):
     import yaml
+
     with open(path, 'r') as f:
       return yaml.load(f, yaml.SafeLoader)
 
   def image(self, path, **kwargs):
     import PIL.Image
+
     return PIL.Image.open(path)
 
   png = image
@@ -68,8 +80,10 @@ load = Loader()
 
 def to_pyarrow_table(x):
   import pyarrow as pa
+
   try:
     import pandas as pd
+
     if isinstance(x, pd.DataFrame):
       x = pa.Table.from_pandas(x)
   except ImportError:
@@ -80,9 +94,16 @@ def to_pyarrow_table(x):
 
   return x
 
+
 class Saver:
   def __call__(
-    self, x, path, format=None, serialize=None, filesystem=None, **kwargs
+    self,
+    x,
+    path,
+    format=None,
+    serialize=None,
+    filesystem=None,
+    **kwargs,
   ):
     path = Path(path)
     format = format or path.suffix[1:]
@@ -102,28 +123,33 @@ class Saver:
 
   def yaml(self, x, path, **kwargs):
     import yaml
+
     with open(path, 'w') as f:
       yaml.dump(x, f, sort_keys=False)
 
   def csv(self, x, path, **kwargs):
     import pyarrow.csv as csv
+
     x = to_pyarrow_table(x)
     csv.write_csv(x, path, **kwargs)
 
   def parquet(
-      self, x: Union['pandas.Dataframe', 'pyarrow.Table'],
-      path,
-      **kwargs
+    self,
+    x: Union['pandas.Dataframe', 'pyarrow.Table'],
+    path,
+    **kwargs,
   ):
-    import pyarrow.parquet as pq
     import pyarrow as pa
+    import pyarrow.parquet as pq
 
     x = to_pyarrow_table(x)
 
     if isinstance(x, pa.Table):
       pq.write_table(x, path, **kwargs)
     else:
-      raise ValueError(f'Unsupported type {type(x)} expected pandas.Dataframe or pyarrow.Table')
+      raise ValueError(
+        f'Unsupported type {type(x)} expected pandas.Dataframe or pyarrow.Table',
+      )
 
   def pickle(self, x, path, **kwargs):
     return save_pickle(x, path, **kwargs)
@@ -133,6 +159,7 @@ class Saver:
   pt = th
   pth = th
 
+
 save = Saver()
 
 
@@ -187,8 +214,9 @@ def untar(path):
 
 def unzip(zip, dest):
   import zipfile
+
   with zipfile.ZipFile(zip, 'r') as f:
-      f.extractall(dest)
+    f.extractall(dest)
 
 
 def iter_csv(path, header=True, tuples=True, sep=',', quote='"', **kwargs):
diff --git a/yann/data/io/download.py b/yann/data/io/download.py
index d380037..8a2a578 100644
--- a/yann/data/io/download.py
+++ b/yann/data/io/download.py
@@ -1,9 +1,9 @@
+import os
+import pathlib
 import urllib.request
 from concurrent import futures
-import pathlib
 from pathlib import Path
 from urllib.parse import urlparse
-import os
 
 from ...utils import progress
 
@@ -13,7 +13,7 @@ class CachedExecutor:
     self._executor = futures.ThreadPoolExecutor(max_workers=workers)
 
     self.pending = {}  # key => future
-    self.results = {} # key => local path
+    self.results = {}  # key => local path
     self.errors = {}  # key => error
 
     self.error_callbacks = []
@@ -88,7 +88,7 @@ class CachedExecutor:
   def enqueue(self, key, *args, **kwargs):
     if key in self.results or key in self.pending:
       return
-    return self.submit(key,  *args, **kwargs)
+    return self.submit(key, *args, **kwargs)
 
   def get(self, key):
     if key in self.results:
@@ -101,12 +101,8 @@ class CachedExecutor:
   def __getitem__(self, key):
     return self.get(key)
 
-  def __contains__(self,key):
-    return (
-      key in self.results
-      or key in self.errors
-      or key in self.pending
-    )
+  def __contains__(self, key):
+    return key in self.results or key in self.errors or key in self.pending
 
   def __delitem__(self, key):
     self.results.pop(key)
@@ -115,6 +111,7 @@ class CachedExecutor:
     if p:
       p.cancel()
 
+
 class Downloader(CachedExecutor):
   def __init__(self, local_root='./', workers=8):
     super(Downloader, self).__init__(workers=workers)
@@ -141,7 +138,7 @@ def download(url, dest=None, skip_existing=True, nest=True, root='./'):
   """
   if not dest:
     root = os.path.abspath(root)
-    dest = (urlparse(url).path if nest else os.path.basename(urlparse(url).path))
+    dest = urlparse(url).path if nest else os.path.basename(urlparse(url).path)
     dest = os.path.join(root, dest[1:] if dest[0] == '/' else dest)
   elif hasattr(dest, '__call__'):
     dest = dest(url)
@@ -153,8 +150,14 @@ def download(url, dest=None, skip_existing=True, nest=True, root='./'):
   return urllib.request.urlretrieve(url, dest)
 
 
-def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
-                  max_workers=12):
+def download_urls(
+  urls,
+  dest=None,
+  skip_existing=True,
+  nest=True,
+  root='./',
+  max_workers=12,
+):
   results, errors = [], []
   with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
     queued_futures = {
@@ -164,7 +167,7 @@ def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
         dest=dest,
         skip_existing=skip_existing,
         nest=nest,
-        root=root
+        root=root,
       ): url
       for url in urls
     }
@@ -173,4 +176,4 @@ def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
         results.append(f.result())
       except Exception as e:
         errors.append((queued_futures[f], e, f))
-  return results, errors
\ No newline at end of file
+  return results, errors
diff --git a/yann/data/loaders.py b/yann/data/loaders.py
index 83f5b05..80037fe 100644
--- a/yann/data/loaders.py
+++ b/yann/data/loaders.py
@@ -1,9 +1,16 @@
-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset
-from typing import Union, Iterable, Optional, Callable
+from typing import Callable, Iterable, Optional, Union
+
+from torch.utils.data import (
+  DataLoader,
+  Dataset,
+  RandomSampler,
+  SequentialSampler,
+)
 
-from ..datasets import TransformDataset
 import yann
 
+from ..datasets import TransformDataset
+
 
 class LoopedDataLoader(DataLoader):
   """
@@ -14,6 +21,7 @@ class LoopedDataLoader(DataLoader):
 
   # might be fixed here https://github.com/pytorch/pytorch/pull/35795
   """
+
   def __init__(self, *args, **kwargs):
     super(DataLoader, self).__init__(*args, **kwargs)
     self.__initialized = False
@@ -40,14 +48,16 @@ class LoopSampler:
 
 class TransformLoader(DataLoader):
   def __init__(self, dataset, transform, **kwargs):
-    super(TransformLoader, self
-         ).__init__(TransformDataset(dataset, transform), **kwargs)
+    super(TransformLoader, self).__init__(
+      TransformDataset(dataset, transform),
+      **kwargs,
+    )
 
 
 def loader(
   data: Union[str, Iterable, Dataset, DataLoader],
   transform: Optional[Callable] = None,
-  **kwargs
+  **kwargs,
 ):
   """instantiate a loader from a dataset name, dataset or loader"""
   if isinstance(data, DataLoader):
diff --git a/yann/data/metrics.py b/yann/data/metrics.py
index 8d2a3e3..719f726 100644
--- a/yann/data/metrics.py
+++ b/yann/data/metrics.py
@@ -1,5 +1,6 @@
-from time import time as get_time
 from statistics import mean
+from time import time as get_time
+
 
 def padded_insert(items, index, value, null_val=None):
   """
@@ -81,12 +82,14 @@ class MetricStore:
 
   def to_pandas(self):
     import pandas as pd
+
     return pd.DataFrame({'times': self.times, **self.values})
 
   def plot(self, metrics=None, time=False, clear=False):
     if clear:
       try:
         from IPython.display import clear_output
+
         clear_output(wait=True)
       except:
         pass
@@ -95,15 +98,14 @@ class MetricStore:
     s = {}
     for metric, values in self.values.items():
       try:
-        s[f"{metric}_min"] = min(values)
-        s[f"{metric}_max"] = max(values)
+        s[f'{metric}_min'] = min(values)
+        s[f'{metric}_max'] = max(values)
       except:
         pass
     return s
 
-
   def __repr__(self):
-    return  f"MetricStore({', '.join(f'{k}=(min={min(v)}, max={max(v)})' for k,v in self.values.items())}, len={len(self)})"
+    return f'MetricStore({", ".join(f"{k}=(min={min(v)}, max={max(v)})" for k, v in self.values.items())}, len={len(self)})'
 
 
 class EventStore(list):
@@ -118,4 +120,4 @@ class Event:
     self.key = key
     self.value = value
     self.step = step
-    self.time = time
\ No newline at end of file
+    self.time = time
diff --git a/yann/data/place.py b/yann/data/place.py
index f9fc1d2..fd21fdb 100644
--- a/yann/data/place.py
+++ b/yann/data/place.py
@@ -18,6 +18,7 @@ class Place:
     place = Place({'inputs': dict(device='cuda', memory_format=torch.channels_last), 'targets': 'cpu'})
     place(dict(inputs=t1, targets=t2, keys=[1,2,3])
   """
+
   def __init__(self, placements=None, **kwargs):
     if isinstance(placements, abc.Sequence):
       self.placements = dict(enumerate(placements))
@@ -38,13 +39,14 @@ class Place:
         for n, x in enumerate(batch)
       )
     elif isinstance(batch, abc.Mapping):
-      return batch.__class__({
-        k: x.to(**self.placements[n]) if n in self.placements else x
-        for k, x in batch.items()
-        })
+      return batch.__class__(
+        {
+          k: x.to(**self.placements[n]) if n in self.placements else x
+          for k, x in batch.items()
+        },
+      )
     elif self.placements is None and hasattr(batch, 'to'):
       return batch.to(**self.kwargs)
     else:
       # TODO: support dataclasses
       raise ValueError('Batch must be a collection or mappable type')
-
diff --git a/yann/data/sampler.py b/yann/data/sampler.py
index b42f0d5..8854bf1 100644
--- a/yann/data/sampler.py
+++ b/yann/data/sampler.py
@@ -1,7 +1,8 @@
-from torch.utils.data.sampler import Sampler
 import random
 from collections import defaultdict
 
+from torch.utils.data.sampler import Sampler
+
 from . import batches
 
 
@@ -69,4 +70,4 @@ class BatchShuffleSampler(Sampler):
   def __iter__(self):
     bs = list(batches(range(len(self)), size=self.batch_size))
     random.shuffle(bs)
-    return (s for b in bs for s in b)
\ No newline at end of file
+    return (s for b in bs for s in b)
diff --git a/yann/data/search/annoy.py b/yann/data/search/annoy.py
index 2a8e1a3..ff1913a 100644
--- a/yann/data/search/annoy.py
+++ b/yann/data/search/annoy.py
@@ -3,7 +3,7 @@ import os.path
 
 from annoy import AnnoyIndex
 
-from ..io import save_json, load_json
+from ..io import load_json, save_json
 from .base import VectorIndex
 
 
@@ -18,14 +18,17 @@ class Annoy(VectorIndex):
     if os.path.isfile(self.path):
       logging.debug(f'Loading existing index: {self.path}')
       self.load_meta()
-      assert self.dims == dims or not dims, \
+      assert self.dims == dims or not dims, (
         'Passed path to existing index but dims do not match'
-      assert self.metric == metric or not metric, \
+      )
+      assert self.metric == metric or not metric, (
         'Passed path to existing index but metrics do not match'
+      )
       self.index = AnnoyIndex(self.dims, metric=self.metric)
     elif dims:
       logging.debug(
-        f'Creating new index with {dims} dimensions and {self.metric} metric')
+        f'Creating new index with {dims} dimensions and {self.metric} metric',
+      )
       self.dims = dims
       self.index = AnnoyIndex(self.dims, metric=self.metric)
       if build_on_disk:
@@ -44,9 +47,7 @@ class Annoy(VectorIndex):
     return [self.path, self.meta_path]
 
   def load_meta(self):
-    self.__dict__.update(
-      load_json(self.meta_path)
-    )
+    self.__dict__.update(load_json(self.meta_path))
 
   def save_meta(self):
     d = {**self.__dict__}
diff --git a/yann/data/search/inverted_index.py b/yann/data/search/inverted_index.py
index 599266f..3285a91 100644
--- a/yann/data/search/inverted_index.py
+++ b/yann/data/search/inverted_index.py
@@ -1,6 +1,5 @@
-
-from collections import defaultdict
 import operator
+from collections import defaultdict
 from functools import reduce
 
 
@@ -24,4 +23,4 @@ class InvertedIndex:
     if not_vals:
       return self[vals] - self[not_vals]
     else:
-      return self[vals]
\ No newline at end of file
+      return self[vals]
diff --git a/yann/data/serialize.py b/yann/data/serialize.py
index 95a4c09..a8549a9 100644
--- a/yann/data/serialize.py
+++ b/yann/data/serialize.py
@@ -1,12 +1,11 @@
 import pyarrow
 
-
 ctx = pyarrow.serialization.default_serialization_context()
 
+
 def enable_torch_serialization(context=ctx):
-  pyarrow.serialization.register_torch_serialization_handlers(
-    context
-  )
+  pyarrow.serialization.register_torch_serialization_handlers(context)
+
 
 # enable by default
 enable_torch_serialization()
@@ -30,4 +29,4 @@ def to_bytes(string: str) -> bytes:
 
 
 def to_unicode(b: bytes) -> str:
-  return b.decode(encoding='utf-8', errors='strict')
\ No newline at end of file
+  return b.decode(encoding='utf-8', errors='strict')
diff --git a/yann/data/stats.py b/yann/data/stats.py
index 86229f3..eeded4c 100644
--- a/yann/data/stats.py
+++ b/yann/data/stats.py
@@ -1,5 +1,6 @@
 import torch
 
+
 class TensorStats:
   def __init__(self, device=None, dim=0):
     self.total = None
@@ -25,8 +26,16 @@ class TensorStats:
     else:
       self.total += sum
       self.count += batch.shape[self.dim]
-      self.max = torch.maximum(self.max, batch.max(dim=self.dim)[0], out=self.max)
-      self.min = torch.minimum(self.min, batch.min(dim=self.dim)[0], out=self.min)
+      self.max = torch.maximum(
+        self.max,
+        batch.max(dim=self.dim)[0],
+        out=self.max,
+      )
+      self.min = torch.minimum(
+        self.min,
+        batch.min(dim=self.dim)[0],
+        out=self.min,
+      )
 
   @property
   def mean(self):
diff --git a/yann/data/storage/lmdb.py b/yann/data/storage/lmdb.py
index c920035..a06c0d8 100644
--- a/yann/data/storage/lmdb.py
+++ b/yann/data/storage/lmdb.py
@@ -1,10 +1,11 @@
-import lmdb
-import pickle
 import json
+import pickle
 from contextlib import contextmanager
 
-from ..serialize import serialize_arrow, deserialize_arrow, to_bytes, to_unicode
-from ..images import image_to_bytes, image_from_bytes
+import lmdb
+
+from ..images import image_from_bytes, image_to_bytes
+from ..serialize import deserialize_arrow, serialize_arrow, to_bytes, to_unicode
 
 
 class LMDB:
@@ -35,14 +36,19 @@ class LMDB:
   def __getitem__(self, key):
     # TODO: support indexing multiple values
     if self._current_transaction:
-      return self.deserialize(self._current_transaction.get(self.serialize_key(key)))
+      return self.deserialize(
+        self._current_transaction.get(self.serialize_key(key)),
+      )
     else:
       with self.db.begin(write=False) as t:
         return self.deserialize(t.get(self.serialize_key(key)))
 
   def __setitem__(self, key, value):
     if self._current_transaction:
-      return self._current_transaction.put(self.serialize_key(key), self.serialize(value))
+      return self._current_transaction.put(
+        self.serialize_key(key),
+        self.serialize(value),
+      )
     else:
       with self.db.begin(write=True) as t:
         return t.put(self.serialize_key(key), self.serialize(value))
@@ -106,19 +112,19 @@ class ArrowLMDB(LMDB):
   """
 
   @staticmethod
-  def serialize_key(x): 
+  def serialize_key(x):
     return to_bytes(x)
 
   @staticmethod
-  def deserialize_key(x): 
+  def deserialize_key(x):
     return to_unicode(x)
 
   @staticmethod
-  def serialize(x): 
+  def serialize(x):
     return serialize_arrow(x)
 
   @staticmethod
-  def deserialize(x): 
+  def deserialize(x):
     return deserialize_arrow(x)
 
 
@@ -171,4 +177,4 @@ class JSONLMDB(LMDB):
 
   @staticmethod
   def deserialize(x):
-    return json.loads(to_unicode(x))
\ No newline at end of file
+    return json.loads(to_unicode(x))
diff --git a/yann/data/storage/parquet.py b/yann/data/storage/parquet.py
index 2f1d87e..5f89295 100644
--- a/yann/data/storage/parquet.py
+++ b/yann/data/storage/parquet.py
@@ -1,6 +1,6 @@
-from pyarrow import parquet as pq
-import pyarrow as pa
 import pandas as pd
+import pyarrow as pa
+from pyarrow import parquet as pq
 
 
 def write_parquet(dest, data, columns=None, **kwargs):
@@ -16,8 +16,8 @@ def write_parquet(dest, data, columns=None, **kwargs):
     with pq.ParquetWriter(dest, schema=table.schema, **kwargs) as writer:
       writer.write_table(table)
       for d in next(data):
-        writer.write_table(pa.Table.from_pandas(pd.DataFrame(d))
+        writer.write_table(pa.Table.from_pandas(pd.DataFrame(d)))
 
 
 def read_parquet():
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/data/transform.py b/yann/data/transform.py
index 1ecda99..d6a8b3c 100644
--- a/yann/data/transform.py
+++ b/yann/data/transform.py
@@ -1,2 +1,2 @@
 # for backwards compatibility, since it was moved to yann.transforms
-from yann.transforms import *
\ No newline at end of file
+from yann.transforms import *
diff --git a/yann/data/utils.py b/yann/data/utils.py
index b1989d2..9af4db1 100644
--- a/yann/data/utils.py
+++ b/yann/data/utils.py
@@ -3,7 +3,8 @@ import torch
 
 
 def pad(tensor, shape, value=0):
-  if tensor.shape == shape: return tensor
+  if tensor.shape == shape:
+    return tensor
 
   if isinstance(tensor, np.ndarray):
     padded = np.zeros(shape, dtype=tensor.dtype)
@@ -18,4 +19,4 @@ def pad(tensor, shape, value=0):
 
 def pad_to_largest(tensors, value=0):
   shape = tuple(max(dim) for dim in zip(*(t.shape for t in tensors)))
-  return [pad(t, shape, value) for t in tensors]
\ No newline at end of file
+  return [pad(t, shape, value) for t in tensors]
diff --git a/yann/datasets/__init__.py b/yann/datasets/__init__.py
index e0ddc6d..39761ea 100644
--- a/yann/datasets/__init__.py
+++ b/yann/datasets/__init__.py
@@ -1,19 +1,24 @@
 import os
 from glob import iglob
-from typing import Union, Iterable, Any
+from typing import Any, Iterable, Union
 
 import torch
 from torch.utils import data
 
-from .wrappers import LookupCache, DatasetWrapper, TransformDataset, IncludeIndex, IndexedView, Subset
 from ..data.classes import Classes
+from .wrappers import (
+  DatasetWrapper,
+  IncludeIndex,
+  IndexedView,
+  LookupCache,
+  Subset,
+  TransformDataset,
+)
 
 
 class Dataset(data.Dataset):
   def state_dict(self):
-    return {
-      'name': self.__class__.__name__
-    }
+    return {'name': self.__class__.__name__}
 
 
 class SupervisedDataset(Dataset):
@@ -77,16 +82,20 @@ class TinyDigits(data.TensorDataset):
   """
 
   def __init__(self, num_classes=10):
-    from sklearn.datasets import load_digits
+    try:
+      from sklearn.datasets import load_digits
+    except ImportError:
+      raise ImportError(
+        "TinyDigits requires scikit-learn. Install it with: pip install scikit-learn"
+      )
+
     digits = load_digits(num_classes)
     super().__init__(
       torch.from_numpy(digits.images).unsqueeze(1).float(),
-      torch.Tensor(digits.target).long()
+      torch.Tensor(digits.target).long(),
     )
 
 
-
 # from .voc import VOCMultilabel
 # from .coco import CocoMultilabel
 # from .imagenette import Imagenette, Imagewoof
-
diff --git a/yann/datasets/coco.py b/yann/datasets/coco.py
index d684198..316e96d 100644
--- a/yann/datasets/coco.py
+++ b/yann/datasets/coco.py
@@ -1,17 +1,17 @@
+import os
+
 from torchvision.datasets import CocoDetection
-from yann.data import Classes
+
 from yann.config.defaults import default
+from yann.data import Classes
 
-import os
 
 class CocoMultilabel(CocoDetection):
-  def __init__(
-      self,
-      root=None,
-      annFile=None
-  ):
+  def __init__(self, root=None, annFile=None):
     root = root or default.datasets_root / 'mscoco/train2017'
-    annFile = annFile or default.datasets_root / 'mscoco/annotations/instances_train2017.json'
+    annFile = (
+      annFile or default.datasets_root / 'mscoco/annotations/instances_train2017.json'
+    )
 
     super(CocoMultilabel, self).__init__(
       root=root,
@@ -22,19 +22,18 @@ class CocoMultilabel(CocoDetection):
       self.cat2cat[cat] = len(self.cat2cat)
     # print(self.cat2cat)
 
-    self.category_id_to_name = {
-      x['id']: x['name']
-      for x in self.coco.cats.values()
-    }
+    self.category_id_to_name = {x['id']: x['name'] for x in self.coco.cats.values()}
 
     self.labels = {}  # map from image id to label name
     for image_id in self.ids:
       annotation_ids = self.coco.getAnnIds(imgIds=image_id)
       annotations = self.coco.loadAnns(annotation_ids)
-      self.labels[image_id] = list({
-        self.category_id_to_name[annotation['category_id']]
-        for annotation in annotations
-      })
+      self.labels[image_id] = list(
+        {
+          self.category_id_to_name[annotation['category_id']]
+          for annotation in annotations
+        },
+      )
 
     self.classes = Classes.from_labels(self.labels.values())
 
@@ -42,4 +41,4 @@ class CocoMultilabel(CocoDetection):
     img_id = self.ids[index]
     file_name = self.coco.loadImgs(img_id)[0]['file_name']
     path = os.path.join(self.root, file_name)
-    return path, self.labels[img_id]
\ No newline at end of file
+    return path, self.labels[img_id]
diff --git a/yann/datasets/dataframe.py b/yann/datasets/dataframe.py
index 83887cb..dee8eae 100644
--- a/yann/datasets/dataframe.py
+++ b/yann/datasets/dataframe.py
@@ -1,20 +1,16 @@
 import pathlib
 
+import pandas as pd
 from torch.utils.data import Dataset
+
 import yann
 from yann.data import Classes
-import pandas as pd
 
 
 class DataFrame(Dataset):
   data: pd.DataFrame
 
-  def __init__(
-      self,
-      source,
-      columns=None,
-      target_col=None
-  ):
+  def __init__(self, source, columns=None, target_col=None):
     if isinstance(source, (str, pathlib.Path)):
       source = yann.load(source)
     self.data = source
@@ -25,7 +21,6 @@ class DataFrame(Dataset):
     if target_col:
       self.classes = Classes.from_labels(self.data[target_col])
 
-
   def __len__(self):
     return len(self.data)
 
@@ -33,4 +28,4 @@ class DataFrame(Dataset):
     row = self.data.iloc[index]
     if self.columns:
       return row[self.columns]
-    return row
\ No newline at end of file
+    return row
diff --git a/yann/datasets/folder.py b/yann/datasets/folder.py
index 82ee170..f67ee41 100644
--- a/yann/datasets/folder.py
+++ b/yann/datasets/folder.py
@@ -1,4 +1,5 @@
-from torchvision.datasets.folder import ImageFolder as IF, default_loader
+from torchvision.datasets.folder import ImageFolder as IF
+from torchvision.datasets.folder import default_loader
 
 from yann.data import Classes
 
@@ -10,14 +11,14 @@ class ImageFolder(IF):
     transform=None,
     target_transform=None,
     loader=default_loader,
-    is_valid_file=None
+    is_valid_file=None,
   ):
     super(ImageFolder, self).__init__(
       root=root,
       transform=transform,
       target_transform=target_transform,
       loader=loader,
-      is_valid_file=is_valid_file
+      is_valid_file=is_valid_file,
     )
 
-    self.classes = Classes(self.classes)
\ No newline at end of file
+    self.classes = Classes(self.classes)
diff --git a/yann/datasets/food101.py b/yann/datasets/food101.py
index b66ebb8..18698bc 100644
--- a/yann/datasets/food101.py
+++ b/yann/datasets/food101.py
@@ -20,18 +20,9 @@ class Food101(Dataset):
   url = 'http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz'
   filename = 'food-101.tar.gz'
 
-  splits = {
-    'train',
-    'test'
-  }
-
-  def __init__(
-      self,
-      root=None,
-      split='train',
-      download=False,
-      shuffle=True
-  ):
+  splits = {'train', 'test'}
+
+  def __init__(self, root=None, split='train', download=False, shuffle=True):
     assert split in self.splits
     self.root = Path(root) if root else yann.default.dataset_root(Food101)
     self.root = self.root.expanduser()
@@ -42,8 +33,7 @@ class Food101(Dataset):
 
     with open(self.meta_path, 'r') as f:
       self.samples = [
-        (self.get_image_path(name.strip()), name.split('/')[0])
-        for name in f
+        (self.get_image_path(name.strip()), name.split('/')[0]) for name in f
       ]
 
     if shuffle:
@@ -70,11 +60,11 @@ class Food101(Dataset):
     extract(self.root / self.filename, self.root)
 
 
-
 class Food101N(Food101):
   """
   https://kuanghuei.github.io/Food-101N/
   """
+
   url = 'https://iudata.blob.core.windows.net/food101/Food-101N_release.zip'
   filename = 'Food-101N_release.zip'
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/datasets/imagenet.py b/yann/datasets/imagenet.py
index 0408e96..f3bb980 100644
--- a/yann/datasets/imagenet.py
+++ b/yann/datasets/imagenet.py
@@ -1,1008 +1,1008 @@
 def get_classes():
-  """Returns classes ordered """
+  """Returns classes ordered"""
   return (
-    ("n01440764", "tench"),
-    ("n01443537", "goldfish"),
-    ("n01484850", "great_white_shark"),
-    ("n01491361", "tiger_shark"),
-    ("n01494475", "hammerhead"),
-    ("n01496331", "electric_ray"),
-    ("n01498041", "stingray"),
-    ("n01514668", "cock"),
-    ("n01514859", "hen"),
-    ("n01518878", "ostrich"),
-    ("n01530575", "brambling"),
-    ("n01531178", "goldfinch"),
-    ("n01532829", "house_finch"),
-    ("n01534433", "junco"),
-    ("n01537544", "indigo_bunting"),
-    ("n01558993", "robin"),
-    ("n01560419", "bulbul"),
-    ("n01580077", "jay"),
-    ("n01582220", "magpie"),
-    ("n01592084", "chickadee"),
-    ("n01601694", "water_ouzel"),
-    ("n01608432", "kite"),
-    ("n01614925", "bald_eagle"),
-    ("n01616318", "vulture"),
-    ("n01622779", "great_grey_owl"),
-    ("n01629819", "European_fire_salamander"),
-    ("n01630670", "common_newt"),
-    ("n01631663", "eft"),
-    ("n01632458", "spotted_salamander"),
-    ("n01632777", "axolotl"),
-    ("n01641577", "bullfrog"),
-    ("n01644373", "tree_frog"),
-    ("n01644900", "tailed_frog"),
-    ("n01664065", "loggerhead"),
-    ("n01665541", "leatherback_turtle"),
-    ("n01667114", "mud_turtle"),
-    ("n01667778", "terrapin"),
-    ("n01669191", "box_turtle"),
-    ("n01675722", "banded_gecko"),
-    ("n01677366", "common_iguana"),
-    ("n01682714", "American_chameleon"),
-    ("n01685808", "whiptail"),
-    ("n01687978", "agama"),
-    ("n01688243", "frilled_lizard"),
-    ("n01689811", "alligator_lizard"),
-    ("n01692333", "Gila_monster"),
-    ("n01693334", "green_lizard"),
-    ("n01694178", "African_chameleon"),
-    ("n01695060", "Komodo_dragon"),
-    ("n01697457", "African_crocodile"),
-    ("n01698640", "American_alligator"),
-    ("n01704323", "triceratops"),
-    ("n01728572", "thunder_snake"),
-    ("n01728920", "ringneck_snake"),
-    ("n01729322", "hognose_snake"),
-    ("n01729977", "green_snake"),
-    ("n01734418", "king_snake"),
-    ("n01735189", "garter_snake"),
-    ("n01737021", "water_snake"),
-    ("n01739381", "vine_snake"),
-    ("n01740131", "night_snake"),
-    ("n01742172", "boa_constrictor"),
-    ("n01744401", "rock_python"),
-    ("n01748264", "Indian_cobra"),
-    ("n01749939", "green_mamba"),
-    ("n01751748", "sea_snake"),
-    ("n01753488", "horned_viper"),
-    ("n01755581", "diamondback"),
-    ("n01756291", "sidewinder"),
-    ("n01768244", "trilobite"),
-    ("n01770081", "harvestman"),
-    ("n01770393", "scorpion"),
-    ("n01773157", "black_and_gold_garden_spider"),
-    ("n01773549", "barn_spider"),
-    ("n01773797", "garden_spider"),
-    ("n01774384", "black_widow"),
-    ("n01774750", "tarantula"),
-    ("n01775062", "wolf_spider"),
-    ("n01776313", "tick"),
-    ("n01784675", "centipede"),
-    ("n01795545", "black_grouse"),
-    ("n01796340", "ptarmigan"),
-    ("n01797886", "ruffed_grouse"),
-    ("n01798484", "prairie_chicken"),
-    ("n01806143", "peacock"),
-    ("n01806567", "quail"),
-    ("n01807496", "partridge"),
-    ("n01817953", "African_grey"),
-    ("n01818515", "macaw"),
-    ("n01819313", "sulphur-crested_cockatoo"),
-    ("n01820546", "lorikeet"),
-    ("n01824575", "coucal"),
-    ("n01828970", "bee_eater"),
-    ("n01829413", "hornbill"),
-    ("n01833805", "hummingbird"),
-    ("n01843065", "jacamar"),
-    ("n01843383", "toucan"),
-    ("n01847000", "drake"),
-    ("n01855032", "red-breasted_merganser"),
-    ("n01855672", "goose"),
-    ("n01860187", "black_swan"),
-    ("n01871265", "tusker"),
-    ("n01872401", "echidna"),
-    ("n01873310", "platypus"),
-    ("n01877812", "wallaby"),
-    ("n01882714", "koala"),
-    ("n01883070", "wombat"),
-    ("n01910747", "jellyfish"),
-    ("n01914609", "sea_anemone"),
-    ("n01917289", "brain_coral"),
-    ("n01924916", "flatworm"),
-    ("n01930112", "nematode"),
-    ("n01943899", "conch"),
-    ("n01944390", "snail"),
-    ("n01945685", "slug"),
-    ("n01950731", "sea_slug"),
-    ("n01955084", "chiton"),
-    ("n01968897", "chambered_nautilus"),
-    ("n01978287", "Dungeness_crab"),
-    ("n01978455", "rock_crab"),
-    ("n01980166", "fiddler_crab"),
-    ("n01981276", "king_crab"),
-    ("n01983481", "American_lobster"),
-    ("n01984695", "spiny_lobster"),
-    ("n01985128", "crayfish"),
-    ("n01986214", "hermit_crab"),
-    ("n01990800", "isopod"),
-    ("n02002556", "white_stork"),
-    ("n02002724", "black_stork"),
-    ("n02006656", "spoonbill"),
-    ("n02007558", "flamingo"),
-    ("n02009229", "little_blue_heron"),
-    ("n02009912", "American_egret"),
-    ("n02011460", "bittern"),
-    ("n02012849", "crane"),
-    ("n02013706", "limpkin"),
-    ("n02017213", "European_gallinule"),
-    ("n02018207", "American_coot"),
-    ("n02018795", "bustard"),
-    ("n02025239", "ruddy_turnstone"),
-    ("n02027492", "red-backed_sandpiper"),
-    ("n02028035", "redshank"),
-    ("n02033041", "dowitcher"),
-    ("n02037110", "oystercatcher"),
-    ("n02051845", "pelican"),
-    ("n02056570", "king_penguin"),
-    ("n02058221", "albatross"),
-    ("n02066245", "grey_whale"),
-    ("n02071294", "killer_whale"),
-    ("n02074367", "dugong"),
-    ("n02077923", "sea_lion"),
-    ("n02085620", "Chihuahua"),
-    ("n02085782", "Japanese_spaniel"),
-    ("n02085936", "Maltese_dog"),
-    ("n02086079", "Pekinese"),
-    ("n02086240", "Shih-Tzu"),
-    ("n02086646", "Blenheim_spaniel"),
-    ("n02086910", "papillon"),
-    ("n02087046", "toy_terrier"),
-    ("n02087394", "Rhodesian_ridgeback"),
-    ("n02088094", "Afghan_hound"),
-    ("n02088238", "basset"),
-    ("n02088364", "beagle"),
-    ("n02088466", "bloodhound"),
-    ("n02088632", "bluetick"),
-    ("n02089078", "black-and-tan_coonhound"),
-    ("n02089867", "Walker_hound"),
-    ("n02089973", "English_foxhound"),
-    ("n02090379", "redbone"),
-    ("n02090622", "borzoi"),
-    ("n02090721", "Irish_wolfhound"),
-    ("n02091032", "Italian_greyhound"),
-    ("n02091134", "whippet"),
-    ("n02091244", "Ibizan_hound"),
-    ("n02091467", "Norwegian_elkhound"),
-    ("n02091635", "otterhound"),
-    ("n02091831", "Saluki"),
-    ("n02092002", "Scottish_deerhound"),
-    ("n02092339", "Weimaraner"),
-    ("n02093256", "Staffordshire_bullterrier"),
-    ("n02093428", "American_Staffordshire_terrier"),
-    ("n02093647", "Bedlington_terrier"),
-    ("n02093754", "Border_terrier"),
-    ("n02093859", "Kerry_blue_terrier"),
-    ("n02093991", "Irish_terrier"),
-    ("n02094114", "Norfolk_terrier"),
-    ("n02094258", "Norwich_terrier"),
-    ("n02094433", "Yorkshire_terrier"),
-    ("n02095314", "wire-haired_fox_terrier"),
-    ("n02095570", "Lakeland_terrier"),
-    ("n02095889", "Sealyham_terrier"),
-    ("n02096051", "Airedale"),
-    ("n02096177", "cairn"),
-    ("n02096294", "Australian_terrier"),
-    ("n02096437", "Dandie_Dinmont"),
-    ("n02096585", "Boston_bull"),
-    ("n02097047", "miniature_schnauzer"),
-    ("n02097130", "giant_schnauzer"),
-    ("n02097209", "standard_schnauzer"),
-    ("n02097298", "Scotch_terrier"),
-    ("n02097474", "Tibetan_terrier"),
-    ("n02097658", "silky_terrier"),
-    ("n02098105", "soft-coated_wheaten_terrier"),
-    ("n02098286", "West_Highland_white_terrier"),
-    ("n02098413", "Lhasa"),
-    ("n02099267", "flat-coated_retriever"),
-    ("n02099429", "curly-coated_retriever"),
-    ("n02099601", "golden_retriever"),
-    ("n02099712", "Labrador_retriever"),
-    ("n02099849", "Chesapeake_Bay_retriever"),
-    ("n02100236", "German_short-haired_pointer"),
-    ("n02100583", "vizsla"),
-    ("n02100735", "English_setter"),
-    ("n02100877", "Irish_setter"),
-    ("n02101006", "Gordon_setter"),
-    ("n02101388", "Brittany_spaniel"),
-    ("n02101556", "clumber"),
-    ("n02102040", "English_springer"),
-    ("n02102177", "Welsh_springer_spaniel"),
-    ("n02102318", "cocker_spaniel"),
-    ("n02102480", "Sussex_spaniel"),
-    ("n02102973", "Irish_water_spaniel"),
-    ("n02104029", "kuvasz"),
-    ("n02104365", "schipperke"),
-    ("n02105056", "groenendael"),
-    ("n02105162", "malinois"),
-    ("n02105251", "briard"),
-    ("n02105412", "kelpie"),
-    ("n02105505", "komondor"),
-    ("n02105641", "Old_English_sheepdog"),
-    ("n02105855", "Shetland_sheepdog"),
-    ("n02106030", "collie"),
-    ("n02106166", "Border_collie"),
-    ("n02106382", "Bouvier_des_Flandres"),
-    ("n02106550", "Rottweiler"),
-    ("n02106662", "German_shepherd"),
-    ("n02107142", "Doberman"),
-    ("n02107312", "miniature_pinscher"),
-    ("n02107574", "Greater_Swiss_Mountain_dog"),
-    ("n02107683", "Bernese_mountain_dog"),
-    ("n02107908", "Appenzeller"),
-    ("n02108000", "EntleBucher"),
-    ("n02108089", "boxer"),
-    ("n02108422", "bull_mastiff"),
-    ("n02108551", "Tibetan_mastiff"),
-    ("n02108915", "French_bulldog"),
-    ("n02109047", "Great_Dane"),
-    ("n02109525", "Saint_Bernard"),
-    ("n02109961", "Eskimo_dog"),
-    ("n02110063", "malamute"),
-    ("n02110185", "Siberian_husky"),
-    ("n02110341", "dalmatian"),
-    ("n02110627", "affenpinscher"),
-    ("n02110806", "basenji"),
-    ("n02110958", "pug"),
-    ("n02111129", "Leonberg"),
-    ("n02111277", "Newfoundland"),
-    ("n02111500", "Great_Pyrenees"),
-    ("n02111889", "Samoyed"),
-    ("n02112018", "Pomeranian"),
-    ("n02112137", "chow"),
-    ("n02112350", "keeshond"),
-    ("n02112706", "Brabancon_griffon"),
-    ("n02113023", "Pembroke"),
-    ("n02113186", "Cardigan"),
-    ("n02113624", "toy_poodle"),
-    ("n02113712", "miniature_poodle"),
-    ("n02113799", "standard_poodle"),
-    ("n02113978", "Mexican_hairless"),
-    ("n02114367", "timber_wolf"),
-    ("n02114548", "white_wolf"),
-    ("n02114712", "red_wolf"),
-    ("n02114855", "coyote"),
-    ("n02115641", "dingo"),
-    ("n02115913", "dhole"),
-    ("n02116738", "African_hunting_dog"),
-    ("n02117135", "hyena"),
-    ("n02119022", "red_fox"),
-    ("n02119789", "kit_fox"),
-    ("n02120079", "Arctic_fox"),
-    ("n02120505", "grey_fox"),
-    ("n02123045", "tabby"),
-    ("n02123159", "tiger_cat"),
-    ("n02123394", "Persian_cat"),
-    ("n02123597", "Siamese_cat"),
-    ("n02124075", "Egyptian_cat"),
-    ("n02125311", "cougar"),
-    ("n02127052", "lynx"),
-    ("n02128385", "leopard"),
-    ("n02128757", "snow_leopard"),
-    ("n02128925", "jaguar"),
-    ("n02129165", "lion"),
-    ("n02129604", "tiger"),
-    ("n02130308", "cheetah"),
-    ("n02132136", "brown_bear"),
-    ("n02133161", "American_black_bear"),
-    ("n02134084", "ice_bear"),
-    ("n02134418", "sloth_bear"),
-    ("n02137549", "mongoose"),
-    ("n02138441", "meerkat"),
-    ("n02165105", "tiger_beetle"),
-    ("n02165456", "ladybug"),
-    ("n02167151", "ground_beetle"),
-    ("n02168699", "long-horned_beetle"),
-    ("n02169497", "leaf_beetle"),
-    ("n02172182", "dung_beetle"),
-    ("n02174001", "rhinoceros_beetle"),
-    ("n02177972", "weevil"),
-    ("n02190166", "fly"),
-    ("n02206856", "bee"),
-    ("n02219486", "ant"),
-    ("n02226429", "grasshopper"),
-    ("n02229544", "cricket"),
-    ("n02231487", "walking_stick"),
-    ("n02233338", "cockroach"),
-    ("n02236044", "mantis"),
-    ("n02256656", "cicada"),
-    ("n02259212", "leafhopper"),
-    ("n02264363", "lacewing"),
-    ("n02268443", "dragonfly"),
-    ("n02268853", "damselfly"),
-    ("n02276258", "admiral"),
-    ("n02277742", "ringlet"),
-    ("n02279972", "monarch"),
-    ("n02280649", "cabbage_butterfly"),
-    ("n02281406", "sulphur_butterfly"),
-    ("n02281787", "lycaenid"),
-    ("n02317335", "starfish"),
-    ("n02319095", "sea_urchin"),
-    ("n02321529", "sea_cucumber"),
-    ("n02325366", "wood_rabbit"),
-    ("n02326432", "hare"),
-    ("n02328150", "Angora"),
-    ("n02342885", "hamster"),
-    ("n02346627", "porcupine"),
-    ("n02356798", "fox_squirrel"),
-    ("n02361337", "marmot"),
-    ("n02363005", "beaver"),
-    ("n02364673", "guinea_pig"),
-    ("n02389026", "sorrel"),
-    ("n02391049", "zebra"),
-    ("n02395406", "hog"),
-    ("n02396427", "wild_boar"),
-    ("n02397096", "warthog"),
-    ("n02398521", "hippopotamus"),
-    ("n02403003", "ox"),
-    ("n02408429", "water_buffalo"),
-    ("n02410509", "bison"),
-    ("n02412080", "ram"),
-    ("n02415577", "bighorn"),
-    ("n02417914", "ibex"),
-    ("n02422106", "hartebeest"),
-    ("n02422699", "impala"),
-    ("n02423022", "gazelle"),
-    ("n02437312", "Arabian_camel"),
-    ("n02437616", "llama"),
-    ("n02441942", "weasel"),
-    ("n02442845", "mink"),
-    ("n02443114", "polecat"),
-    ("n02443484", "black-footed_ferret"),
-    ("n02444819", "otter"),
-    ("n02445715", "skunk"),
-    ("n02447366", "badger"),
-    ("n02454379", "armadillo"),
-    ("n02457408", "three-toed_sloth"),
-    ("n02480495", "orangutan"),
-    ("n02480855", "gorilla"),
-    ("n02481823", "chimpanzee"),
-    ("n02483362", "gibbon"),
-    ("n02483708", "siamang"),
-    ("n02484975", "guenon"),
-    ("n02486261", "patas"),
-    ("n02486410", "baboon"),
-    ("n02487347", "macaque"),
-    ("n02488291", "langur"),
-    ("n02488702", "colobus"),
-    ("n02489166", "proboscis_monkey"),
-    ("n02490219", "marmoset"),
-    ("n02492035", "capuchin"),
-    ("n02492660", "howler_monkey"),
-    ("n02493509", "titi"),
-    ("n02493793", "spider_monkey"),
-    ("n02494079", "squirrel_monkey"),
-    ("n02497673", "Madagascar_cat"),
-    ("n02500267", "indri"),
-    ("n02504013", "Indian_elephant"),
-    ("n02504458", "African_elephant"),
-    ("n02509815", "lesser_panda"),
-    ("n02510455", "giant_panda"),
-    ("n02514041", "barracouta"),
-    ("n02526121", "eel"),
-    ("n02536864", "coho"),
-    ("n02606052", "rock_beauty"),
-    ("n02607072", "anemone_fish"),
-    ("n02640242", "sturgeon"),
-    ("n02641379", "gar"),
-    ("n02643566", "lionfish"),
-    ("n02655020", "puffer"),
-    ("n02666196", "abacus"),
-    ("n02667093", "abaya"),
-    ("n02669723", "academic_gown"),
-    ("n02672831", "accordion"),
-    ("n02676566", "acoustic_guitar"),
-    ("n02687172", "aircraft_carrier"),
-    ("n02690373", "airliner"),
-    ("n02692877", "airship"),
-    ("n02699494", "altar"),
-    ("n02701002", "ambulance"),
-    ("n02704792", "amphibian"),
-    ("n02708093", "analog_clock"),
-    ("n02727426", "apiary"),
-    ("n02730930", "apron"),
-    ("n02747177", "ashcan"),
-    ("n02749479", "assault_rifle"),
-    ("n02769748", "backpack"),
-    ("n02776631", "bakery"),
-    ("n02777292", "balance_beam"),
-    ("n02782093", "balloon"),
-    ("n02783161", "ballpoint"),
-    ("n02786058", "Band_Aid"),
-    ("n02787622", "banjo"),
-    ("n02788148", "bannister"),
-    ("n02790996", "barbell"),
-    ("n02791124", "barber_chair"),
-    ("n02791270", "barbershop"),
-    ("n02793495", "barn"),
-    ("n02794156", "barometer"),
-    ("n02795169", "barrel"),
-    ("n02797295", "barrow"),
-    ("n02799071", "baseball"),
-    ("n02802426", "basketball"),
-    ("n02804414", "bassinet"),
-    ("n02804610", "bassoon"),
-    ("n02807133", "bathing_cap"),
-    ("n02808304", "bath_towel"),
-    ("n02808440", "bathtub"),
-    ("n02814533", "beach_wagon"),
-    ("n02814860", "beacon"),
-    ("n02815834", "beaker"),
-    ("n02817516", "bearskin"),
-    ("n02823428", "beer_bottle"),
-    ("n02823750", "beer_glass"),
-    ("n02825657", "bell_cote"),
-    ("n02834397", "bib"),
-    ("n02835271", "bicycle-built-for-two"),
-    ("n02837789", "bikini"),
-    ("n02840245", "binder"),
-    ("n02841315", "binoculars"),
-    ("n02843684", "birdhouse"),
-    ("n02859443", "boathouse"),
-    ("n02860847", "bobsled"),
-    ("n02865351", "bolo_tie"),
-    ("n02869837", "bonnet"),
-    ("n02870880", "bookcase"),
-    ("n02871525", "bookshop"),
-    ("n02877765", "bottlecap"),
-    ("n02879718", "bow"),
-    ("n02883205", "bow_tie"),
-    ("n02892201", "brass"),
-    ("n02892767", "brassiere"),
-    ("n02894605", "breakwater"),
-    ("n02895154", "breastplate"),
-    ("n02906734", "broom"),
-    ("n02909870", "bucket"),
-    ("n02910353", "buckle"),
-    ("n02916936", "bulletproof_vest"),
-    ("n02917067", "bullet_train"),
-    ("n02927161", "butcher_shop"),
-    ("n02930766", "cab"),
-    ("n02939185", "caldron"),
-    ("n02948072", "candle"),
-    ("n02950826", "cannon"),
-    ("n02951358", "canoe"),
-    ("n02951585", "can_opener"),
-    ("n02963159", "cardigan"),
-    ("n02965783", "car_mirror"),
-    ("n02966193", "carousel"),
-    ("n02966687", "carpenter's_kit"),
-    ("n02971356", "carton"),
-    ("n02974003", "car_wheel"),
-    ("n02977058", "cash_machine"),
-    ("n02978881", "cassette"),
-    ("n02979186", "cassette_player"),
-    ("n02980441", "castle"),
-    ("n02981792", "catamaran"),
-    ("n02988304", "CD_player"),
-    ("n02992211", "cello"),
-    ("n02992529", "cellular_telephone"),
-    ("n02999410", "chain"),
-    ("n03000134", "chainlink_fence"),
-    ("n03000247", "chain_mail"),
-    ("n03000684", "chain_saw"),
-    ("n03014705", "chest"),
-    ("n03016953", "chiffonier"),
-    ("n03017168", "chime"),
-    ("n03018349", "china_cabinet"),
-    ("n03026506", "Christmas_stocking"),
-    ("n03028079", "church"),
-    ("n03032252", "cinema"),
-    ("n03041632", "cleaver"),
-    ("n03042490", "cliff_dwelling"),
-    ("n03045698", "cloak"),
-    ("n03047690", "clog"),
-    ("n03062245", "cocktail_shaker"),
-    ("n03063599", "coffee_mug"),
-    ("n03063689", "coffeepot"),
-    ("n03065424", "coil"),
-    ("n03075370", "combination_lock"),
-    ("n03085013", "computer_keyboard"),
-    ("n03089624", "confectionery"),
-    ("n03095699", "container_ship"),
-    ("n03100240", "convertible"),
-    ("n03109150", "corkscrew"),
-    ("n03110669", "cornet"),
-    ("n03124043", "cowboy_boot"),
-    ("n03124170", "cowboy_hat"),
-    ("n03125729", "cradle"),
-    ("n03126707", "crane"),
-    ("n03127747", "crash_helmet"),
-    ("n03127925", "crate"),
-    ("n03131574", "crib"),
-    ("n03133878", "Crock_Pot"),
-    ("n03134739", "croquet_ball"),
-    ("n03141823", "crutch"),
-    ("n03146219", "cuirass"),
-    ("n03160309", "dam"),
-    ("n03179701", "desk"),
-    ("n03180011", "desktop_computer"),
-    ("n03187595", "dial_telephone"),
-    ("n03188531", "diaper"),
-    ("n03196217", "digital_clock"),
-    ("n03197337", "digital_watch"),
-    ("n03201208", "dining_table"),
-    ("n03207743", "dishrag"),
-    ("n03207941", "dishwasher"),
-    ("n03208938", "disk_brake"),
-    ("n03216828", "dock"),
-    ("n03218198", "dogsled"),
-    ("n03220513", "dome"),
-    ("n03223299", "doormat"),
-    ("n03240683", "drilling_platform"),
-    ("n03249569", "drum"),
-    ("n03250847", "drumstick"),
-    ("n03255030", "dumbbell"),
-    ("n03259280", "Dutch_oven"),
-    ("n03271574", "electric_fan"),
-    ("n03272010", "electric_guitar"),
-    ("n03272562", "electric_locomotive"),
-    ("n03290653", "entertainment_center"),
-    ("n03291819", "envelope"),
-    ("n03297495", "espresso_maker"),
-    ("n03314780", "face_powder"),
-    ("n03325584", "feather_boa"),
-    ("n03337140", "file"),
-    ("n03344393", "fireboat"),
-    ("n03345487", "fire_engine"),
-    ("n03347037", "fire_screen"),
-    ("n03355925", "flagpole"),
-    ("n03372029", "flute"),
-    ("n03376595", "folding_chair"),
-    ("n03379051", "football_helmet"),
-    ("n03384352", "forklift"),
-    ("n03388043", "fountain"),
-    ("n03388183", "fountain_pen"),
-    ("n03388549", "four-poster"),
-    ("n03393912", "freight_car"),
-    ("n03394916", "French_horn"),
-    ("n03400231", "frying_pan"),
-    ("n03404251", "fur_coat"),
-    ("n03417042", "garbage_truck"),
-    ("n03424325", "gasmask"),
-    ("n03425413", "gas_pump"),
-    ("n03443371", "goblet"),
-    ("n03444034", "go-kart"),
-    ("n03445777", "golf_ball"),
-    ("n03445924", "golfcart"),
-    ("n03447447", "gondola"),
-    ("n03447721", "gong"),
-    ("n03450230", "gown"),
-    ("n03452741", "grand_piano"),
-    ("n03457902", "greenhouse"),
-    ("n03459775", "grille"),
-    ("n03461385", "grocery_store"),
-    ("n03467068", "guillotine"),
-    ("n03476684", "hair_slide"),
-    ("n03476991", "hair_spray"),
-    ("n03478589", "half_track"),
-    ("n03481172", "hammer"),
-    ("n03482405", "hamper"),
-    ("n03483316", "hand_blower"),
-    ("n03485407", "hand-held_computer"),
-    ("n03485794", "handkerchief"),
-    ("n03492542", "hard_disc"),
-    ("n03494278", "harmonica"),
-    ("n03495258", "harp"),
-    ("n03496892", "harvester"),
-    ("n03498962", "hatchet"),
-    ("n03527444", "holster"),
-    ("n03529860", "home_theater"),
-    ("n03530642", "honeycomb"),
-    ("n03532672", "hook"),
-    ("n03534580", "hoopskirt"),
-    ("n03535780", "horizontal_bar"),
-    ("n03538406", "horse_cart"),
-    ("n03544143", "hourglass"),
-    ("n03584254", "iPod"),
-    ("n03584829", "iron"),
-    ("n03590841", "jack-o'-lantern"),
-    ("n03594734", "jean"),
-    ("n03594945", "jeep"),
-    ("n03595614", "jersey"),
-    ("n03598930", "jigsaw_puzzle"),
-    ("n03599486", "jinrikisha"),
-    ("n03602883", "joystick"),
-    ("n03617480", "kimono"),
-    ("n03623198", "knee_pad"),
-    ("n03627232", "knot"),
-    ("n03630383", "lab_coat"),
-    ("n03633091", "ladle"),
-    ("n03637318", "lampshade"),
-    ("n03642806", "laptop"),
-    ("n03649909", "lawn_mower"),
-    ("n03657121", "lens_cap"),
-    ("n03658185", "letter_opener"),
-    ("n03661043", "library"),
-    ("n03662601", "lifeboat"),
-    ("n03666591", "lighter"),
-    ("n03670208", "limousine"),
-    ("n03673027", "liner"),
-    ("n03676483", "lipstick"),
-    ("n03680355", "Loafer"),
-    ("n03690938", "lotion"),
-    ("n03691459", "loudspeaker"),
-    ("n03692522", "loupe"),
-    ("n03697007", "lumbermill"),
-    ("n03706229", "magnetic_compass"),
-    ("n03709823", "mailbag"),
-    ("n03710193", "mailbox"),
-    ("n03710637", "maillot"),
-    ("n03710721", "maillot"),
-    ("n03717622", "manhole_cover"),
-    ("n03720891", "maraca"),
-    ("n03721384", "marimba"),
-    ("n03724870", "mask"),
-    ("n03729826", "matchstick"),
-    ("n03733131", "maypole"),
-    ("n03733281", "maze"),
-    ("n03733805", "measuring_cup"),
-    ("n03742115", "medicine_chest"),
-    ("n03743016", "megalith"),
-    ("n03759954", "microphone"),
-    ("n03761084", "microwave"),
-    ("n03763968", "military_uniform"),
-    ("n03764736", "milk_can"),
-    ("n03769881", "minibus"),
-    ("n03770439", "miniskirt"),
-    ("n03770679", "minivan"),
-    ("n03773504", "missile"),
-    ("n03775071", "mitten"),
-    ("n03775546", "mixing_bowl"),
-    ("n03776460", "mobile_home"),
-    ("n03777568", "Model_T"),
-    ("n03777754", "modem"),
-    ("n03781244", "monastery"),
-    ("n03782006", "monitor"),
-    ("n03785016", "moped"),
-    ("n03786901", "mortar"),
-    ("n03787032", "mortarboard"),
-    ("n03788195", "mosque"),
-    ("n03788365", "mosquito_net"),
-    ("n03791053", "motor_scooter"),
-    ("n03792782", "mountain_bike"),
-    ("n03792972", "mountain_tent"),
-    ("n03793489", "mouse"),
-    ("n03794056", "mousetrap"),
-    ("n03796401", "moving_van"),
-    ("n03803284", "muzzle"),
-    ("n03804744", "nail"),
-    ("n03814639", "neck_brace"),
-    ("n03814906", "necklace"),
-    ("n03825788", "nipple"),
-    ("n03832673", "notebook"),
-    ("n03837869", "obelisk"),
-    ("n03838899", "oboe"),
-    ("n03840681", "ocarina"),
-    ("n03841143", "odometer"),
-    ("n03843555", "oil_filter"),
-    ("n03854065", "organ"),
-    ("n03857828", "oscilloscope"),
-    ("n03866082", "overskirt"),
-    ("n03868242", "oxcart"),
-    ("n03868863", "oxygen_mask"),
-    ("n03871628", "packet"),
-    ("n03873416", "paddle"),
-    ("n03874293", "paddlewheel"),
-    ("n03874599", "padlock"),
-    ("n03876231", "paintbrush"),
-    ("n03877472", "pajama"),
-    ("n03877845", "palace"),
-    ("n03884397", "panpipe"),
-    ("n03887697", "paper_towel"),
-    ("n03888257", "parachute"),
-    ("n03888605", "parallel_bars"),
-    ("n03891251", "park_bench"),
-    ("n03891332", "parking_meter"),
-    ("n03895866", "passenger_car"),
-    ("n03899768", "patio"),
-    ("n03902125", "pay-phone"),
-    ("n03903868", "pedestal"),
-    ("n03908618", "pencil_box"),
-    ("n03908714", "pencil_sharpener"),
-    ("n03916031", "perfume"),
-    ("n03920288", "Petri_dish"),
-    ("n03924679", "photocopier"),
-    ("n03929660", "pick"),
-    ("n03929855", "pickelhaube"),
-    ("n03930313", "picket_fence"),
-    ("n03930630", "pickup"),
-    ("n03933933", "pier"),
-    ("n03935335", "piggy_bank"),
-    ("n03937543", "pill_bottle"),
-    ("n03938244", "pillow"),
-    ("n03942813", "ping-pong_ball"),
-    ("n03944341", "pinwheel"),
-    ("n03947888", "pirate"),
-    ("n03950228", "pitcher"),
-    ("n03954731", "plane"),
-    ("n03956157", "planetarium"),
-    ("n03958227", "plastic_bag"),
-    ("n03961711", "plate_rack"),
-    ("n03967562", "plow"),
-    ("n03970156", "plunger"),
-    ("n03976467", "Polaroid_camera"),
-    ("n03976657", "pole"),
-    ("n03977966", "police_van"),
-    ("n03980874", "poncho"),
-    ("n03982430", "pool_table"),
-    ("n03983396", "pop_bottle"),
-    ("n03991062", "pot"),
-    ("n03992509", "potter's_wheel"),
-    ("n03995372", "power_drill"),
-    ("n03998194", "prayer_rug"),
-    ("n04004767", "printer"),
-    ("n04005630", "prison"),
-    ("n04008634", "projectile"),
-    ("n04009552", "projector"),
-    ("n04019541", "puck"),
-    ("n04023962", "punching_bag"),
-    ("n04026417", "purse"),
-    ("n04033901", "quill"),
-    ("n04033995", "quilt"),
-    ("n04037443", "racer"),
-    ("n04039381", "racket"),
-    ("n04040759", "radiator"),
-    ("n04041544", "radio"),
-    ("n04044716", "radio_telescope"),
-    ("n04049303", "rain_barrel"),
-    ("n04065272", "recreational_vehicle"),
-    ("n04067472", "reel"),
-    ("n04069434", "reflex_camera"),
-    ("n04070727", "refrigerator"),
-    ("n04074963", "remote_control"),
-    ("n04081281", "restaurant"),
-    ("n04086273", "revolver"),
-    ("n04090263", "rifle"),
-    ("n04099969", "rocking_chair"),
-    ("n04111531", "rotisserie"),
-    ("n04116512", "rubber_eraser"),
-    ("n04118538", "rugby_ball"),
-    ("n04118776", "rule"),
-    ("n04120489", "running_shoe"),
-    ("n04125021", "safe"),
-    ("n04127249", "safety_pin"),
-    ("n04131690", "saltshaker"),
-    ("n04133789", "sandal"),
-    ("n04136333", "sarong"),
-    ("n04141076", "sax"),
-    ("n04141327", "scabbard"),
-    ("n04141975", "scale"),
-    ("n04146614", "school_bus"),
-    ("n04147183", "schooner"),
-    ("n04149813", "scoreboard"),
-    ("n04152593", "screen"),
-    ("n04153751", "screw"),
-    ("n04154565", "screwdriver"),
-    ("n04162706", "seat_belt"),
-    ("n04179913", "sewing_machine"),
-    ("n04192698", "shield"),
-    ("n04200800", "shoe_shop"),
-    ("n04201297", "shoji"),
-    ("n04204238", "shopping_basket"),
-    ("n04204347", "shopping_cart"),
-    ("n04208210", "shovel"),
-    ("n04209133", "shower_cap"),
-    ("n04209239", "shower_curtain"),
-    ("n04228054", "ski"),
-    ("n04229816", "ski_mask"),
-    ("n04235860", "sleeping_bag"),
-    ("n04238763", "slide_rule"),
-    ("n04239074", "sliding_door"),
-    ("n04243546", "slot"),
-    ("n04251144", "snorkel"),
-    ("n04252077", "snowmobile"),
-    ("n04252225", "snowplow"),
-    ("n04254120", "soap_dispenser"),
-    ("n04254680", "soccer_ball"),
-    ("n04254777", "sock"),
-    ("n04258138", "solar_dish"),
-    ("n04259630", "sombrero"),
-    ("n04263257", "soup_bowl"),
-    ("n04264628", "space_bar"),
-    ("n04265275", "space_heater"),
-    ("n04266014", "space_shuttle"),
-    ("n04270147", "spatula"),
-    ("n04273569", "speedboat"),
-    ("n04275548", "spider_web"),
-    ("n04277352", "spindle"),
-    ("n04285008", "sports_car"),
-    ("n04286575", "spotlight"),
-    ("n04296562", "stage"),
-    ("n04310018", "steam_locomotive"),
-    ("n04311004", "steel_arch_bridge"),
-    ("n04311174", "steel_drum"),
-    ("n04317175", "stethoscope"),
-    ("n04325704", "stole"),
-    ("n04326547", "stone_wall"),
-    ("n04328186", "stopwatch"),
-    ("n04330267", "stove"),
-    ("n04332243", "strainer"),
-    ("n04335435", "streetcar"),
-    ("n04336792", "stretcher"),
-    ("n04344873", "studio_couch"),
-    ("n04346328", "stupa"),
-    ("n04347754", "submarine"),
-    ("n04350905", "suit"),
-    ("n04355338", "sundial"),
-    ("n04355933", "sunglass"),
-    ("n04356056", "sunglasses"),
-    ("n04357314", "sunscreen"),
-    ("n04366367", "suspension_bridge"),
-    ("n04367480", "swab"),
-    ("n04370456", "sweatshirt"),
-    ("n04371430", "swimming_trunks"),
-    ("n04371774", "swing"),
-    ("n04372370", "switch"),
-    ("n04376876", "syringe"),
-    ("n04380533", "table_lamp"),
-    ("n04389033", "tank"),
-    ("n04392985", "tape_player"),
-    ("n04398044", "teapot"),
-    ("n04399382", "teddy"),
-    ("n04404412", "television"),
-    ("n04409515", "tennis_ball"),
-    ("n04417672", "thatch"),
-    ("n04418357", "theater_curtain"),
-    ("n04423845", "thimble"),
-    ("n04428191", "thresher"),
-    ("n04429376", "throne"),
-    ("n04435653", "tile_roof"),
-    ("n04442312", "toaster"),
-    ("n04443257", "tobacco_shop"),
-    ("n04447861", "toilet_seat"),
-    ("n04456115", "torch"),
-    ("n04458633", "totem_pole"),
-    ("n04461696", "tow_truck"),
-    ("n04462240", "toyshop"),
-    ("n04465501", "tractor"),
-    ("n04467665", "trailer_truck"),
-    ("n04476259", "tray"),
-    ("n04479046", "trench_coat"),
-    ("n04482393", "tricycle"),
-    ("n04483307", "trimaran"),
-    ("n04485082", "tripod"),
-    ("n04486054", "triumphal_arch"),
-    ("n04487081", "trolleybus"),
-    ("n04487394", "trombone"),
-    ("n04493381", "tub"),
-    ("n04501370", "turnstile"),
-    ("n04505470", "typewriter_keyboard"),
-    ("n04507155", "umbrella"),
-    ("n04509417", "unicycle"),
-    ("n04515003", "upright"),
-    ("n04517823", "vacuum"),
-    ("n04522168", "vase"),
-    ("n04523525", "vault"),
-    ("n04525038", "velvet"),
-    ("n04525305", "vending_machine"),
-    ("n04532106", "vestment"),
-    ("n04532670", "viaduct"),
-    ("n04536866", "violin"),
-    ("n04540053", "volleyball"),
-    ("n04542943", "waffle_iron"),
-    ("n04548280", "wall_clock"),
-    ("n04548362", "wallet"),
-    ("n04550184", "wardrobe"),
-    ("n04552348", "warplane"),
-    ("n04553703", "washbasin"),
-    ("n04554684", "washer"),
-    ("n04557648", "water_bottle"),
-    ("n04560804", "water_jug"),
-    ("n04562935", "water_tower"),
-    ("n04579145", "whiskey_jug"),
-    ("n04579432", "whistle"),
-    ("n04584207", "wig"),
-    ("n04589890", "window_screen"),
-    ("n04590129", "window_shade"),
-    ("n04591157", "Windsor_tie"),
-    ("n04591713", "wine_bottle"),
-    ("n04592741", "wing"),
-    ("n04596742", "wok"),
-    ("n04597913", "wooden_spoon"),
-    ("n04599235", "wool"),
-    ("n04604644", "worm_fence"),
-    ("n04606251", "wreck"),
-    ("n04612504", "yawl"),
-    ("n04613696", "yurt"),
-    ("n06359193", "web_site"),
-    ("n06596364", "comic_book"),
-    ("n06785654", "crossword_puzzle"),
-    ("n06794110", "street_sign"),
-    ("n06874185", "traffic_light"),
-    ("n07248320", "book_jacket"),
-    ("n07565083", "menu"),
-    ("n07579787", "plate"),
-    ("n07583066", "guacamole"),
-    ("n07584110", "consomme"),
-    ("n07590611", "hot_pot"),
-    ("n07613480", "trifle"),
-    ("n07614500", "ice_cream"),
-    ("n07615774", "ice_lolly"),
-    ("n07684084", "French_loaf"),
-    ("n07693725", "bagel"),
-    ("n07695742", "pretzel"),
-    ("n07697313", "cheeseburger"),
-    ("n07697537", "hotdog"),
-    ("n07711569", "mashed_potato"),
-    ("n07714571", "head_cabbage"),
-    ("n07714990", "broccoli"),
-    ("n07715103", "cauliflower"),
-    ("n07716358", "zucchini"),
-    ("n07716906", "spaghetti_squash"),
-    ("n07717410", "acorn_squash"),
-    ("n07717556", "butternut_squash"),
-    ("n07718472", "cucumber"),
-    ("n07718747", "artichoke"),
-    ("n07720875", "bell_pepper"),
-    ("n07730033", "cardoon"),
-    ("n07734744", "mushroom"),
-    ("n07742313", "Granny_Smith"),
-    ("n07745940", "strawberry"),
-    ("n07747607", "orange"),
-    ("n07749582", "lemon"),
-    ("n07753113", "fig"),
-    ("n07753275", "pineapple"),
-    ("n07753592", "banana"),
-    ("n07754684", "jackfruit"),
-    ("n07760859", "custard_apple"),
-    ("n07768694", "pomegranate"),
-    ("n07802026", "hay"),
-    ("n07831146", "carbonara"),
-    ("n07836838", "chocolate_sauce"),
-    ("n07860988", "dough"),
-    ("n07871810", "meat_loaf"),
-    ("n07873807", "pizza"),
-    ("n07875152", "potpie"),
-    ("n07880968", "burrito"),
-    ("n07892512", "red_wine"),
-    ("n07920052", "espresso"),
-    ("n07930864", "cup"),
-    ("n07932039", "eggnog"),
-    ("n09193705", "alp"),
-    ("n09229709", "bubble"),
-    ("n09246464", "cliff"),
-    ("n09256479", "coral_reef"),
-    ("n09288635", "geyser"),
-    ("n09332890", "lakeside"),
-    ("n09399592", "promontory"),
-    ("n09421951", "sandbar"),
-    ("n09428293", "seashore"),
-    ("n09468604", "valley"),
-    ("n09472597", "volcano"),
-    ("n09835506", "ballplayer"),
-    ("n10148035", "groom"),
-    ("n10565667", "scuba_diver"),
-    ("n11879895", "rapeseed"),
-    ("n11939491", "daisy"),
-    ("n12057211", "yellow_lady's_slipper"),
-    ("n12144580", "corn"),
-    ("n12267677", "acorn"),
-    ("n12620546", "hip"),
-    ("n12768682", "buckeye"),
-    ("n12985857", "coral_fungus"),
-    ("n12998815", "agaric"),
-    ("n13037406", "gyromitra"),
-    ("n13040303", "stinkhorn"),
-    ("n13044778", "earthstar"),
-    ("n13052670", "hen-of-the-woods"),
-    ("n13054560", "bolete"),
-    ("n13133613", "ear"),
-    ("n15075141", "toilet_tissue")
+    ('n01440764', 'tench'),
+    ('n01443537', 'goldfish'),
+    ('n01484850', 'great_white_shark'),
+    ('n01491361', 'tiger_shark'),
+    ('n01494475', 'hammerhead'),
+    ('n01496331', 'electric_ray'),
+    ('n01498041', 'stingray'),
+    ('n01514668', 'cock'),
+    ('n01514859', 'hen'),
+    ('n01518878', 'ostrich'),
+    ('n01530575', 'brambling'),
+    ('n01531178', 'goldfinch'),
+    ('n01532829', 'house_finch'),
+    ('n01534433', 'junco'),
+    ('n01537544', 'indigo_bunting'),
+    ('n01558993', 'robin'),
+    ('n01560419', 'bulbul'),
+    ('n01580077', 'jay'),
+    ('n01582220', 'magpie'),
+    ('n01592084', 'chickadee'),
+    ('n01601694', 'water_ouzel'),
+    ('n01608432', 'kite'),
+    ('n01614925', 'bald_eagle'),
+    ('n01616318', 'vulture'),
+    ('n01622779', 'great_grey_owl'),
+    ('n01629819', 'European_fire_salamander'),
+    ('n01630670', 'common_newt'),
+    ('n01631663', 'eft'),
+    ('n01632458', 'spotted_salamander'),
+    ('n01632777', 'axolotl'),
+    ('n01641577', 'bullfrog'),
+    ('n01644373', 'tree_frog'),
+    ('n01644900', 'tailed_frog'),
+    ('n01664065', 'loggerhead'),
+    ('n01665541', 'leatherback_turtle'),
+    ('n01667114', 'mud_turtle'),
+    ('n01667778', 'terrapin'),
+    ('n01669191', 'box_turtle'),
+    ('n01675722', 'banded_gecko'),
+    ('n01677366', 'common_iguana'),
+    ('n01682714', 'American_chameleon'),
+    ('n01685808', 'whiptail'),
+    ('n01687978', 'agama'),
+    ('n01688243', 'frilled_lizard'),
+    ('n01689811', 'alligator_lizard'),
+    ('n01692333', 'Gila_monster'),
+    ('n01693334', 'green_lizard'),
+    ('n01694178', 'African_chameleon'),
+    ('n01695060', 'Komodo_dragon'),
+    ('n01697457', 'African_crocodile'),
+    ('n01698640', 'American_alligator'),
+    ('n01704323', 'triceratops'),
+    ('n01728572', 'thunder_snake'),
+    ('n01728920', 'ringneck_snake'),
+    ('n01729322', 'hognose_snake'),
+    ('n01729977', 'green_snake'),
+    ('n01734418', 'king_snake'),
+    ('n01735189', 'garter_snake'),
+    ('n01737021', 'water_snake'),
+    ('n01739381', 'vine_snake'),
+    ('n01740131', 'night_snake'),
+    ('n01742172', 'boa_constrictor'),
+    ('n01744401', 'rock_python'),
+    ('n01748264', 'Indian_cobra'),
+    ('n01749939', 'green_mamba'),
+    ('n01751748', 'sea_snake'),
+    ('n01753488', 'horned_viper'),
+    ('n01755581', 'diamondback'),
+    ('n01756291', 'sidewinder'),
+    ('n01768244', 'trilobite'),
+    ('n01770081', 'harvestman'),
+    ('n01770393', 'scorpion'),
+    ('n01773157', 'black_and_gold_garden_spider'),
+    ('n01773549', 'barn_spider'),
+    ('n01773797', 'garden_spider'),
+    ('n01774384', 'black_widow'),
+    ('n01774750', 'tarantula'),
+    ('n01775062', 'wolf_spider'),
+    ('n01776313', 'tick'),
+    ('n01784675', 'centipede'),
+    ('n01795545', 'black_grouse'),
+    ('n01796340', 'ptarmigan'),
+    ('n01797886', 'ruffed_grouse'),
+    ('n01798484', 'prairie_chicken'),
+    ('n01806143', 'peacock'),
+    ('n01806567', 'quail'),
+    ('n01807496', 'partridge'),
+    ('n01817953', 'African_grey'),
+    ('n01818515', 'macaw'),
+    ('n01819313', 'sulphur-crested_cockatoo'),
+    ('n01820546', 'lorikeet'),
+    ('n01824575', 'coucal'),
+    ('n01828970', 'bee_eater'),
+    ('n01829413', 'hornbill'),
+    ('n01833805', 'hummingbird'),
+    ('n01843065', 'jacamar'),
+    ('n01843383', 'toucan'),
+    ('n01847000', 'drake'),
+    ('n01855032', 'red-breasted_merganser'),
+    ('n01855672', 'goose'),
+    ('n01860187', 'black_swan'),
+    ('n01871265', 'tusker'),
+    ('n01872401', 'echidna'),
+    ('n01873310', 'platypus'),
+    ('n01877812', 'wallaby'),
+    ('n01882714', 'koala'),
+    ('n01883070', 'wombat'),
+    ('n01910747', 'jellyfish'),
+    ('n01914609', 'sea_anemone'),
+    ('n01917289', 'brain_coral'),
+    ('n01924916', 'flatworm'),
+    ('n01930112', 'nematode'),
+    ('n01943899', 'conch'),
+    ('n01944390', 'snail'),
+    ('n01945685', 'slug'),
+    ('n01950731', 'sea_slug'),
+    ('n01955084', 'chiton'),
+    ('n01968897', 'chambered_nautilus'),
+    ('n01978287', 'Dungeness_crab'),
+    ('n01978455', 'rock_crab'),
+    ('n01980166', 'fiddler_crab'),
+    ('n01981276', 'king_crab'),
+    ('n01983481', 'American_lobster'),
+    ('n01984695', 'spiny_lobster'),
+    ('n01985128', 'crayfish'),
+    ('n01986214', 'hermit_crab'),
+    ('n01990800', 'isopod'),
+    ('n02002556', 'white_stork'),
+    ('n02002724', 'black_stork'),
+    ('n02006656', 'spoonbill'),
+    ('n02007558', 'flamingo'),
+    ('n02009229', 'little_blue_heron'),
+    ('n02009912', 'American_egret'),
+    ('n02011460', 'bittern'),
+    ('n02012849', 'crane'),
+    ('n02013706', 'limpkin'),
+    ('n02017213', 'European_gallinule'),
+    ('n02018207', 'American_coot'),
+    ('n02018795', 'bustard'),
+    ('n02025239', 'ruddy_turnstone'),
+    ('n02027492', 'red-backed_sandpiper'),
+    ('n02028035', 'redshank'),
+    ('n02033041', 'dowitcher'),
+    ('n02037110', 'oystercatcher'),
+    ('n02051845', 'pelican'),
+    ('n02056570', 'king_penguin'),
+    ('n02058221', 'albatross'),
+    ('n02066245', 'grey_whale'),
+    ('n02071294', 'killer_whale'),
+    ('n02074367', 'dugong'),
+    ('n02077923', 'sea_lion'),
+    ('n02085620', 'Chihuahua'),
+    ('n02085782', 'Japanese_spaniel'),
+    ('n02085936', 'Maltese_dog'),
+    ('n02086079', 'Pekinese'),
+    ('n02086240', 'Shih-Tzu'),
+    ('n02086646', 'Blenheim_spaniel'),
+    ('n02086910', 'papillon'),
+    ('n02087046', 'toy_terrier'),
+    ('n02087394', 'Rhodesian_ridgeback'),
+    ('n02088094', 'Afghan_hound'),
+    ('n02088238', 'basset'),
+    ('n02088364', 'beagle'),
+    ('n02088466', 'bloodhound'),
+    ('n02088632', 'bluetick'),
+    ('n02089078', 'black-and-tan_coonhound'),
+    ('n02089867', 'Walker_hound'),
+    ('n02089973', 'English_foxhound'),
+    ('n02090379', 'redbone'),
+    ('n02090622', 'borzoi'),
+    ('n02090721', 'Irish_wolfhound'),
+    ('n02091032', 'Italian_greyhound'),
+    ('n02091134', 'whippet'),
+    ('n02091244', 'Ibizan_hound'),
+    ('n02091467', 'Norwegian_elkhound'),
+    ('n02091635', 'otterhound'),
+    ('n02091831', 'Saluki'),
+    ('n02092002', 'Scottish_deerhound'),
+    ('n02092339', 'Weimaraner'),
+    ('n02093256', 'Staffordshire_bullterrier'),
+    ('n02093428', 'American_Staffordshire_terrier'),
+    ('n02093647', 'Bedlington_terrier'),
+    ('n02093754', 'Border_terrier'),
+    ('n02093859', 'Kerry_blue_terrier'),
+    ('n02093991', 'Irish_terrier'),
+    ('n02094114', 'Norfolk_terrier'),
+    ('n02094258', 'Norwich_terrier'),
+    ('n02094433', 'Yorkshire_terrier'),
+    ('n02095314', 'wire-haired_fox_terrier'),
+    ('n02095570', 'Lakeland_terrier'),
+    ('n02095889', 'Sealyham_terrier'),
+    ('n02096051', 'Airedale'),
+    ('n02096177', 'cairn'),
+    ('n02096294', 'Australian_terrier'),
+    ('n02096437', 'Dandie_Dinmont'),
+    ('n02096585', 'Boston_bull'),
+    ('n02097047', 'miniature_schnauzer'),
+    ('n02097130', 'giant_schnauzer'),
+    ('n02097209', 'standard_schnauzer'),
+    ('n02097298', 'Scotch_terrier'),
+    ('n02097474', 'Tibetan_terrier'),
+    ('n02097658', 'silky_terrier'),
+    ('n02098105', 'soft-coated_wheaten_terrier'),
+    ('n02098286', 'West_Highland_white_terrier'),
+    ('n02098413', 'Lhasa'),
+    ('n02099267', 'flat-coated_retriever'),
+    ('n02099429', 'curly-coated_retriever'),
+    ('n02099601', 'golden_retriever'),
+    ('n02099712', 'Labrador_retriever'),
+    ('n02099849', 'Chesapeake_Bay_retriever'),
+    ('n02100236', 'German_short-haired_pointer'),
+    ('n02100583', 'vizsla'),
+    ('n02100735', 'English_setter'),
+    ('n02100877', 'Irish_setter'),
+    ('n02101006', 'Gordon_setter'),
+    ('n02101388', 'Brittany_spaniel'),
+    ('n02101556', 'clumber'),
+    ('n02102040', 'English_springer'),
+    ('n02102177', 'Welsh_springer_spaniel'),
+    ('n02102318', 'cocker_spaniel'),
+    ('n02102480', 'Sussex_spaniel'),
+    ('n02102973', 'Irish_water_spaniel'),
+    ('n02104029', 'kuvasz'),
+    ('n02104365', 'schipperke'),
+    ('n02105056', 'groenendael'),
+    ('n02105162', 'malinois'),
+    ('n02105251', 'briard'),
+    ('n02105412', 'kelpie'),
+    ('n02105505', 'komondor'),
+    ('n02105641', 'Old_English_sheepdog'),
+    ('n02105855', 'Shetland_sheepdog'),
+    ('n02106030', 'collie'),
+    ('n02106166', 'Border_collie'),
+    ('n02106382', 'Bouvier_des_Flandres'),
+    ('n02106550', 'Rottweiler'),
+    ('n02106662', 'German_shepherd'),
+    ('n02107142', 'Doberman'),
+    ('n02107312', 'miniature_pinscher'),
+    ('n02107574', 'Greater_Swiss_Mountain_dog'),
+    ('n02107683', 'Bernese_mountain_dog'),
+    ('n02107908', 'Appenzeller'),
+    ('n02108000', 'EntleBucher'),
+    ('n02108089', 'boxer'),
+    ('n02108422', 'bull_mastiff'),
+    ('n02108551', 'Tibetan_mastiff'),
+    ('n02108915', 'French_bulldog'),
+    ('n02109047', 'Great_Dane'),
+    ('n02109525', 'Saint_Bernard'),
+    ('n02109961', 'Eskimo_dog'),
+    ('n02110063', 'malamute'),
+    ('n02110185', 'Siberian_husky'),
+    ('n02110341', 'dalmatian'),
+    ('n02110627', 'affenpinscher'),
+    ('n02110806', 'basenji'),
+    ('n02110958', 'pug'),
+    ('n02111129', 'Leonberg'),
+    ('n02111277', 'Newfoundland'),
+    ('n02111500', 'Great_Pyrenees'),
+    ('n02111889', 'Samoyed'),
+    ('n02112018', 'Pomeranian'),
+    ('n02112137', 'chow'),
+    ('n02112350', 'keeshond'),
+    ('n02112706', 'Brabancon_griffon'),
+    ('n02113023', 'Pembroke'),
+    ('n02113186', 'Cardigan'),
+    ('n02113624', 'toy_poodle'),
+    ('n02113712', 'miniature_poodle'),
+    ('n02113799', 'standard_poodle'),
+    ('n02113978', 'Mexican_hairless'),
+    ('n02114367', 'timber_wolf'),
+    ('n02114548', 'white_wolf'),
+    ('n02114712', 'red_wolf'),
+    ('n02114855', 'coyote'),
+    ('n02115641', 'dingo'),
+    ('n02115913', 'dhole'),
+    ('n02116738', 'African_hunting_dog'),
+    ('n02117135', 'hyena'),
+    ('n02119022', 'red_fox'),
+    ('n02119789', 'kit_fox'),
+    ('n02120079', 'Arctic_fox'),
+    ('n02120505', 'grey_fox'),
+    ('n02123045', 'tabby'),
+    ('n02123159', 'tiger_cat'),
+    ('n02123394', 'Persian_cat'),
+    ('n02123597', 'Siamese_cat'),
+    ('n02124075', 'Egyptian_cat'),
+    ('n02125311', 'cougar'),
+    ('n02127052', 'lynx'),
+    ('n02128385', 'leopard'),
+    ('n02128757', 'snow_leopard'),
+    ('n02128925', 'jaguar'),
+    ('n02129165', 'lion'),
+    ('n02129604', 'tiger'),
+    ('n02130308', 'cheetah'),
+    ('n02132136', 'brown_bear'),
+    ('n02133161', 'American_black_bear'),
+    ('n02134084', 'ice_bear'),
+    ('n02134418', 'sloth_bear'),
+    ('n02137549', 'mongoose'),
+    ('n02138441', 'meerkat'),
+    ('n02165105', 'tiger_beetle'),
+    ('n02165456', 'ladybug'),
+    ('n02167151', 'ground_beetle'),
+    ('n02168699', 'long-horned_beetle'),
+    ('n02169497', 'leaf_beetle'),
+    ('n02172182', 'dung_beetle'),
+    ('n02174001', 'rhinoceros_beetle'),
+    ('n02177972', 'weevil'),
+    ('n02190166', 'fly'),
+    ('n02206856', 'bee'),
+    ('n02219486', 'ant'),
+    ('n02226429', 'grasshopper'),
+    ('n02229544', 'cricket'),
+    ('n02231487', 'walking_stick'),
+    ('n02233338', 'cockroach'),
+    ('n02236044', 'mantis'),
+    ('n02256656', 'cicada'),
+    ('n02259212', 'leafhopper'),
+    ('n02264363', 'lacewing'),
+    ('n02268443', 'dragonfly'),
+    ('n02268853', 'damselfly'),
+    ('n02276258', 'admiral'),
+    ('n02277742', 'ringlet'),
+    ('n02279972', 'monarch'),
+    ('n02280649', 'cabbage_butterfly'),
+    ('n02281406', 'sulphur_butterfly'),
+    ('n02281787', 'lycaenid'),
+    ('n02317335', 'starfish'),
+    ('n02319095', 'sea_urchin'),
+    ('n02321529', 'sea_cucumber'),
+    ('n02325366', 'wood_rabbit'),
+    ('n02326432', 'hare'),
+    ('n02328150', 'Angora'),
+    ('n02342885', 'hamster'),
+    ('n02346627', 'porcupine'),
+    ('n02356798', 'fox_squirrel'),
+    ('n02361337', 'marmot'),
+    ('n02363005', 'beaver'),
+    ('n02364673', 'guinea_pig'),
+    ('n02389026', 'sorrel'),
+    ('n02391049', 'zebra'),
+    ('n02395406', 'hog'),
+    ('n02396427', 'wild_boar'),
+    ('n02397096', 'warthog'),
+    ('n02398521', 'hippopotamus'),
+    ('n02403003', 'ox'),
+    ('n02408429', 'water_buffalo'),
+    ('n02410509', 'bison'),
+    ('n02412080', 'ram'),
+    ('n02415577', 'bighorn'),
+    ('n02417914', 'ibex'),
+    ('n02422106', 'hartebeest'),
+    ('n02422699', 'impala'),
+    ('n02423022', 'gazelle'),
+    ('n02437312', 'Arabian_camel'),
+    ('n02437616', 'llama'),
+    ('n02441942', 'weasel'),
+    ('n02442845', 'mink'),
+    ('n02443114', 'polecat'),
+    ('n02443484', 'black-footed_ferret'),
+    ('n02444819', 'otter'),
+    ('n02445715', 'skunk'),
+    ('n02447366', 'badger'),
+    ('n02454379', 'armadillo'),
+    ('n02457408', 'three-toed_sloth'),
+    ('n02480495', 'orangutan'),
+    ('n02480855', 'gorilla'),
+    ('n02481823', 'chimpanzee'),
+    ('n02483362', 'gibbon'),
+    ('n02483708', 'siamang'),
+    ('n02484975', 'guenon'),
+    ('n02486261', 'patas'),
+    ('n02486410', 'baboon'),
+    ('n02487347', 'macaque'),
+    ('n02488291', 'langur'),
+    ('n02488702', 'colobus'),
+    ('n02489166', 'proboscis_monkey'),
+    ('n02490219', 'marmoset'),
+    ('n02492035', 'capuchin'),
+    ('n02492660', 'howler_monkey'),
+    ('n02493509', 'titi'),
+    ('n02493793', 'spider_monkey'),
+    ('n02494079', 'squirrel_monkey'),
+    ('n02497673', 'Madagascar_cat'),
+    ('n02500267', 'indri'),
+    ('n02504013', 'Indian_elephant'),
+    ('n02504458', 'African_elephant'),
+    ('n02509815', 'lesser_panda'),
+    ('n02510455', 'giant_panda'),
+    ('n02514041', 'barracouta'),
+    ('n02526121', 'eel'),
+    ('n02536864', 'coho'),
+    ('n02606052', 'rock_beauty'),
+    ('n02607072', 'anemone_fish'),
+    ('n02640242', 'sturgeon'),
+    ('n02641379', 'gar'),
+    ('n02643566', 'lionfish'),
+    ('n02655020', 'puffer'),
+    ('n02666196', 'abacus'),
+    ('n02667093', 'abaya'),
+    ('n02669723', 'academic_gown'),
+    ('n02672831', 'accordion'),
+    ('n02676566', 'acoustic_guitar'),
+    ('n02687172', 'aircraft_carrier'),
+    ('n02690373', 'airliner'),
+    ('n02692877', 'airship'),
+    ('n02699494', 'altar'),
+    ('n02701002', 'ambulance'),
+    ('n02704792', 'amphibian'),
+    ('n02708093', 'analog_clock'),
+    ('n02727426', 'apiary'),
+    ('n02730930', 'apron'),
+    ('n02747177', 'ashcan'),
+    ('n02749479', 'assault_rifle'),
+    ('n02769748', 'backpack'),
+    ('n02776631', 'bakery'),
+    ('n02777292', 'balance_beam'),
+    ('n02782093', 'balloon'),
+    ('n02783161', 'ballpoint'),
+    ('n02786058', 'Band_Aid'),
+    ('n02787622', 'banjo'),
+    ('n02788148', 'bannister'),
+    ('n02790996', 'barbell'),
+    ('n02791124', 'barber_chair'),
+    ('n02791270', 'barbershop'),
+    ('n02793495', 'barn'),
+    ('n02794156', 'barometer'),
+    ('n02795169', 'barrel'),
+    ('n02797295', 'barrow'),
+    ('n02799071', 'baseball'),
+    ('n02802426', 'basketball'),
+    ('n02804414', 'bassinet'),
+    ('n02804610', 'bassoon'),
+    ('n02807133', 'bathing_cap'),
+    ('n02808304', 'bath_towel'),
+    ('n02808440', 'bathtub'),
+    ('n02814533', 'beach_wagon'),
+    ('n02814860', 'beacon'),
+    ('n02815834', 'beaker'),
+    ('n02817516', 'bearskin'),
+    ('n02823428', 'beer_bottle'),
+    ('n02823750', 'beer_glass'),
+    ('n02825657', 'bell_cote'),
+    ('n02834397', 'bib'),
+    ('n02835271', 'bicycle-built-for-two'),
+    ('n02837789', 'bikini'),
+    ('n02840245', 'binder'),
+    ('n02841315', 'binoculars'),
+    ('n02843684', 'birdhouse'),
+    ('n02859443', 'boathouse'),
+    ('n02860847', 'bobsled'),
+    ('n02865351', 'bolo_tie'),
+    ('n02869837', 'bonnet'),
+    ('n02870880', 'bookcase'),
+    ('n02871525', 'bookshop'),
+    ('n02877765', 'bottlecap'),
+    ('n02879718', 'bow'),
+    ('n02883205', 'bow_tie'),
+    ('n02892201', 'brass'),
+    ('n02892767', 'brassiere'),
+    ('n02894605', 'breakwater'),
+    ('n02895154', 'breastplate'),
+    ('n02906734', 'broom'),
+    ('n02909870', 'bucket'),
+    ('n02910353', 'buckle'),
+    ('n02916936', 'bulletproof_vest'),
+    ('n02917067', 'bullet_train'),
+    ('n02927161', 'butcher_shop'),
+    ('n02930766', 'cab'),
+    ('n02939185', 'caldron'),
+    ('n02948072', 'candle'),
+    ('n02950826', 'cannon'),
+    ('n02951358', 'canoe'),
+    ('n02951585', 'can_opener'),
+    ('n02963159', 'cardigan'),
+    ('n02965783', 'car_mirror'),
+    ('n02966193', 'carousel'),
+    ('n02966687', "carpenter's_kit"),
+    ('n02971356', 'carton'),
+    ('n02974003', 'car_wheel'),
+    ('n02977058', 'cash_machine'),
+    ('n02978881', 'cassette'),
+    ('n02979186', 'cassette_player'),
+    ('n02980441', 'castle'),
+    ('n02981792', 'catamaran'),
+    ('n02988304', 'CD_player'),
+    ('n02992211', 'cello'),
+    ('n02992529', 'cellular_telephone'),
+    ('n02999410', 'chain'),
+    ('n03000134', 'chainlink_fence'),
+    ('n03000247', 'chain_mail'),
+    ('n03000684', 'chain_saw'),
+    ('n03014705', 'chest'),
+    ('n03016953', 'chiffonier'),
+    ('n03017168', 'chime'),
+    ('n03018349', 'china_cabinet'),
+    ('n03026506', 'Christmas_stocking'),
+    ('n03028079', 'church'),
+    ('n03032252', 'cinema'),
+    ('n03041632', 'cleaver'),
+    ('n03042490', 'cliff_dwelling'),
+    ('n03045698', 'cloak'),
+    ('n03047690', 'clog'),
+    ('n03062245', 'cocktail_shaker'),
+    ('n03063599', 'coffee_mug'),
+    ('n03063689', 'coffeepot'),
+    ('n03065424', 'coil'),
+    ('n03075370', 'combination_lock'),
+    ('n03085013', 'computer_keyboard'),
+    ('n03089624', 'confectionery'),
+    ('n03095699', 'container_ship'),
+    ('n03100240', 'convertible'),
+    ('n03109150', 'corkscrew'),
+    ('n03110669', 'cornet'),
+    ('n03124043', 'cowboy_boot'),
+    ('n03124170', 'cowboy_hat'),
+    ('n03125729', 'cradle'),
+    ('n03126707', 'crane'),
+    ('n03127747', 'crash_helmet'),
+    ('n03127925', 'crate'),
+    ('n03131574', 'crib'),
+    ('n03133878', 'Crock_Pot'),
+    ('n03134739', 'croquet_ball'),
+    ('n03141823', 'crutch'),
+    ('n03146219', 'cuirass'),
+    ('n03160309', 'dam'),
+    ('n03179701', 'desk'),
+    ('n03180011', 'desktop_computer'),
+    ('n03187595', 'dial_telephone'),
+    ('n03188531', 'diaper'),
+    ('n03196217', 'digital_clock'),
+    ('n03197337', 'digital_watch'),
+    ('n03201208', 'dining_table'),
+    ('n03207743', 'dishrag'),
+    ('n03207941', 'dishwasher'),
+    ('n03208938', 'disk_brake'),
+    ('n03216828', 'dock'),
+    ('n03218198', 'dogsled'),
+    ('n03220513', 'dome'),
+    ('n03223299', 'doormat'),
+    ('n03240683', 'drilling_platform'),
+    ('n03249569', 'drum'),
+    ('n03250847', 'drumstick'),
+    ('n03255030', 'dumbbell'),
+    ('n03259280', 'Dutch_oven'),
+    ('n03271574', 'electric_fan'),
+    ('n03272010', 'electric_guitar'),
+    ('n03272562', 'electric_locomotive'),
+    ('n03290653', 'entertainment_center'),
+    ('n03291819', 'envelope'),
+    ('n03297495', 'espresso_maker'),
+    ('n03314780', 'face_powder'),
+    ('n03325584', 'feather_boa'),
+    ('n03337140', 'file'),
+    ('n03344393', 'fireboat'),
+    ('n03345487', 'fire_engine'),
+    ('n03347037', 'fire_screen'),
+    ('n03355925', 'flagpole'),
+    ('n03372029', 'flute'),
+    ('n03376595', 'folding_chair'),
+    ('n03379051', 'football_helmet'),
+    ('n03384352', 'forklift'),
+    ('n03388043', 'fountain'),
+    ('n03388183', 'fountain_pen'),
+    ('n03388549', 'four-poster'),
+    ('n03393912', 'freight_car'),
+    ('n03394916', 'French_horn'),
+    ('n03400231', 'frying_pan'),
+    ('n03404251', 'fur_coat'),
+    ('n03417042', 'garbage_truck'),
+    ('n03424325', 'gasmask'),
+    ('n03425413', 'gas_pump'),
+    ('n03443371', 'goblet'),
+    ('n03444034', 'go-kart'),
+    ('n03445777', 'golf_ball'),
+    ('n03445924', 'golfcart'),
+    ('n03447447', 'gondola'),
+    ('n03447721', 'gong'),
+    ('n03450230', 'gown'),
+    ('n03452741', 'grand_piano'),
+    ('n03457902', 'greenhouse'),
+    ('n03459775', 'grille'),
+    ('n03461385', 'grocery_store'),
+    ('n03467068', 'guillotine'),
+    ('n03476684', 'hair_slide'),
+    ('n03476991', 'hair_spray'),
+    ('n03478589', 'half_track'),
+    ('n03481172', 'hammer'),
+    ('n03482405', 'hamper'),
+    ('n03483316', 'hand_blower'),
+    ('n03485407', 'hand-held_computer'),
+    ('n03485794', 'handkerchief'),
+    ('n03492542', 'hard_disc'),
+    ('n03494278', 'harmonica'),
+    ('n03495258', 'harp'),
+    ('n03496892', 'harvester'),
+    ('n03498962', 'hatchet'),
+    ('n03527444', 'holster'),
+    ('n03529860', 'home_theater'),
+    ('n03530642', 'honeycomb'),
+    ('n03532672', 'hook'),
+    ('n03534580', 'hoopskirt'),
+    ('n03535780', 'horizontal_bar'),
+    ('n03538406', 'horse_cart'),
+    ('n03544143', 'hourglass'),
+    ('n03584254', 'iPod'),
+    ('n03584829', 'iron'),
+    ('n03590841', "jack-o'-lantern"),
+    ('n03594734', 'jean'),
+    ('n03594945', 'jeep'),
+    ('n03595614', 'jersey'),
+    ('n03598930', 'jigsaw_puzzle'),
+    ('n03599486', 'jinrikisha'),
+    ('n03602883', 'joystick'),
+    ('n03617480', 'kimono'),
+    ('n03623198', 'knee_pad'),
+    ('n03627232', 'knot'),
+    ('n03630383', 'lab_coat'),
+    ('n03633091', 'ladle'),
+    ('n03637318', 'lampshade'),
+    ('n03642806', 'laptop'),
+    ('n03649909', 'lawn_mower'),
+    ('n03657121', 'lens_cap'),
+    ('n03658185', 'letter_opener'),
+    ('n03661043', 'library'),
+    ('n03662601', 'lifeboat'),
+    ('n03666591', 'lighter'),
+    ('n03670208', 'limousine'),
+    ('n03673027', 'liner'),
+    ('n03676483', 'lipstick'),
+    ('n03680355', 'Loafer'),
+    ('n03690938', 'lotion'),
+    ('n03691459', 'loudspeaker'),
+    ('n03692522', 'loupe'),
+    ('n03697007', 'lumbermill'),
+    ('n03706229', 'magnetic_compass'),
+    ('n03709823', 'mailbag'),
+    ('n03710193', 'mailbox'),
+    ('n03710637', 'maillot'),
+    ('n03710721', 'maillot'),
+    ('n03717622', 'manhole_cover'),
+    ('n03720891', 'maraca'),
+    ('n03721384', 'marimba'),
+    ('n03724870', 'mask'),
+    ('n03729826', 'matchstick'),
+    ('n03733131', 'maypole'),
+    ('n03733281', 'maze'),
+    ('n03733805', 'measuring_cup'),
+    ('n03742115', 'medicine_chest'),
+    ('n03743016', 'megalith'),
+    ('n03759954', 'microphone'),
+    ('n03761084', 'microwave'),
+    ('n03763968', 'military_uniform'),
+    ('n03764736', 'milk_can'),
+    ('n03769881', 'minibus'),
+    ('n03770439', 'miniskirt'),
+    ('n03770679', 'minivan'),
+    ('n03773504', 'missile'),
+    ('n03775071', 'mitten'),
+    ('n03775546', 'mixing_bowl'),
+    ('n03776460', 'mobile_home'),
+    ('n03777568', 'Model_T'),
+    ('n03777754', 'modem'),
+    ('n03781244', 'monastery'),
+    ('n03782006', 'monitor'),
+    ('n03785016', 'moped'),
+    ('n03786901', 'mortar'),
+    ('n03787032', 'mortarboard'),
+    ('n03788195', 'mosque'),
+    ('n03788365', 'mosquito_net'),
+    ('n03791053', 'motor_scooter'),
+    ('n03792782', 'mountain_bike'),
+    ('n03792972', 'mountain_tent'),
+    ('n03793489', 'mouse'),
+    ('n03794056', 'mousetrap'),
+    ('n03796401', 'moving_van'),
+    ('n03803284', 'muzzle'),
+    ('n03804744', 'nail'),
+    ('n03814639', 'neck_brace'),
+    ('n03814906', 'necklace'),
+    ('n03825788', 'nipple'),
+    ('n03832673', 'notebook'),
+    ('n03837869', 'obelisk'),
+    ('n03838899', 'oboe'),
+    ('n03840681', 'ocarina'),
+    ('n03841143', 'odometer'),
+    ('n03843555', 'oil_filter'),
+    ('n03854065', 'organ'),
+    ('n03857828', 'oscilloscope'),
+    ('n03866082', 'overskirt'),
+    ('n03868242', 'oxcart'),
+    ('n03868863', 'oxygen_mask'),
+    ('n03871628', 'packet'),
+    ('n03873416', 'paddle'),
+    ('n03874293', 'paddlewheel'),
+    ('n03874599', 'padlock'),
+    ('n03876231', 'paintbrush'),
+    ('n03877472', 'pajama'),
+    ('n03877845', 'palace'),
+    ('n03884397', 'panpipe'),
+    ('n03887697', 'paper_towel'),
+    ('n03888257', 'parachute'),
+    ('n03888605', 'parallel_bars'),
+    ('n03891251', 'park_bench'),
+    ('n03891332', 'parking_meter'),
+    ('n03895866', 'passenger_car'),
+    ('n03899768', 'patio'),
+    ('n03902125', 'pay-phone'),
+    ('n03903868', 'pedestal'),
+    ('n03908618', 'pencil_box'),
+    ('n03908714', 'pencil_sharpener'),
+    ('n03916031', 'perfume'),
+    ('n03920288', 'Petri_dish'),
+    ('n03924679', 'photocopier'),
+    ('n03929660', 'pick'),
+    ('n03929855', 'pickelhaube'),
+    ('n03930313', 'picket_fence'),
+    ('n03930630', 'pickup'),
+    ('n03933933', 'pier'),
+    ('n03935335', 'piggy_bank'),
+    ('n03937543', 'pill_bottle'),
+    ('n03938244', 'pillow'),
+    ('n03942813', 'ping-pong_ball'),
+    ('n03944341', 'pinwheel'),
+    ('n03947888', 'pirate'),
+    ('n03950228', 'pitcher'),
+    ('n03954731', 'plane'),
+    ('n03956157', 'planetarium'),
+    ('n03958227', 'plastic_bag'),
+    ('n03961711', 'plate_rack'),
+    ('n03967562', 'plow'),
+    ('n03970156', 'plunger'),
+    ('n03976467', 'Polaroid_camera'),
+    ('n03976657', 'pole'),
+    ('n03977966', 'police_van'),
+    ('n03980874', 'poncho'),
+    ('n03982430', 'pool_table'),
+    ('n03983396', 'pop_bottle'),
+    ('n03991062', 'pot'),
+    ('n03992509', "potter's_wheel"),
+    ('n03995372', 'power_drill'),
+    ('n03998194', 'prayer_rug'),
+    ('n04004767', 'printer'),
+    ('n04005630', 'prison'),
+    ('n04008634', 'projectile'),
+    ('n04009552', 'projector'),
+    ('n04019541', 'puck'),
+    ('n04023962', 'punching_bag'),
+    ('n04026417', 'purse'),
+    ('n04033901', 'quill'),
+    ('n04033995', 'quilt'),
+    ('n04037443', 'racer'),
+    ('n04039381', 'racket'),
+    ('n04040759', 'radiator'),
+    ('n04041544', 'radio'),
+    ('n04044716', 'radio_telescope'),
+    ('n04049303', 'rain_barrel'),
+    ('n04065272', 'recreational_vehicle'),
+    ('n04067472', 'reel'),
+    ('n04069434', 'reflex_camera'),
+    ('n04070727', 'refrigerator'),
+    ('n04074963', 'remote_control'),
+    ('n04081281', 'restaurant'),
+    ('n04086273', 'revolver'),
+    ('n04090263', 'rifle'),
+    ('n04099969', 'rocking_chair'),
+    ('n04111531', 'rotisserie'),
+    ('n04116512', 'rubber_eraser'),
+    ('n04118538', 'rugby_ball'),
+    ('n04118776', 'rule'),
+    ('n04120489', 'running_shoe'),
+    ('n04125021', 'safe'),
+    ('n04127249', 'safety_pin'),
+    ('n04131690', 'saltshaker'),
+    ('n04133789', 'sandal'),
+    ('n04136333', 'sarong'),
+    ('n04141076', 'sax'),
+    ('n04141327', 'scabbard'),
+    ('n04141975', 'scale'),
+    ('n04146614', 'school_bus'),
+    ('n04147183', 'schooner'),
+    ('n04149813', 'scoreboard'),
+    ('n04152593', 'screen'),
+    ('n04153751', 'screw'),
+    ('n04154565', 'screwdriver'),
+    ('n04162706', 'seat_belt'),
+    ('n04179913', 'sewing_machine'),
+    ('n04192698', 'shield'),
+    ('n04200800', 'shoe_shop'),
+    ('n04201297', 'shoji'),
+    ('n04204238', 'shopping_basket'),
+    ('n04204347', 'shopping_cart'),
+    ('n04208210', 'shovel'),
+    ('n04209133', 'shower_cap'),
+    ('n04209239', 'shower_curtain'),
+    ('n04228054', 'ski'),
+    ('n04229816', 'ski_mask'),
+    ('n04235860', 'sleeping_bag'),
+    ('n04238763', 'slide_rule'),
+    ('n04239074', 'sliding_door'),
+    ('n04243546', 'slot'),
+    ('n04251144', 'snorkel'),
+    ('n04252077', 'snowmobile'),
+    ('n04252225', 'snowplow'),
+    ('n04254120', 'soap_dispenser'),
+    ('n04254680', 'soccer_ball'),
+    ('n04254777', 'sock'),
+    ('n04258138', 'solar_dish'),
+    ('n04259630', 'sombrero'),
+    ('n04263257', 'soup_bowl'),
+    ('n04264628', 'space_bar'),
+    ('n04265275', 'space_heater'),
+    ('n04266014', 'space_shuttle'),
+    ('n04270147', 'spatula'),
+    ('n04273569', 'speedboat'),
+    ('n04275548', 'spider_web'),
+    ('n04277352', 'spindle'),
+    ('n04285008', 'sports_car'),
+    ('n04286575', 'spotlight'),
+    ('n04296562', 'stage'),
+    ('n04310018', 'steam_locomotive'),
+    ('n04311004', 'steel_arch_bridge'),
+    ('n04311174', 'steel_drum'),
+    ('n04317175', 'stethoscope'),
+    ('n04325704', 'stole'),
+    ('n04326547', 'stone_wall'),
+    ('n04328186', 'stopwatch'),
+    ('n04330267', 'stove'),
+    ('n04332243', 'strainer'),
+    ('n04335435', 'streetcar'),
+    ('n04336792', 'stretcher'),
+    ('n04344873', 'studio_couch'),
+    ('n04346328', 'stupa'),
+    ('n04347754', 'submarine'),
+    ('n04350905', 'suit'),
+    ('n04355338', 'sundial'),
+    ('n04355933', 'sunglass'),
+    ('n04356056', 'sunglasses'),
+    ('n04357314', 'sunscreen'),
+    ('n04366367', 'suspension_bridge'),
+    ('n04367480', 'swab'),
+    ('n04370456', 'sweatshirt'),
+    ('n04371430', 'swimming_trunks'),
+    ('n04371774', 'swing'),
+    ('n04372370', 'switch'),
+    ('n04376876', 'syringe'),
+    ('n04380533', 'table_lamp'),
+    ('n04389033', 'tank'),
+    ('n04392985', 'tape_player'),
+    ('n04398044', 'teapot'),
+    ('n04399382', 'teddy'),
+    ('n04404412', 'television'),
+    ('n04409515', 'tennis_ball'),
+    ('n04417672', 'thatch'),
+    ('n04418357', 'theater_curtain'),
+    ('n04423845', 'thimble'),
+    ('n04428191', 'thresher'),
+    ('n04429376', 'throne'),
+    ('n04435653', 'tile_roof'),
+    ('n04442312', 'toaster'),
+    ('n04443257', 'tobacco_shop'),
+    ('n04447861', 'toilet_seat'),
+    ('n04456115', 'torch'),
+    ('n04458633', 'totem_pole'),
+    ('n04461696', 'tow_truck'),
+    ('n04462240', 'toyshop'),
+    ('n04465501', 'tractor'),
+    ('n04467665', 'trailer_truck'),
+    ('n04476259', 'tray'),
+    ('n04479046', 'trench_coat'),
+    ('n04482393', 'tricycle'),
+    ('n04483307', 'trimaran'),
+    ('n04485082', 'tripod'),
+    ('n04486054', 'triumphal_arch'),
+    ('n04487081', 'trolleybus'),
+    ('n04487394', 'trombone'),
+    ('n04493381', 'tub'),
+    ('n04501370', 'turnstile'),
+    ('n04505470', 'typewriter_keyboard'),
+    ('n04507155', 'umbrella'),
+    ('n04509417', 'unicycle'),
+    ('n04515003', 'upright'),
+    ('n04517823', 'vacuum'),
+    ('n04522168', 'vase'),
+    ('n04523525', 'vault'),
+    ('n04525038', 'velvet'),
+    ('n04525305', 'vending_machine'),
+    ('n04532106', 'vestment'),
+    ('n04532670', 'viaduct'),
+    ('n04536866', 'violin'),
+    ('n04540053', 'volleyball'),
+    ('n04542943', 'waffle_iron'),
+    ('n04548280', 'wall_clock'),
+    ('n04548362', 'wallet'),
+    ('n04550184', 'wardrobe'),
+    ('n04552348', 'warplane'),
+    ('n04553703', 'washbasin'),
+    ('n04554684', 'washer'),
+    ('n04557648', 'water_bottle'),
+    ('n04560804', 'water_jug'),
+    ('n04562935', 'water_tower'),
+    ('n04579145', 'whiskey_jug'),
+    ('n04579432', 'whistle'),
+    ('n04584207', 'wig'),
+    ('n04589890', 'window_screen'),
+    ('n04590129', 'window_shade'),
+    ('n04591157', 'Windsor_tie'),
+    ('n04591713', 'wine_bottle'),
+    ('n04592741', 'wing'),
+    ('n04596742', 'wok'),
+    ('n04597913', 'wooden_spoon'),
+    ('n04599235', 'wool'),
+    ('n04604644', 'worm_fence'),
+    ('n04606251', 'wreck'),
+    ('n04612504', 'yawl'),
+    ('n04613696', 'yurt'),
+    ('n06359193', 'web_site'),
+    ('n06596364', 'comic_book'),
+    ('n06785654', 'crossword_puzzle'),
+    ('n06794110', 'street_sign'),
+    ('n06874185', 'traffic_light'),
+    ('n07248320', 'book_jacket'),
+    ('n07565083', 'menu'),
+    ('n07579787', 'plate'),
+    ('n07583066', 'guacamole'),
+    ('n07584110', 'consomme'),
+    ('n07590611', 'hot_pot'),
+    ('n07613480', 'trifle'),
+    ('n07614500', 'ice_cream'),
+    ('n07615774', 'ice_lolly'),
+    ('n07684084', 'French_loaf'),
+    ('n07693725', 'bagel'),
+    ('n07695742', 'pretzel'),
+    ('n07697313', 'cheeseburger'),
+    ('n07697537', 'hotdog'),
+    ('n07711569', 'mashed_potato'),
+    ('n07714571', 'head_cabbage'),
+    ('n07714990', 'broccoli'),
+    ('n07715103', 'cauliflower'),
+    ('n07716358', 'zucchini'),
+    ('n07716906', 'spaghetti_squash'),
+    ('n07717410', 'acorn_squash'),
+    ('n07717556', 'butternut_squash'),
+    ('n07718472', 'cucumber'),
+    ('n07718747', 'artichoke'),
+    ('n07720875', 'bell_pepper'),
+    ('n07730033', 'cardoon'),
+    ('n07734744', 'mushroom'),
+    ('n07742313', 'Granny_Smith'),
+    ('n07745940', 'strawberry'),
+    ('n07747607', 'orange'),
+    ('n07749582', 'lemon'),
+    ('n07753113', 'fig'),
+    ('n07753275', 'pineapple'),
+    ('n07753592', 'banana'),
+    ('n07754684', 'jackfruit'),
+    ('n07760859', 'custard_apple'),
+    ('n07768694', 'pomegranate'),
+    ('n07802026', 'hay'),
+    ('n07831146', 'carbonara'),
+    ('n07836838', 'chocolate_sauce'),
+    ('n07860988', 'dough'),
+    ('n07871810', 'meat_loaf'),
+    ('n07873807', 'pizza'),
+    ('n07875152', 'potpie'),
+    ('n07880968', 'burrito'),
+    ('n07892512', 'red_wine'),
+    ('n07920052', 'espresso'),
+    ('n07930864', 'cup'),
+    ('n07932039', 'eggnog'),
+    ('n09193705', 'alp'),
+    ('n09229709', 'bubble'),
+    ('n09246464', 'cliff'),
+    ('n09256479', 'coral_reef'),
+    ('n09288635', 'geyser'),
+    ('n09332890', 'lakeside'),
+    ('n09399592', 'promontory'),
+    ('n09421951', 'sandbar'),
+    ('n09428293', 'seashore'),
+    ('n09468604', 'valley'),
+    ('n09472597', 'volcano'),
+    ('n09835506', 'ballplayer'),
+    ('n10148035', 'groom'),
+    ('n10565667', 'scuba_diver'),
+    ('n11879895', 'rapeseed'),
+    ('n11939491', 'daisy'),
+    ('n12057211', "yellow_lady's_slipper"),
+    ('n12144580', 'corn'),
+    ('n12267677', 'acorn'),
+    ('n12620546', 'hip'),
+    ('n12768682', 'buckeye'),
+    ('n12985857', 'coral_fungus'),
+    ('n12998815', 'agaric'),
+    ('n13037406', 'gyromitra'),
+    ('n13040303', 'stinkhorn'),
+    ('n13044778', 'earthstar'),
+    ('n13052670', 'hen-of-the-woods'),
+    ('n13054560', 'bolete'),
+    ('n13133613', 'ear'),
+    ('n15075141', 'toilet_tissue'),
   )
 
 
 mean = [0.485, 0.456, 0.406]
-std = [0.229, 0.224, 0.225]
\ No newline at end of file
+std = [0.229, 0.224, 0.225]
diff --git a/yann/datasets/imagenette.py b/yann/datasets/imagenette.py
index 2609c3e..06f3d66 100644
--- a/yann/datasets/imagenette.py
+++ b/yann/datasets/imagenette.py
@@ -1,8 +1,9 @@
-from pathlib import Path
 import os
+import random
 import tarfile
 from glob import iglob
-import random
+from pathlib import Path
+
 from torchvision.datasets import utils
 
 from . import ClassificationDataset
@@ -11,17 +12,18 @@ from . import ClassificationDataset
 def extract(src, dst=None):
   dst = dst or os.path.dirname(src)
   with tarfile.open(src, 'r:gz') as tar:
-      tar.extractall(path=dst)
+    tar.extractall(path=dst)
 
 
 class Imagenette(ClassificationDataset):
   """
   https://github.com/fastai/imagenette
   """
+
   urls = {
     160: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz',
     320: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz',
-    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz'
+    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz',
   }
 
   @classmethod
@@ -32,11 +34,18 @@ class Imagenette(ClassificationDataset):
   def get_filename(cls, url):
     return url.split('/')[-1]
 
-  def __init__(self, root='./datasets/', size=None, split='train', download=True, shuffle=True):
+  def __init__(
+    self,
+    root='./datasets/',
+    size=None,
+    split='train',
+    download=True,
+    shuffle=True,
+  ):
     if size not in self.urls:
       raise ValueError(
-        f"Unsupported size '{size}', "
-        f"must be one of '{', '.join(self.urls.keys())}'")
+        f"Unsupported size '{size}', must be one of '{', '.join(self.urls.keys())}'",
+      )
     self.size = size
     self.root = Path(root).expanduser()
     self.size_root = self.root / self.get_dirname(self.urls[self.size])
@@ -46,16 +55,16 @@ class Imagenette(ClassificationDataset):
     if not self.size_root.exists():
       if not download:
         raise ValueError(
-          f'Could not find dataset at provided root ({self.size_root}) and download=False'
+          f'Could not find dataset at provided root ({self.size_root}) and download=False',
         )
       self.download()
 
     paths = list(iglob(f'{self.size_root}/{self.split}/**/*.*', recursive=True))
     if shuffle:
       random.shuffle(paths)
-    self.inputs, self.targets = list(zip(*(
-      (p, p.split('/')[-2]) for p in paths
-    )))
+    self.inputs, self.targets = list(
+      zip(*((p, p.split('/')[-2]) for p in paths)),
+    )
 
     super(Imagenette, self).__init__(classes=sorted(set(self.targets)))
 
@@ -75,49 +84,43 @@ class Imagenette(ClassificationDataset):
 
 class Imagenette160(Imagenette):
   size = 160
+
   def __init__(self, **kwargs):
     if 'size' in kwargs and kwargs['size'] != self.__class__.size:
       raise ValueError('size is not a valid argument')
-    super(Imagenette160, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    super(Imagenette160, self).__init__(size=self.__class__.size, **kwargs)
+
 
 class Imagenette320(Imagenette):
   size = 320
+
   def __init__(self, **kwargs):
     if 'size' in kwargs and kwargs['size'] != self.__class__.size:
       raise ValueError('size is not a valid argument')
-    super(Imagenette320, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    super(Imagenette320, self).__init__(size=self.__class__.size, **kwargs)
+
 
 class Imagewoof(Imagenette):
   urls = {
     160: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160.tgz',
     320: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-320.tgz',
-    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof.tgz'
+    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof.tgz',
   }
 
 
 class Imagewoof160(Imagenette):
   size = 160
+
   def __init__(self, **kwargs):
     if 'size' in kwargs and kwargs['size'] != self.__class__.size:
       raise ValueError('size is not a valid argument')
-    super(Imagewoof160, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    super(Imagewoof160, self).__init__(size=self.__class__.size, **kwargs)
 
 
 class Imagewoof320(Imagenette):
   size = 320
+
   def __init__(self, **kwargs):
     if 'size' in kwargs and kwargs['size'] != self.__class__.size:
       raise ValueError('size is not a valid argument')
-    super(Imagewoof320, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
\ No newline at end of file
+    super(Imagewoof320, self).__init__(size=self.__class__.size, **kwargs)
diff --git a/yann/datasets/voc.py b/yann/datasets/voc.py
index 144350d..bac0bc8 100644
--- a/yann/datasets/voc.py
+++ b/yann/datasets/voc.py
@@ -1,24 +1,25 @@
 import collections
-from typing import Any, Callable, Dict, Optional, Tuple, List
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
-from torchvision.datasets.voc import _VOCBase, ET_Element, ET_parse
+from torchvision.datasets.voc import ET_Element, ET_parse, _VOCBase
 
 from yann.data import Classes
 
+
 class VOCMultilabel(_VOCBase):
-  _SPLITS_DIR = "Main"
-  _TARGET_DIR = "Annotations"
-  _TARGET_FILE_EXT = ".xml"
+  _SPLITS_DIR = 'Main'
+  _TARGET_DIR = 'Annotations'
+  _TARGET_FILE_EXT = '.xml'
 
   def __init__(
-      self,
-      root: str,
-      year: str = "2012",
-      image_set: str = "train",
-      download: bool = False,
-      transform: Optional[Callable] = None,
-      target_transform: Optional[Callable] = None,
-      transforms: Optional[Callable] = None,
+    self,
+    root: str,
+    year: str = '2012',
+    image_set: str = 'train',
+    download: bool = False,
+    transform: Optional[Callable] = None,
+    target_transform: Optional[Callable] = None,
+    transforms: Optional[Callable] = None,
   ):
     super(VOCMultilabel, self).__init__(
       root=root,
@@ -27,12 +28,11 @@ class VOCMultilabel(_VOCBase):
       download=download,
       transform=transform,
       target_transform=target_transform,
-      transforms=transforms
+      transforms=transforms,
     )
 
     self.classes = Classes.from_labels(x[1] for x in self)
 
-
   @property
   def annotations(self) -> List[str]:
     return self.targets
@@ -59,13 +59,13 @@ class VOCMultilabel(_VOCBase):
       for dc in map(self.parse_voc_xml, children):
         for ind, v in dc.items():
           def_dic[ind].append(v)
-      if node.tag == "annotation":
-        def_dic["object"] = [def_dic["object"]]
-      voc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in
-                             def_dic.items()}
-                  }
+      if node.tag == 'annotation':
+        def_dic['object'] = [def_dic['object']]
+      voc_dict = {
+        node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()},
+      }
     if node.text:
       text = node.text.strip()
       if not children:
         voc_dict[node.tag] = text
-    return voc_dict
\ No newline at end of file
+    return voc_dict
diff --git a/yann/datasets/wrappers.py b/yann/datasets/wrappers.py
index 21c8eb8..ff64ae0 100644
--- a/yann/datasets/wrappers.py
+++ b/yann/datasets/wrappers.py
@@ -1,9 +1,9 @@
-from itertools import zip_longest
 import logging
-from typing import Union
+import math
 import typing
+from itertools import zip_longest
+from typing import Union
 
-import math
 import numpy as np
 import torch
 
@@ -62,16 +62,22 @@ class Sliceable(DatasetWrapper):
 
 class Subset(DatasetWrapper):
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, indices: Union[np.ndarray, torch.Tensor]):
-    ...
+  def __init__(
+    self,
+    dataset: typing.Mapping,
+    indices: Union[np.ndarray, torch.Tensor],
+  ): ...
 
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, end: Union[float, int]):
-    ...
+  def __init__(self, dataset: typing.Mapping, end: Union[float, int]): ...
 
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, start: Union[float, int], end: Union[float, int]):
-    ...
+  def __init__(
+    self,
+    dataset: typing.Mapping,
+    start: Union[float, int],
+    end: Union[float, int],
+  ): ...
 
   def __init__(self, dataset, *args):
     super(Subset, self).__init__(dataset)
@@ -83,7 +89,9 @@ class Subset(DatasetWrapper):
     if len(args) == 1:
       if isinstance(args[0], int):
         self.start = 0
-        self.end = args[0] if not (0 < args[0] < 1) else math.floor(len(dataset) * args[0])
+        self.end = (
+          args[0] if not (0 < args[0] < 1) else math.floor(len(dataset) * args[0])
+        )
       elif isinstance(args[0], Union[np.ndarray, torch.Tensor]):
         self.indices = args[0]
     elif len(args) == 2:
@@ -127,6 +135,7 @@ class Slice(DatasetWrapper):
 # class Subset(DatasetWrapper):
 #   pass
 
+
 class IndexedView(DatasetWrapper):
   def __init__(self, dataset, indices):
     super(IndexedView, self).__init__(dataset)
@@ -180,13 +189,11 @@ class LookupCache(DatasetWrapper):
 class TransformDataset(DatasetWrapper):
   def __init__(self, dataset, transform):
     super().__init__(dataset)
-    self.transforms = transform if isinstance(transform, tuple) else (
-      transform,)
+    self.transforms = transform if isinstance(transform, tuple) else (transform,)
 
   def __getitem__(self, idx):
     return tuple(
-      t(x) if t else x
-      for (x, t) in zip_longest(self.dataset[idx], self.transforms)
+      t(x) if t else x for (x, t) in zip_longest(self.dataset[idx], self.transforms)
     )
 
   def __repr__(self):
@@ -194,7 +201,8 @@ class TransformDataset(DatasetWrapper):
       f'{self.__class__.__name__}('
       f'\nDataset: {repr(self.dataset)}'
       f'\nTransforms: {repr(self.transforms)}'
-      f'\n)')
+      f'\n)'
+    )
 
 
 # class Noisy(DatasetWrapper):
@@ -237,4 +245,3 @@ class VariableLength(DatasetWrapper):
 
   def __getitem__(self, idx):
     return self.dataset[idx % len(self.dataset)]
-
diff --git a/yann/distributed.py b/yann/distributed.py
index b04a839..c2ed6d7 100644
--- a/yann/distributed.py
+++ b/yann/distributed.py
@@ -1,20 +1,32 @@
-from torch import distributed as dist
-import torch
 import os
 from typing import NamedTuple, Union
 
+import torch
+from torch import distributed as dist
+
 
 class Dist:
   """
   torch.distributed wrapper that also supports non distributed mode
   """
-  def __init__(self, backend='nccl', init_method='env://', world_size=None, rank=None):
+
+  def __init__(
+    self,
+    backend='nccl',
+    init_method='env://',
+    world_size=None,
+    rank=None,
+  ):
     self.backend = backend
     self.init_method = init_method
 
     self.world_size = int(
-        world_size if world_size is not None else
-        os.environ.get('WORLD_SIZE', torch.cuda.device_count() if torch.cuda.is_available() else 1)
+      world_size
+      if world_size is not None
+      else os.environ.get(
+        'WORLD_SIZE',
+        torch.cuda.device_count() if torch.cuda.is_available() else 1,
+      ),
     )
     self.rank = rank if rank is not None else int(os.environ.get('RANK', 0))
     self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
@@ -27,7 +39,7 @@ class Dist:
       backend=self.backend,
       init_method=self.init_method,
       world_size=self.world_size,
-      rank=self.rank
+      rank=self.rank,
     )
 
     if self.backend == 'nccl':
@@ -44,11 +56,11 @@ class Dist:
 
   @property
   def is_enabled(self):
-    return "RANK" in os.environ and "WORLD_SIZE" in os.environ
+    return 'RANK' in os.environ and 'WORLD_SIZE' in os.environ
 
   @property
   def device(self):
-    return f"cuda:{self.local_rank}"
+    return f'cuda:{self.local_rank}'
 
   @property
   def is_main(self):
@@ -74,14 +86,11 @@ class Dist:
     )"""
 
 
-
-
 class DistPlacement(NamedTuple):
   rank: Union[int, None] = None
   local_rank: Union[int, None] = None
 
 
-
 def matches(placement: Union[int, DistPlacement, None], dist: Dist):
   if placement is None:
     return True
@@ -93,4 +102,4 @@ def matches(placement: Union[int, DistPlacement, None], dist: Dist):
       return rank == dist.rank
     if local_rank is not None:
       return local_rank == dist.local_rank
-    return True
\ No newline at end of file
+    return True
diff --git a/yann/evaluation/__init__.py b/yann/evaluation/__init__.py
index 9efbcce..bd3f750 100644
--- a/yann/evaluation/__init__.py
+++ b/yann/evaluation/__init__.py
@@ -1,5 +1,6 @@
 import torch
 
+
 def evaluate_metrics(targets=None, outputs=None, metrics=None):
   values = {}
   with torch.inference_mode():
@@ -46,4 +47,4 @@ class RegressionEvaluator(Evaluator):
 
 
 class RetrievalEvaluator(Evaluator):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/exceptions.py b/yann/exceptions.py
index 6db398d..45a2cab 100644
--- a/yann/exceptions.py
+++ b/yann/exceptions.py
@@ -1,5 +1,3 @@
-
-
 class YannException(Exception):
   pass
 
@@ -9,4 +7,4 @@ class ShapeInferenceError(YannException):
 
 
 class CheckFailure(YannException):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/export.py b/yann/export.py
index eecd2b1..878c1c6 100644
--- a/yann/export.py
+++ b/yann/export.py
@@ -8,48 +8,51 @@ import torch
 
 from .data import Classes
 from .data.io import (
-  load_pickle, load_json, save_json, save_pickle, tar_dir,
-  untar
+  load_json,
+  load_pickle,
+  save_json,
+  save_pickle,
+  tar_dir,
+  untar,
 )
 
 
 # TODO: add way to pass validation data to check model outputs when loaded again
 def export(
-    model=None,
-    preprocess=None,
-    postprocess=None,
-    predict=None,
-    classes=None,
-    trace=False,
-    state_dict=False,
-    path=None,
-    # validation=None,
-    meta=None,
-    tar=False,
-    **kwargs
+  model=None,
+  preprocess=None,
+  postprocess=None,
+  predict=None,
+  classes=None,
+  trace=False,
+  state_dict=False,
+  path=None,
+  # validation=None,
+  meta=None,
+  tar=False,
+  **kwargs,
 ):
   os.makedirs(path, exist_ok=True)
   if os.listdir(path):
     raise ValueError(
-      f'Failed to export because {path} already exists and is not empty')
+      f'Failed to export because {path} already exists and is not empty',
+    )
 
   path = Path(path)
 
   if model:
     if trace is not False and trace is not None:
       from torch import jit
+
       traced = jit.trace(model, trace)
       traced.save(os.path.join(path, 'model.traced.th'))
     else:
       if not state_dict:
-        torch.save(
-          model,
-          os.path.join(path, 'model.th')
-        )
+        torch.save(model, os.path.join(path, 'model.th'))
       else:
         torch.save(
           model.state_dict(),
-          os.path.join(path, 'model.state_dict.th')
+          os.path.join(path, 'model.state_dict.th'),
         )
   if preprocess:
     save_pickle(preprocess, path / 'preprocess.pkl')
@@ -63,7 +66,7 @@ def export(
   if classes:
     save_json(
       classes.state_dict() if isinstance(classes, Classes) else classes,
-      path / 'classes.json'
+      path / 'classes.json',
     )
 
   if kwargs:
@@ -72,16 +75,9 @@ def export(
   if predict:
     save_pickle(predict, path / 'predict.pkl')
 
-  try:
-    subprocess.call(
-      ['conda', 'env', 'export', '-f', os.path.join(path, 'env.yml')]
-    )
-  except:
-    pass # FIXME
-
-  subprocess.call(
-    ['pip', 'freeze'], stdout=open(os.path.join(path, 'requirements.txt'), 'w')
-  )
+  # Export environment information
+  from .utils import get_env_info
+  get_env_info(save_to_path=path)
 
   if tar:
     tar_dir(path)
@@ -116,15 +112,18 @@ def load(path, eval=True):
 
   p.model = None
   if (path / 'model.th').exists():
-    p.model = torch.load(str(path / 'model.th'))
+    # weights_only=False needed for loading complete models (not just state dicts)
+    p.model = torch.load(str(path / 'model.th'), weights_only=False)
   elif (path / 'model.traced.th').exists():
     from torch import jit
+
     p.model = jit.load(str(path / 'model.traced.th'))
 
   if p.model and eval:
     p.model.eval()
 
   with suppress(FileNotFoundError):
+    # State dicts can be loaded with weights_only=True (default)
     p.model_state_dict = torch.load(str(path / 'model.state_dict.th'))
 
   with suppress(FileNotFoundError):
diff --git a/yann/hooks.py b/yann/hooks.py
index e059055..96f0565 100644
--- a/yann/hooks.py
+++ b/yann/hooks.py
@@ -6,7 +6,6 @@ def zero_out_nans(module, grad_input, grad_output):
     grad[grad != grad] = 0  # technically shouldn't modify inputs
 
 
-
 class Hook:
   def __init__(self, module):
     self.module = module
@@ -27,25 +26,27 @@ class Hook:
 
 
 def shape_hook(name, depth=None, show=False):
-    depth = depth if depth is not None else name.count('.')
-    indent = '  ⬐' * depth if name else ''
+  depth = depth if depth is not None else name.count('.')
+  indent = '  ⬐' * depth if name else ''
+
+  def print_shapes(m, input, output):
+    print(
+      f'{f"{indent}{name}:": <20}',
+      f'{f"({m.__class__.__name__})": <15}',
+      ', '.join(f'{str(tuple(x.shape)): <15}' for x in input),
+      '=>',
+      f'{str(tuple(output.shape)): <15}',
+    )
 
-    def print_shapes(m, input, output):
-      print(
-        f'{f"{indent}{name}:": <20}',
-        f"{f'({m.__class__.__name__})': <15}",
-        ', '.join(f"{str(tuple(x.shape)): <15}" for x in input),
-        "=>",
-        f"{str(tuple(output.shape)): <15}")
+    if show is not False:
+      import yann
 
-      if show is not False:
-        import yann
-        if show is True:
-          yann.show(output)
-        else:
-          yann.show(output[show])
+      if show is True:
+        yann.show(output)
+      else:
+        yann.show(output[show])
 
-    return print_shapes
+  return print_shapes
 
 
 class ShapeHook(Hook):
@@ -53,12 +54,13 @@ class ShapeHook(Hook):
     self.show = show
     super().__init__(module)
 
-
   def register(self, module):
     self.handles = []
     for n, m in module.named_modules():
-      self.handles.append(m.register_forward_hook(shape_hook(n, show=self.show)))
+      self.handles.append(
+        m.register_forward_hook(shape_hook(n, show=self.show)),
+      )
 
   def remove(self):
     for h in self.handles:
-      h.remove()
\ No newline at end of file
+      h.remove()
diff --git a/yann/inference/core.py b/yann/inference/core.py
index dfaffb6..cf108bb 100644
--- a/yann/inference/core.py
+++ b/yann/inference/core.py
@@ -1,27 +1,29 @@
-from typing import Union, Callable, Iterable
 import time
+from typing import Callable, Iterable, Union
+
+import torch
+import torch.jit
 from torch import nn
-from torch.utils.data.dataset import Dataset
 from torch.utils.data.dataloader import DataLoader
-import torch.jit
-import torch
+from torch.utils.data.dataset import Dataset
+
 import yann
 
 from ..data.loaders import TransformLoader
 
 
 def inference_stream(
-    model: Union[nn.Module, Callable, str],
-    data: Union[Dataset, DataLoader, Iterable, str],
-    device=None,
-    transform=None,
-    batch_size=64,
-    parallel=False,
-    num_workers=1,
-    pin_memory=False,
-    shuffle=False,
-    progress=10,
-    eval=True,
+  model: Union[nn.Module, Callable, str],
+  data: Union[Dataset, DataLoader, Iterable, str],
+  device=None,
+  transform=None,
+  batch_size=64,
+  parallel=False,
+  num_workers=1,
+  pin_memory=False,
+  shuffle=False,
+  progress=10,
+  eval=True,
 ):
   device = device or yann.default.device
 
@@ -31,7 +33,8 @@ def inference_stream(
   if isinstance(model, nn.Module):
     if parallel:
       model = nn.DataParallel(model)
-    if eval: model.eval()
+    if eval:
+      model.eval()
     model.to(device)
 
   if isinstance(data, str):
@@ -44,7 +47,7 @@ def inference_stream(
       pin_memory=pin_memory,
       batch_size=batch_size,
       shuffle=shuffle,
-      num_workers=num_workers
+      num_workers=num_workers,
     )
 
   try:
@@ -62,7 +65,8 @@ def inference_stream(
       yield (inputs, *rest, outputs)
 
       if progress and idx % progress == 0:
-        print(f"[{idx} / {size}] ({time.time() - begin}, total: {time.time() - start})")
+        print(
+          f'[{idx} / {size}] ({time.time() - begin}, total: {time.time() - start})',
+        )
 
       begin = time.time()
-
diff --git a/yann/inference/predict.py b/yann/inference/predict.py
index d570b00..56a2e7f 100644
--- a/yann/inference/predict.py
+++ b/yann/inference/predict.py
@@ -9,7 +9,6 @@ class Predictor:
   def __init__(self, model):
     self.model: torch.nn.Module = model
 
-
   def __call__(self, *args, **kwargs):
     x = self.load(*args, **kwargs)
     x = self.transform(x)
diff --git a/yann/init.py b/yann/init.py
index 31d88b1..8825376 100644
--- a/yann/init.py
+++ b/yann/init.py
@@ -1,3 +1,5 @@
+import math
+
 from torch import nn
 from torch.nn import init
 
@@ -20,3 +22,8 @@ def kaiming(model: nn.Module):
 
 
 msr = kaiming
+
+
+def linear_zero_bias(linear: nn.Module, num_classes):
+  init.zeros_(linear.weight)
+  init.constant_(linear.bias, -math.log(num_classes))
diff --git a/yann/lr.py b/yann/lr.py
index aae70a1..b089f0f 100644
--- a/yann/lr.py
+++ b/yann/lr.py
@@ -1,12 +1,18 @@
 import os
 
 from . import set_param
-from .train import train, Trainer
-
-
-
-def lr_range_test(trainer: Trainer,  min_lr=.00001, max_lr=1, steps=None,
-                  step=None, log_freq=None, restore=True):
+from .train import Trainer, train
+
+
+def lr_range_test(
+  trainer: Trainer,
+  min_lr=0.00001,
+  max_lr=1,
+  steps=None,
+  step=None,
+  log_freq=None,
+  restore=True,
+):
   # assert max_lr > min_lr
   if restore:
     checkpoint_path = trainer.checkpoint(name='lr-range-test')
@@ -22,9 +28,13 @@ def lr_range_test(trainer: Trainer,  min_lr=.00001, max_lr=1, steps=None,
     cur_step = 0
 
     while cur_step < steps:
-      for x, y, pred, loss in train(trainer.model, trainer.loader,
-                                    trainer.optimizer, trainer.loss,
-                                    trainer.device):
+      for x, y, pred, loss in train(
+        trainer.model,
+        trainer.loader,
+        trainer.optimizer,
+        trainer.loss,
+        trainer.device,
+      ):
         yield (cur_lr, loss)
 
         if log_freq and cur_step % log_freq == 0:
@@ -34,16 +44,8 @@ def lr_range_test(trainer: Trainer,  min_lr=.00001, max_lr=1, steps=None,
         cur_step += 1
         set_param(trainer.optimizer, 'lr', cur_lr)
 
-
-
   finally:
     if restore:
       print('loading checkpoint')
       trainer.load_checkpoint(checkpoint_path)
       os.remove(checkpoint_path)
-
-
-
-
-
-
diff --git a/yann/metrics.py b/yann/metrics.py
index 98ccd71..beadbb7 100644
--- a/yann/metrics.py
+++ b/yann/metrics.py
@@ -1,16 +1,16 @@
-import torch
-import numpy as np
-from sklearn import metrics
-from functools import partial
 from collections import deque
+from functools import partial
 
-from .utils import to_numpy
+import numpy as np
+import torch
 
+from .utils import to_numpy
 
 
-def threshold_targets(metric, threshold=.5, **defaults):
+def threshold_targets(metric, threshold=0.5, **defaults):
   def m(preds, targets, **kwargs):
     return metric(preds, targets > threshold, **defaults, **kwargs)
+
   return m
 
 
@@ -36,12 +36,12 @@ def top_k_accuracy(targets, preds, k=1):
     _, targets = torch.max(targets, dim=1)
   scores, preds = preds.topk(k, 1, True, True)
   preds = preds.t()
-  correct = (preds == targets.view(1, -1).expand_as(preds))
+  correct = preds == targets.view(1, -1).expand_as(preds)
 
   return correct.sum().float() / len(targets)
 
 
-def mAP(targs, preds, pos_thresh=.5):
+def mAP(targs, preds, pos_thresh=0.5):
   preds = preds.to('cpu').numpy()
   targs = (targs.to('cpu') > pos_thresh).float().numpy()
   if np.size(preds) == 0:
@@ -76,10 +76,12 @@ def average_precision(output, target):
 
   return precision_at_i
 
+
 top_3_accuracy = partial(top_k_accuracy, k=3)
 top_5_accuracy = partial(top_k_accuracy, k=5)
 top_10_accuracy = partial(top_k_accuracy, k=10)
 
+
 def precision_at_k(targets, outputs, k=5):
   scores, top_preds = top_k(outputs, k=k)
   raise NotImplementedError()
@@ -109,14 +111,57 @@ def label_ranking_average_precision(targets, preds, target_threshold=0):
   targets, preds = to_numpy(targets), to_numpy(preds)
   if target_threshold is not None:
     targets = targets > target_threshold
-  return metrics.label_ranking_average_precision_score(targets, preds)
+
+  # Inlined NumPy implementation
+  n_samples, n_labels = targets.shape
+  scores = np.zeros(n_samples)
+
+  for i in range(n_samples):
+    true_indices = np.flatnonzero(targets[i])
+    if len(true_indices) == 0:
+      continue
+
+    pred_ranking = np.argsort(preds[i])[::-1]
+    ranks = np.empty_like(pred_ranking)
+    ranks[pred_ranking] = np.arange(1, n_labels + 1)  # 1-based rank
+
+    true_ranks = ranks[true_indices]
+    relevant_ranks_sorted = np.sort(true_ranks)
+
+    precisions = np.arange(1, len(true_indices) + 1) / relevant_ranks_sorted
+    scores[i] = np.mean(precisions)
+
+  return np.mean(scores)
 
 
 def coverage_error(targets, preds, target_threshold=0):
   targets, preds = to_numpy(targets), to_numpy(preds)
   if target_threshold is not None:
     targets = targets > target_threshold
-  return metrics.coverage_error(targets, preds)
+
+  # Inlined NumPy implementation
+  n_samples, n_labels = targets.shape
+  max_ranks = np.zeros(n_samples)
+
+  for i in range(n_samples):
+    true_indices = np.flatnonzero(targets[i])
+    if len(true_indices) == 0:
+      # Assign 0 coverage error if no true labels, consistent with sklearn
+      max_ranks[i] = 0
+      continue
+
+    pred_ranking = np.argsort(preds[i])[::-1]
+    ranks = np.empty_like(pred_ranking)
+    ranks[pred_ranking] = np.arange(1, n_labels + 1)  # 1-based rank
+
+    true_ranks = ranks[true_indices]
+    max_ranks[i] = np.max(true_ranks)
+
+  # sklearn definition: average over samples of (max_rank - 1) / n_labels
+  # Let's match that definition for consistency if users expect it
+  # The definition actually seems to be just mean(max_rank) / n_labels for scikit-learn.
+  # Let's stick to that.
+  return np.mean(max_ranks) / n_labels if n_labels > 0 else 0
 
 
 def average_precision_at_k(targets, preds, k=5):
@@ -127,31 +172,26 @@ def mean_average_precision_at_k(targets, preds, k=5):
   raise NotImplementedError()
 
 
-def evaluate_multiclass(
-    targets,
-    outputs,
-    preds=None,
-    classes=None
-):
+def evaluate_multiclass(targets, outputs, preds=None, classes=None):
   preds = preds or get_preds(outputs)
   targets, outputs, preds = (
     to_numpy(targets),
     to_numpy(outputs),
-    to_numpy(preds)
+    to_numpy(preds),
   )
   raise NotImplementedError()
 
 
 def evaluate_multilabel(
-    targets,
-    outputs,
-    preds=None,
-    classes=None,
+  targets,
+  outputs,
+  preds=None,
+  classes=None,
 ):
   targets, outputs, preds = (
     to_numpy(targets),
     to_numpy(outputs),
-    to_numpy(preds)
+    to_numpy(preds),
   )
   raise NotImplementedError()
 
@@ -205,16 +245,17 @@ class WindowMeter:
 
   @property
   def average(self):
-    if not self.values: return None
+    if not self.values:
+      return None
     return sum(self.values) / len(self.values)
 
 
-def exp_moving_avg(cur, prev=None, alpha=.05, steps=None):
+def exp_moving_avg(cur, prev=None, alpha=0.05, steps=None):
   """exponential moving average"""
   if prev is None:
     return cur
   avg = alpha * cur + prev * (1 - alpha)
-  return avg / (1 - alpha ** steps) if steps else avg
+  return avg / (1 - alpha**steps) if steps else avg
 
 
 def moving_average(data, window=10):
diff --git a/yann/models/classifier.py b/yann/models/classifier.py
index a824369..e44aa61 100644
--- a/yann/models/classifier.py
+++ b/yann/models/classifier.py
@@ -17,15 +17,14 @@ class Classifier(nn.Module):
     raise NotImplementedError()
 
 
-
 class LinearClassifier(Classifier):
   def __init__(
-      self,
-      in_features,
-      classes,
-      bias=True,
-      activation=None,
-      test_activation=None
+    self,
+    in_features,
+    classes,
+    bias=True,
+    activation=None,
+    test_activation=None,
   ):
     super(Classifier, self).__init__()
 
@@ -33,4 +32,4 @@ class LinearClassifier(Classifier):
     self.classes = classes
 
     self.activation = activation
-    self.test_activation = test_activation or activation
\ No newline at end of file
+    self.test_activation = test_activation or activation
diff --git a/yann/models/model.py b/yann/models/model.py
index c509423..6f6ff5b 100644
--- a/yann/models/model.py
+++ b/yann/models/model.py
@@ -7,10 +7,7 @@ class ModelMixin:
   def __init__(self, *args, **kwargs):
     # Need to store this for when we save the model,
     # so that we can restore it later without knowing them
-    self._init_args = {
-      'args': args,
-      'kwargs': kwargs
-    }
+    self._init_args = {'args': args, 'kwargs': kwargs}
     super().__init__(*args, **kwargs)
 
   def predict(self, inputs) -> Outputs:
diff --git a/yann/models/vision/base.py b/yann/models/vision/base.py
index 72aa1cc..b9d7811 100644
--- a/yann/models/vision/base.py
+++ b/yann/models/vision/base.py
@@ -1,8 +1,8 @@
+from typing import Callable
+
 from torch import nn
 
 from ...data import Classes
-from typing import Callable
-
 
 
 class CNN(nn.Module):
@@ -17,14 +17,12 @@ class RecognitionModel(nn.Module):
   input_shape = (None, None, None, None)
   classes: Classes
 
-
   backbone: Callable
   # "Global" pooling layer that converts features into an embedding
   pool_features: Callable
 
   classifier: Callable
 
-
   def get_features(self, inputs):
     raise NotImplementedError()
 
@@ -44,4 +42,4 @@ class RecognitionModel(nn.Module):
     pass
 
   def replace_classifier(self, linear=None, num_classes=None, init=None):
-    raise NotImplementedError()
\ No newline at end of file
+    raise NotImplementedError()
diff --git a/yann/models/vision/vgg.py b/yann/models/vision/vgg.py
index 44b51ff..ce711bb 100644
--- a/yann/models/vision/vgg.py
+++ b/yann/models/vision/vgg.py
@@ -1,5 +1,6 @@
 from torch import nn
-from yann.modules import Stack, Flatten
+
+from yann.modules import Flatten, Stack
 
 
 class VGG(nn.Module):
@@ -12,14 +13,7 @@ class VGG(nn.Module):
 
   in_channels = 3
 
-  channels = [
-    (64,),
-    (128,),
-    (256,),
-    (512,),
-    (512,)
-
-  ]
+  channels = [(64,), (128,), (256,), (512,), (512,)]
 
   def __init__(self, num_classes):
     super(VGG, self).__init__()
@@ -28,7 +22,7 @@ class VGG(nn.Module):
 
     self.activations_to_features = Stack(
       pool=self.Reduce(kernel_size=1, stride=1),
-      flatten=Flatten()
+      flatten=Flatten(),
     )
 
     self.project = nn.Linear(self.channels[-1][-1], self.num_classes)
@@ -49,8 +43,8 @@ class VGG(nn.Module):
           Stack(
             conv=self.Conv(prev_channels, c, kernel_size=3, padding=1),
             norm=self.Norm(c),
-            activation=self.Activation(inplace=True)
-          )
+            activation=self.Activation(inplace=True),
+          ),
         )
         prev_channels = c
       layers.append(self.Downsample(kernel_size=2, stride=2))
@@ -95,4 +89,3 @@ class VGG19(VGG):
     (512,) * 4,
     (512,) * 4,
   ]
-
diff --git a/yann/modules/__init__.py b/yann/modules/__init__.py
index ab95d01..6c323d4 100644
--- a/yann/modules/__init__.py
+++ b/yann/modules/__init__.py
@@ -4,15 +4,24 @@ from .conv import (
   ConvBlock,
   ConvBlock1x1,
   ConvBlock3x3,
+  DepthwiseConv2d,
+  DepthwiseSeparableConv2d,
+  EfficientChannelAttention,
   MixConv,
   SqueezeExcitation,
-  EfficientChannelAttention,
-  DepthwiseSeparableConv2d,
-  DepthwiseConv2d
 )
-from .shape import View, Flatten, Infer, Squeeze, Reshape, Permute, Transpose, FlattenSequences
-from .stack import Stack
 from .residual import Residual
+from .shape import (
+  Flatten,
+  FlattenSequences,
+  Infer,
+  Permute,
+  Reshape,
+  Squeeze,
+  Transpose,
+  View,
+)
+from .stack import Stack
 
 
 class Init:
@@ -25,7 +34,6 @@ class Init:
     return self.cls(*self.args, *args, **{**self.kwargs, **kwargs})
 
 
-
 class TrainEvalSwitch(nn.Module):
   def __init__(self, train=None, eval=None):
     super().__init__()
diff --git a/yann/modules/conv/__init__.py b/yann/modules/conv/__init__.py
index 176ba1f..47d8c1c 100644
--- a/yann/modules/conv/__init__.py
+++ b/yann/modules/conv/__init__.py
@@ -1,5 +1,5 @@
-from .mixconv import MixConv
+from .attention import EfficientChannelAttention
 from .conv import ConvBlock, ConvBlock1x1, ConvBlock3x3
 from .depthwise import DepthwiseConv2d, DepthwiseSeparableConv2d
+from .mixconv import MixConv
 from .squeeze_excitation import SqueezeExcitation
-from .attention import EfficientChannelAttention
\ No newline at end of file
diff --git a/yann/modules/conv/attention.py b/yann/modules/conv/attention.py
index ae0ce6e..6c8eeca 100644
--- a/yann/modules/conv/attention.py
+++ b/yann/modules/conv/attention.py
@@ -5,14 +5,16 @@ class EfficientChannelAttention(nn.Module):
   """
   https://github.com/BangguWu/ECANet
   """
+
   def __init__(self, kernel_size=3):
     super().__init__()
     self.pool = nn.AdaptiveAvgPool2d(1)
     self.conv = nn.Conv1d(
-      1, 1,
+      1,
+      1,
       kernel_size=kernel_size,
       padding=(kernel_size - 1) // 2,
-      bias=False
+      bias=False,
     )
     self.sigmoid = nn.Sigmoid()
 
@@ -23,4 +25,5 @@ class EfficientChannelAttention(nn.Module):
 
     return input * x.expand_as(input)
 
+
 ECA = EfficientChannelAttention
diff --git a/yann/modules/conv/conv.py b/yann/modules/conv/conv.py
index 4013c23..0b96a08 100644
--- a/yann/modules/conv/conv.py
+++ b/yann/modules/conv/conv.py
@@ -1,9 +1,9 @@
 from torch import nn
+
 from ..stack import Stack
 
 
 class ConvBlock(Stack):
-
   class default:
     conv = nn.Conv2d
     norm = nn.BatchNorm2d
@@ -24,7 +24,7 @@ class ConvBlock(Stack):
     norm=None,
     activation=True,
     order=('conv', 'norm', 'activation'),
-    **extra
+    **extra,
   ):
     if conv is True:
       if bias is None:
@@ -39,38 +39,43 @@ class ConvBlock(Stack):
         padding=padding,
         dilation=dilation,
         groups=groups,
-        padding_mode=padding_mode
+        padding_mode=padding_mode,
       )
     if norm is True:
       if conv is not None:
         norm = self.default.norm(
           num_features=conv.in_channels
-          if order.index('conv') > order.index('norm') else conv.out_channels
+          if order.index('conv') > order.index('norm')
+          else conv.out_channels,
         )
       else:
         norm = self.default.norm(in_channels)
 
     if activation is True:
-      activation = self.default.activation() if isinstance(
-        self.default.activation, type
-      ) else self.default.activation
+      activation = (
+        self.default.activation()
+        if isinstance(self.default.activation, type)
+        else self.default.activation
+      )
 
     d = dict(**extra, conv=conv, norm=norm, activation=activation)
     # NOTE: this depends on dict ordering
     super(ConvBlock, self).__init__(**{k: d[k] for k in order})
 
 
-
-
 class ConvBlock1x1(ConvBlock):
   def __init__(self, **kwargs):
     if kwargs.get('kernel_size', 1) != 1:
-      raise ValueError(f'kernel_size must be `1` if provided as an argument, got kernel_size={kwargs["kernel_size"]}')
+      raise ValueError(
+        f'kernel_size must be `1` if provided as an argument, got kernel_size={kwargs["kernel_size"]}',
+      )
     super(ConvBlock1x1, self).__init__(kernel_size=1, padding=0, **kwargs)
 
 
 class ConvBlock3x3(ConvBlock):
   def __init__(self, **kwargs):
     if kwargs.get('kernel_size', 3) != 3:
-      raise ValueError(f'kernel_size must be `3` if provided as an argument, got kernel_size={kwargs["kernel_size"]}')
-    super(ConvBlock3x3, self).__init__(kernel_size=3, padding=1, **kwargs)
\ No newline at end of file
+      raise ValueError(
+        f'kernel_size must be `3` if provided as an argument, got kernel_size={kwargs["kernel_size"]}',
+      )
+    super(ConvBlock3x3, self).__init__(kernel_size=3, padding=1, **kwargs)
diff --git a/yann/modules/conv/depthwise.py b/yann/modules/conv/depthwise.py
index 938704b..6d8dd24 100644
--- a/yann/modules/conv/depthwise.py
+++ b/yann/modules/conv/depthwise.py
@@ -2,9 +2,17 @@ from torch import nn
 
 
 class DepthwiseConv2d(nn.Conv2d):
-  def __init__(self, in_channels, out_channels, kernel_size, stride=1,
-                 padding=0, dilation=1,
-                 bias=True, padding_mode='zeros'):
+  def __init__(
+    self,
+    in_channels,
+    out_channels,
+    kernel_size,
+    stride=1,
+    padding=0,
+    dilation=1,
+    bias=True,
+    padding_mode='zeros',
+  ):
     if out_channels % in_channels:
       raise ValueError('out_channels must be a multiple of in_channels')
     super(DepthwiseConv2d, self).__init__(
@@ -16,27 +24,37 @@ class DepthwiseConv2d(nn.Conv2d):
       dilation=dilation,
       bias=bias,
       padding_mode=padding_mode,
-      groups=in_channels
+      groups=in_channels,
     )
 
 
 class DepthwiseSeparableConv2d(nn.Module):
-  def __init__(self, in_channels, out_channels, kernel_size, stride=1,
-                 padding=0, dilation=1,
-                 bias=True, padding_mode='zeros'):
+  def __init__(
+    self,
+    in_channels,
+    out_channels,
+    kernel_size,
+    stride=1,
+    padding=0,
+    dilation=1,
+    bias=True,
+    padding_mode='zeros',
+  ):
     super(DepthwiseSeparableConv2d, self).__init__()
 
-    self.depthwise = DepthwiseConv2d(in_channels=in_channels,
+    self.depthwise = DepthwiseConv2d(
+      in_channels=in_channels,
       out_channels=out_channels,
       kernel_size=kernel_size,
       stride=stride,
       padding=padding,
       dilation=dilation,
       bias=bias,
-      padding_mode=padding_mode)
+      padding_mode=padding_mode,
+    )
 
     self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)
 
   def forward(self, input):
     x = self.depthwise(input)
-    return self.pointwise(x)
\ No newline at end of file
+    return self.pointwise(x)
diff --git a/yann/modules/conv/mixconv.py b/yann/modules/conv/mixconv.py
index 18f781d..a2bf1c4 100644
--- a/yann/modules/conv/mixconv.py
+++ b/yann/modules/conv/mixconv.py
@@ -1,5 +1,5 @@
-from torch import nn
 import torch
+from torch import nn
 
 from .utils import get_same_padding
 
@@ -17,12 +17,13 @@ class MixConv(nn.Module):
   so kernel_size = 7 might lead to crashes on CPUs (seeing this on a Mac) 
   https://github.com/pytorch/pytorch/issues/20583
   """
+
   def __init__(
-      self,
-      in_channels,
-      out_channels,
-      kernel_size=None,
-      depthwise=True
+    self,
+    in_channels,
+    out_channels,
+    kernel_size=None,
+    depthwise=True,
   ):
     super(MixConv, self).__init__()
 
@@ -32,38 +33,51 @@ class MixConv(nn.Module):
       self.kernel_sizes = list(kernel_size)
     elif kernel_size is None:
       if not isinstance(in_channels, (list, tuple)):
-        raise ValueError('kernel_size must be provided if in_channels is not an iterable')
+        raise ValueError(
+          'kernel_size must be provided if in_channels is not an iterable',
+        )
       self.kernel_sizes = [3 + 2 * n for n in range(len(in_channels))]
 
     if isinstance(in_channels, (list, tuple)):
       self.input_channel_counts = in_channels
     else:
-      self.input_channel_counts = self.split_groups(in_channels, len(self.kernel_sizes))
+      self.input_channel_counts = self.split_groups(
+        in_channels,
+        len(self.kernel_sizes),
+      )
     if isinstance(out_channels, (list, tuple)):
       self.output_channel_counts = out_channels
     else:
-      self.output_channel_counts = self.split_groups(out_channels, len(self.kernel_sizes))
+      self.output_channel_counts = self.split_groups(
+        out_channels,
+        len(self.kernel_sizes),
+      )
 
     if len(self.input_channel_counts) != len(self.output_channel_counts):
       raise ValueError(
         f'in_channels and out_channels should have same number of groups,'
-        f' but got {len(self.input_channel_counts)} and {len(self.output_channel_counts)}'
+        f' but got {len(self.input_channel_counts)} and {len(self.output_channel_counts)}',
       )
 
-    self.convs = nn.ModuleList([
-      nn.Conv2d(
-        in_channels=ic,
-        out_channels=oc,
-        kernel_size=ks,
-        groups=min(ic, oc) if depthwise else 1,
-        padding=get_same_padding(ks)
-      )
-      for ic, oc, ks
-      in zip(self.input_channel_counts, self.output_channel_counts, self.kernel_sizes)
-    ])
+    self.convs = nn.ModuleList(
+      [
+        nn.Conv2d(
+          in_channels=ic,
+          out_channels=oc,
+          kernel_size=ks,
+          groups=min(ic, oc) if depthwise else 1,
+          padding=get_same_padding(ks),
+        )
+        for ic, oc, ks in zip(
+          self.input_channel_counts,
+          self.output_channel_counts,
+          self.kernel_sizes,
+        )
+      ],
+    )
 
   def __repr__(self):
-    return f"MixConv({self.input_channel_counts}, {self.output_channel_counts}, kernel_sizes={self.kernel_sizes}, convs={self.convs})"
+    return f'MixConv({self.input_channel_counts}, {self.output_channel_counts}, kernel_sizes={self.kernel_sizes}, convs={self.convs})'
 
   def split_groups(self, num_channels, num_groups):
     channel_counts = [num_channels // num_groups] * num_groups
@@ -76,5 +90,3 @@ class MixConv(nn.Module):
     parts = torch.split(input, self.input_channel_counts, 1)
     outputs = [conv(part) for conv, part in zip(self.convs, parts)]
     return torch.cat(outputs, 1)
-
-
diff --git a/yann/modules/conv/squeeze_excitation.py b/yann/modules/conv/squeeze_excitation.py
index 4af3db1..6e505c6 100644
--- a/yann/modules/conv/squeeze_excitation.py
+++ b/yann/modules/conv/squeeze_excitation.py
@@ -1,10 +1,10 @@
 from torch import nn
-from ..stack import Stack
+
 from ..residual import Residual
+from ..stack import Stack
 
 
 class SqueezeExcitation(Residual):
-
   def __init__(self, channels: int, reduction: int):
     """
     Args:
@@ -18,9 +18,9 @@ class SqueezeExcitation(Residual):
         nn.Conv2d(channels, inner_channels, kernel_size=1, padding=0),
         nn.ReLU(inplace=True),
         nn.Conv2d(inner_channels, channels, kernel_size=1, padding=0),
-        nn.Sigmoid()
-      )
+        nn.Sigmoid(),
+      ),
     )
 
 
-SE = SqueezeExcitation
\ No newline at end of file
+SE = SqueezeExcitation
diff --git a/yann/modules/conv/utils.py b/yann/modules/conv/utils.py
index 5743d24..69829c6 100644
--- a/yann/modules/conv/utils.py
+++ b/yann/modules/conv/utils.py
@@ -1,33 +1,47 @@
 import math
 
 
-def get_tf_same_padding(
-    tensor,
-    kernel_size,
-    stride=1,
-    dilation=1
-):
+def get_tf_same_padding(tensor, kernel_size, stride=1, dilation=1):
   if isinstance(kernel_size, int):
     return tuple(
-      *tf_same_pad(tensor.shape[2], kernel_size, stride=stride, dilation=dilation),
-      *tf_same_pad(tensor.shape[3], kernel_size, stride=stride, dilation=dilation)
+      *tf_same_pad(
+        tensor.shape[2],
+        kernel_size,
+        stride=stride,
+        dilation=dilation,
+      ),
+      *tf_same_pad(
+        tensor.shape[3],
+        kernel_size,
+        stride=stride,
+        dilation=dilation,
+      ),
     )
   else:
     return tuple(
-      *tf_same_pad(tensor.shape[2], kernel_size[0], stride=stride, dilation=dilation),
-      *tf_same_pad(tensor.shape[3], kernel_size[1], stride=stride, dilation=dilation)
+      *tf_same_pad(
+        tensor.shape[2],
+        kernel_size[0],
+        stride=stride,
+        dilation=dilation,
+      ),
+      *tf_same_pad(
+        tensor.shape[3],
+        kernel_size[1],
+        stride=stride,
+        dilation=dilation,
+      ),
     )
 
+
 def tf_same_pad(size, kernel_size, stride=1, dilation=1):
   pad = max(
     0,
-    (math.ceil(size / stride) - 1) * stride
-      + (kernel_size - 1) * dilation
-      + 1 - size
+    (math.ceil(size / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - size,
   )
   left_pad = pad // 2
   return left_pad, pad - left_pad
 
 
 def get_same_padding(kernel_size, stride=1, dilation=1):
-  return ((stride - 1) + dilation * (kernel_size - 1)) // 2
\ No newline at end of file
+  return ((stride - 1) + dilation * (kernel_size - 1)) // 2
diff --git a/yann/modules/heads/ml_decoder.py b/yann/modules/heads/ml_decoder.py
index fc34f82..3955c9e 100644
--- a/yann/modules/heads/ml_decoder.py
+++ b/yann/modules/heads/ml_decoder.py
@@ -1,23 +1,22 @@
 import torch
 from torch import nn
 
-from yann.modules import Stack, Residual
-
+from yann.modules import Residual, Stack
 
 
 class MLTransformerDecoderLayer(Stack):
   def __init__(
-      self,
-      embed_dim,
-      num_heads=8,
-      feedforward_dim=2048,
-      dropout=0.1,
-      layer_norm_eps=1e-5,
+    self,
+    embed_dim,
+    num_heads=8,
+    feedforward_dim=2048,
+    dropout=0.1,
+    layer_norm_eps=1e-5,
   ):
     super().__init__(
       norm1=Stack(
         Residual(nn.Dropout(dropout)),
-        nn.LayerNorm(embed_dim, eps=layer_norm_eps)
+        nn.LayerNorm(embed_dim, eps=layer_norm_eps),
       ),
       attention=nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout),
       norm2=Stack(
@@ -28,12 +27,12 @@ class MLTransformerDecoderLayer(Stack):
         nn.Linear(embed_dim, feedforward_dim),
         nn.ReLU(),
         nn.Dropout(dropout),
-        nn.Linear(feedforward_dim, embed_dim)
+        nn.Linear(feedforward_dim, embed_dim),
       ),
       norm3=Stack(
         Residual(nn.Dropout(dropout)),
         nn.LayerNorm(embed_dim, eps=layer_norm_eps),
-      )
+      ),
     )
 
   def forward(self, x, memory, **kwargs):
@@ -44,22 +43,21 @@ class MLTransformerDecoderLayer(Stack):
     return self.norm3(x)
 
 
-
 @torch.jit.script
 class GroupFC(object):
   def __init__(self, embed_len_decoder: int):
     self.embed_len_decoder = embed_len_decoder
 
   def __call__(
-      self,
-      h: torch.Tensor,
-      duplicate_pooling: torch.Tensor,
-      out_extrap: torch.Tensor
+    self,
+    h: torch.Tensor,
+    duplicate_pooling: torch.Tensor,
+    out_extrap: torch.Tensor,
   ):
-      for i in range(self.embed_len_decoder):
-          h_i = h[:, i, :]
-          w_i = duplicate_pooling[i, :, :]
-          out_extrap[:, i, :] = torch.matmul(h_i, w_i)
+    for i in range(self.embed_len_decoder):
+      h_i = h[:, i, :]
+      w_i = duplicate_pooling[i, :, :]
+      out_extrap[:, i, :] = torch.matmul(h_i, w_i)
 
 
 class GroupFullyConnectedPooling(nn.Module):
@@ -69,7 +67,7 @@ class GroupFullyConnectedPooling(nn.Module):
     self.num_classes = num_classes
     self.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)
     self.duplicate_pooling = torch.nn.Parameter(
-      torch.Tensor(embed_len_decoder, decoder_embedding, self.duplicate_factor)
+      torch.Tensor(embed_len_decoder, decoder_embedding, self.duplicate_factor),
     )
     self.bias = torch.nn.Parameter(torch.Tensor(num_classes))
     self.group_fc = GroupFC(embed_len_decoder)
@@ -81,26 +79,29 @@ class GroupFullyConnectedPooling(nn.Module):
     torch.nn.init.constant_(self.bias, 0)
 
   def forward(self, h):
-    out_extrap = torch.zeros(h.shape[0], h.shape[1], self.duplicate_factor,
+    out_extrap = torch.zeros(
+      h.shape[0],
+      h.shape[1],
+      self.duplicate_factor,
       device=h.device,
-      dtype=h.dtype
+      dtype=h.dtype,
     )
     self.group_fc(h, self.duplicate_pooling, out_extrap)
-    logits = out_extrap.flatten(1)[:, :self.num_classes]
+    logits = out_extrap.flatten(1)[:, : self.num_classes]
     logits += self.bias
     return logits
 
 
 class MLDecoder(nn.Module):
   def __init__(
-      self,
-      num_classes,
-      num_groups=None,
-      decoder_embed_dim=768,
-      initial_num_features=2048,
-      feedforward_dim=1024,
-      num_heads=8,
-      dropout=0.1
+    self,
+    num_classes,
+    num_groups=None,
+    decoder_embed_dim=768,
+    initial_num_features=2048,
+    feedforward_dim=1024,
+    num_heads=8,
+    dropout=0.1,
   ):
     super().__init__()
     num_groups = num_groups or num_classes
@@ -116,7 +117,7 @@ class MLDecoder(nn.Module):
 
     self.embed_input = Stack(
       nn.Linear(initial_num_features, decoder_embed_dim),
-      nn.ReLU(inplace=True)
+      nn.ReLU(inplace=True),
     )
 
     self.decoder = nn.TransformerDecoder(
@@ -124,15 +125,15 @@ class MLDecoder(nn.Module):
         embed_dim=decoder_embed_dim,
         feedforward_dim=feedforward_dim,
         num_heads=num_heads,
-        dropout=dropout
+        dropout=dropout,
       ),
-      num_layers=1
+      num_layers=1,
     )
 
     self.group_pooling = GroupFullyConnectedPooling(
       num_classes=num_classes,
       embed_len_decoder=num_groups,
-      decoder_embedding=decoder_embed_dim
+      decoder_embedding=decoder_embed_dim,
     )
 
   def forward(self, x: torch.Tensor):
@@ -142,8 +143,10 @@ class MLDecoder(nn.Module):
     x = self.embed_input(x)
     # no allocation of memory with expand
     target = self.query_embed.weight.unsqueeze(1).expand(-1, x.shape[0], -1)
-    h = self.decoder(target, x.transpose(0, 1))  # [embed_len_decoder, batch, 768]
+    h = self.decoder(
+      target,
+      x.transpose(0, 1),
+    )  # [embed_len_decoder, batch, 768]
     h = h.transpose(0, 1)
 
     return self.group_pooling(h)
-
diff --git a/yann/modules/loss/__init__.py b/yann/modules/loss/__init__.py
index 0fae1f5..a443c90 100644
--- a/yann/modules/loss/__init__.py
+++ b/yann/modules/loss/__init__.py
@@ -1,7 +1,7 @@
 import torch
 import torch.nn.functional as F
-from torch.nn.modules.loss import _Loss, _WeightedLoss
 from torch import nn
+from torch.nn.modules.loss import _Loss, _WeightedLoss
 
 from yann.data.classes import smooth as label_smoothing
 
@@ -21,13 +21,14 @@ def _reduce(x, reduce=True, reduction=None):
 
 
 def soft_target_cross_entropy(
-    inputs,
-    targets,
-    smooth=None,
-    reduce=True,
-    dim=1,
-    reduction='mean'):
-  """"like cross_entropy but using soft targets"""
+  inputs,
+  targets,
+  smooth=None,
+  reduce=True,
+  dim=1,
+  reduction='mean',
+):
+  """ "like cross_entropy but using soft targets"""
   if smooth:
     targets = label_smoothing(targets, smooth)
 
@@ -35,7 +36,6 @@ def soft_target_cross_entropy(
   return _reduce(vals, reduce=reduce, reduction=reduction)
 
 
-
 class SoftTargetCrossEntropyLoss(_Loss):
   def __init__(self, smooth=None, reduce=True, dim=1, reduction='mean'):
     super().__init__(reduce=reduce, reduction=reduction)
@@ -51,11 +51,19 @@ class SoftTargetCrossEntropyLoss(_Loss):
       smooth=self.smooth,
       reduce=self.reduce,
       dim=self.dim,
-      reduction=self.reduction)
-
+      reduction=self.reduction,
+    )
 
 
-def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, reduce=True, reduction='mean'):
+def binary_focal_loss(
+  logits,
+  targets,
+  gamma=2,
+  alpha=None,
+  pos_weight=None,
+  reduce=True,
+  reduction='mean',
+):
   """
   Binary focal loss (with sigmoids)
 
@@ -93,7 +101,12 @@ def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, red
   # TODO try this numerically stable version https://github.com/richardaecn/class-balanced-loss/issues/1
 
   probs = torch.sigmoid(logits)
-  bce = F.binary_cross_entropy_with_logits(logits, targets, pos_weight=pos_weight, reduction='none')
+  bce = F.binary_cross_entropy_with_logits(
+    logits,
+    targets,
+    pos_weight=pos_weight,
+    reduction='none',
+  )
 
   pt = targets * probs + (1 - targets) * (1 - probs)
   modulate = 1 if gamma is None else (1 - pt) ** gamma
@@ -109,7 +122,14 @@ def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, red
 
 
 class BinaryFocalLoss(_Loss):
-  def __init__(self, gamma=2, alpha=None, pos_weight=None, reduce=True, reduction='mean'):
+  def __init__(
+    self,
+    gamma=2,
+    alpha=None,
+    pos_weight=None,
+    reduce=True,
+    reduction='mean',
+  ):
     super(BinaryFocalLoss, self).__init__()
 
     self.gamma = gamma
@@ -125,9 +145,10 @@ class BinaryFocalLoss(_Loss):
       gamma=self.gamma,
       alpha=self.alpha,
       pos_weight=self.pos_weight,
-      reduction=self.reduction
+      reduction=self.reduction,
     )
 
+
 class ClassWeighted(_Loss):
   def __init__(self, loss, weights=None, reduce=True, reduction='mean'):
     super(ClassWeighted, self).__init__(reduce=reduce, reduction=reduction)
@@ -151,15 +172,15 @@ def triplet_loss():
   pass
 
 
-
-
 def tempered_log(x, temperature=1):
-  if temperature == 1: return torch.log(x)
+  if temperature == 1:
+    return torch.log(x)
   return (x ** (1 - temperature) - 1) / (1 - temperature)
 
 
 def tempered_exp(x, temperature=1):
-  if temperature == 1: return torch.exp(x)
+  if temperature == 1:
+    return torch.exp(x)
   return torch.relu(1 + (1 - temperature) * x) ** (1 / (1 - temperature))
 
 
@@ -175,7 +196,6 @@ def bi_tempered_binary_logistic_loss():
   pass
 
 
-
 class WeightedLoss(_WeightedLoss):
   def __init__(self, loss, weight=1, **kwargs):
     super(WeightedLoss, self).__init__(weight=weight, **kwargs)
@@ -206,7 +226,15 @@ MultiTaskLoss = CombinedLoss
 
 
 class KeepK(_Loss):
-  def __init__(self, loss, top=None, bottom=None, reduce=True, reduction='mean', **kwargs):
+  def __init__(
+    self,
+    loss,
+    top=None,
+    bottom=None,
+    reduce=True,
+    reduction='mean',
+    **kwargs,
+  ):
     super().__init__(reduce=reduce, reduction=reduction, **kwargs)
     self.loss = loss
     if hasattr(self.loss, 'reduction'):
@@ -236,9 +264,16 @@ class KeepK(_Loss):
     return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
 
 
-
 class KeepRange(_Loss):
-  def __init__(self, loss, min=None, max=None, reduce=True, reduction='mean', **kwargs):
+  def __init__(
+    self,
+    loss,
+    min=None,
+    max=None,
+    reduce=True,
+    reduction='mean',
+    **kwargs,
+  ):
     super().__init__(reduce=reduce, reduction=reduction, **kwargs)
     self.loss = loss
     if hasattr(self.loss, 'reduction'):
@@ -264,4 +299,4 @@ class KeepRange(_Loss):
     if mask is not None:
       losses = losses[mask]
 
-    return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
\ No newline at end of file
+    return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
diff --git a/yann/modules/loss/asymmetric.py b/yann/modules/loss/asymmetric.py
index c372703..182a88c 100644
--- a/yann/modules/loss/asymmetric.py
+++ b/yann/modules/loss/asymmetric.py
@@ -1,8 +1,8 @@
-from torch import nn
 import torch
-from yann.typedefs import Logits, MultiLabelOneHot
+from torch import nn
 
 from yann.modules.loss import _reduce
+from yann.typedefs import Logits, MultiLabelOneHot
 
 
 class AsymmetricLoss(nn.Module):
@@ -25,7 +25,14 @@ class AsymmetricLoss(nn.Module):
     https://github.com/Alibaba-MIIL/ASL
   """
 
-  def __init__(self, neg_decay=4, pos_decay=1, prob_shift=0.05, eps=1e-8, reduction='mean'):
+  def __init__(
+    self,
+    neg_decay=4,
+    pos_decay=1,
+    prob_shift=0.05,
+    eps=1e-8,
+    reduction='mean',
+  ):
     super(AsymmetricLoss, self).__init__()
     self.pos_decay = pos_decay
     self.neg_decay = neg_decay
@@ -36,7 +43,6 @@ class AsymmetricLoss(nn.Module):
     self.reduction = reduction
     self.calculate_focal_loss_gradients = True
 
-
   def forward(self, inputs: Logits, targets: MultiLabelOneHot, reduction=None):
     pos_probs = torch.sigmoid(inputs)
     neg_probs = 1 - pos_probs
@@ -47,7 +53,7 @@ class AsymmetricLoss(nn.Module):
       # paper claims this also helps handling mislabeled negative examples
       neg_probs = (neg_probs + self.prob_shift).clamp(max=1)
 
-    neg_targets = (1 - targets)
+    neg_targets = 1 - targets
 
     pos_losses = targets * torch.log(pos_probs.clamp(min=self.eps))
     neg_losses = neg_targets * torch.log(neg_probs.clamp(min=self.eps))
@@ -66,7 +72,7 @@ class AsymmetricLoss(nn.Module):
 class AsymmetricLossOptimized(AsymmetricLoss):
   def forward(self, inputs: Logits, targets: MultiLabelOneHot, reduction=None):
     self.targets = targets
-    self.neg_targets = (1 - self.targets)
+    self.neg_targets = 1 - self.targets
 
     self.pos_probs = torch.sigmoid(inputs)
     self.neg_probs = 1 - self.pos_probs
@@ -83,8 +89,8 @@ class AsymmetricLossOptimized(AsymmetricLoss):
         self.neg_probs.mul_(self.neg_targets)
         weights = torch.pow(
           1 - self.pos_probs - self.neg_probs,
-          self.pos_decay * self.targets + self.neg_decay * self.neg_targets
+          self.pos_decay * self.targets + self.neg_decay * self.neg_targets,
         )
       self.losses *= weights
 
-    return _reduce(-self.losses, reduction=reduction or self.reduction)
\ No newline at end of file
+    return _reduce(-self.losses, reduction=reduction or self.reduction)
diff --git a/yann/modules/loss/multilabel.py b/yann/modules/loss/multilabel.py
index ebdf2a6..1d7da85 100644
--- a/yann/modules/loss/multilabel.py
+++ b/yann/modules/loss/multilabel.py
@@ -1,18 +1,19 @@
+import math
+
 import torch
 import torch.nn.functional as F
-import math
 
 from yann.modules.loss import _reduce
 
 
 class LargeLossNegativeRejection(torch.nn.Module):
   def __init__(
-      self,
-      loss=F.binary_cross_entropy_with_logits,
-      threshold=None,
-      percent=None,
-      reduction: str = 'mean',
-      pos_thresh=.5
+    self,
+    loss=F.binary_cross_entropy_with_logits,
+    threshold=None,
+    percent=None,
+    reduction: str = 'mean',
+    pos_thresh=0.5,
   ):
     """
     Args:
@@ -27,6 +28,7 @@ class LargeLossNegativeRejection(torch.nn.Module):
     # need to disable reduction on wrapped loss
     self._loss_args = {}
     import inspect
+
     if hasattr(self.loss, 'reduction'):
       self.loss.reduction = 'none'
     elif 'reduction' in inspect.getfullargspec(self.loss).args:
@@ -38,7 +40,13 @@ class LargeLossNegativeRejection(torch.nn.Module):
 
     self.pos_thresh = pos_thresh
 
-  def forward(self, preds: torch.Tensor, targets: torch.Tensor, percent=None, threshold=None):
+  def forward(
+    self,
+    preds: torch.Tensor,
+    targets: torch.Tensor,
+    percent=None,
+    threshold=None,
+  ):
     percent = percent or self.percent
     threshold = threshold or self.threshold
 
@@ -49,7 +57,10 @@ class LargeLossNegativeRejection(torch.nn.Module):
     if percent is not None and percent > 0:
       unobserved_count = torch.count_nonzero(unobserved_losses)
       k = torch.ceil(unobserved_count * percent)
-      largest_unobserved_losses, _ = torch.topk(unobserved_losses.flatten(), int(k))
+      largest_unobserved_losses, _ = torch.topk(
+        unobserved_losses.flatten(),
+        int(k),
+      )
       keep_mask = (unobserved_losses < largest_unobserved_losses[-1]).float()
       losses = losses * keep_mask
 
diff --git a/yann/modules/norm.py b/yann/modules/norm.py
index a4e0e81..02736f2 100644
--- a/yann/modules/norm.py
+++ b/yann/modules/norm.py
@@ -2,7 +2,6 @@ import torch
 
 
 def normalize(batch, p=2, eps=1e-8):
-  return (
-      batch
-      / (torch.norm(batch, p=p, dim=1, keepdim=True) + eps).expand_as(batch)
+  return batch / (torch.norm(batch, p=p, dim=1, keepdim=True) + eps).expand_as(
+    batch,
   )
diff --git a/yann/modules/pool.py b/yann/modules/pool.py
index e15d4dc..1c1ef03 100644
--- a/yann/modules/pool.py
+++ b/yann/modules/pool.py
@@ -1,11 +1,13 @@
-from torch.nn import functional as F
 import torch
 from torch import nn
+from torch.nn import functional as F
+
 
 def mac(batch):
   """MAC Pooling"""
   return F.adaptive_max_pool2d(batch, (1, 1))
 
+
 def spoc(batch):
   """SPoC Pooling"""
   return F.adaptive_avg_pool2d(batch, (1, 1))
@@ -18,13 +20,10 @@ def generalized_mean(batch, p=3, eps=1e-8):
 
   larger p leads to more localized (max) features
   """
-  return F.adaptive_avg_pool2d(
-    batch.clamp(min=eps) ** p,
-    (1, 1)
-  ) ** (1 / p)
+  return F.adaptive_avg_pool2d(batch.clamp(min=eps) ** p, (1, 1)) ** (1 / p)
 
-gem = generalized_mean
 
+gem = generalized_mean
 
 
 class GeM(nn.Module):
@@ -34,4 +33,4 @@ class GeM(nn.Module):
     self.eps = eps
 
   def forward(self, x):
-    return gem(x, p=self.p, eps=self.eps)
\ No newline at end of file
+    return gem(x, p=self.p, eps=self.eps)
diff --git a/yann/modules/residual.py b/yann/modules/residual.py
index 182ef1d..97f9d46 100644
--- a/yann/modules/residual.py
+++ b/yann/modules/residual.py
@@ -1,6 +1,5 @@
 from torch import nn
 
-# TODO: handle downsampling logic
 
 def residual(input, block, identity=None):
   p = block(input)
@@ -10,9 +9,11 @@ def residual(input, block, identity=None):
 
 
 class Residual(nn.Module):
-  def __init__(self, block, identity=None, activation=None):
+  def __init__(self, *block, identity=None, activation=None):
     super().__init__()
-    self.block = block
+    from . import Stack
+
+    self.block = block[0] if len(block) == 1 else Stack(*block)
     self.identity = identity
     self.activation = activation
 
@@ -25,4 +26,4 @@ class Residual(nn.Module):
     if self.activation:
       return self.activation(input)
     else:
-      return input
\ No newline at end of file
+      return input
diff --git a/yann/modules/selective_backprop.py b/yann/modules/selective_backprop.py
index 1b744df..c2e3f98 100644
--- a/yann/modules/selective_backprop.py
+++ b/yann/modules/selective_backprop.py
@@ -1,7 +1,6 @@
 import torch
 
 
-
 class SelectiveBackprop(torch.nn.Module):
   def __init__(self, model, loss, k=None, percent=None, min=None):
     super(SelectiveBackprop, self).__init__()
@@ -25,13 +24,9 @@ class SelectiveBackprop(torch.nn.Module):
           indices = losses >= min
         # elif self.percent:
 
-
       inputs, targets = inputs[indices], targets[indices]
       outputs = self.model(inputs)
       loss = self.loss(outputs, targets)
       return loss
     else:
       return self.model(inputs)
-
-
-
diff --git a/yann/modules/shape.py b/yann/modules/shape.py
index 8138f38..77a51bb 100644
--- a/yann/modules/shape.py
+++ b/yann/modules/shape.py
@@ -1,6 +1,8 @@
 from torch.nn import Module
+
 from ..exceptions import ShapeInferenceError
 
+
 class Reshape(Module):
   method = None
 
@@ -12,9 +14,7 @@ class Reshape(Module):
     return getattr(input, self.method)(*self.dims)
 
   def state_dict(self, destination=None, prefix='', keep_vars=False):
-    return {
-      'dims': self.dims
-    }
+    return {'dims': self.dims}
 
   def load_state_dict(self, state_dict, strict=True):
     self.dims = state_dict['dims']
@@ -72,12 +72,18 @@ class Infer(Module):
   def forward(self, x):
     if self.module is None:
       try:
-        self.module = self.cls(x.shape[self.shape_dim], *self.args, **self.kwargs)
+        self.module = self.cls(
+          x.shape[self.shape_dim],
+          *self.args,
+          **self.kwargs,
+        )
       except IndexError as e:
-        raise ShapeInferenceError(f"Improper shape dim ({self.shape_dim}) selected for {self.cls} with input of shape {x.shape}")
+        raise ShapeInferenceError(
+          f'Improper shape dim ({self.shape_dim}) selected for {self.cls} with input of shape {x.shape}',
+        )
     return self.module(x)
 
   @classmethod
   def shed(cls, module):
     # TODO: modify the model to drop the Infer nodes and replace them with the initialized module
-    raise NotImplementedError()
\ No newline at end of file
+    raise NotImplementedError()
diff --git a/yann/modules/stack.py b/yann/modules/stack.py
index b4d10b4..e0d2ae7 100644
--- a/yann/modules/stack.py
+++ b/yann/modules/stack.py
@@ -58,7 +58,7 @@ class Stack(nn.Module):
         stop = layers.index(x.stop)
 
       return self.__class__(
-        **dict(list(self.named_children())[start:stop:x.step])
+        **dict(list(self.named_children())[start : stop : x.step]),
       )
     elif isclass(x):
       return [m for m in self.children() if isinstance(m, x)]
@@ -77,4 +77,4 @@ class Stack(nn.Module):
   def upto(self, module):
     layers = list(self.children())
     stop = layers.index(module)
-    return self[:stop + 1]
\ No newline at end of file
+    return self[: stop + 1]
diff --git a/yann/optim/__init__.py b/yann/optim/__init__.py
index 9c3a500..c4e6e52 100644
--- a/yann/optim/__init__.py
+++ b/yann/optim/__init__.py
@@ -1 +1 @@
-from .clip import GradClipper, clip_grad_, clip_grad_adaptive_
\ No newline at end of file
+from .clip import GradClipper, clip_grad_, clip_grad_adaptive_
diff --git a/yann/optim/clip.py b/yann/optim/clip.py
index 7845653..74c3bb0 100644
--- a/yann/optim/clip.py
+++ b/yann/optim/clip.py
@@ -1,6 +1,7 @@
 """
 Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/agc.py
 """
+
 import torch
 from torch.nn.utils import clip_grad_norm_, clip_grad_value_
 
@@ -12,7 +13,7 @@ def unitwise_norm(x: torch.Tensor, p=2.0):
     return x.norm(p, dim=tuple(range(1, x.ndim)), keepdim=True)
 
 
-def clip_grad_adaptive_(parameters, value=.01, norm_type=2.0, eps=1e-3):
+def clip_grad_adaptive_(parameters, value=0.01, norm_type=2.0, eps=1e-3):
   """
   Adaptive grad clipping
   """
@@ -20,14 +21,10 @@ def clip_grad_adaptive_(parameters, value=.01, norm_type=2.0, eps=1e-3):
     parameters = [parameters]
   parameters = [p for p in parameters if p.grad is not None]
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   for param in parameters:
     weights, grads = param.detach(), param.grad.detach()
-    max_norm = (
-      unitwise_norm(weights, p=norm_type)
-        .clamp_(min=eps)
-        .mul_(value)
-    )
+    max_norm = unitwise_norm(weights, p=norm_type).clamp_(min=eps).mul_(value)
     grad_norm = unitwise_norm(grads, p=norm_type)
     clipped_grad = grads * (max_norm / grad_norm.clamp(min=1e-6))
     new_grads = torch.where(grad_norm < max_norm, grads, clipped_grad)
@@ -39,13 +36,13 @@ def clip_grad_(parameters, value, norm_type=2.0, mode='adaptive'):
     return clip_grad_adaptive_(
       parameters=parameters,
       value=value,
-      norm_type=norm_type
+      norm_type=norm_type,
     )
   elif mode == 'norm':
     return clip_grad_norm_(
       parameters=parameters,
       max_norm=value,
-      norm_type=norm_type
+      norm_type=norm_type,
     )
   elif mode == 'value':
     return clip_grad_value_(
@@ -53,7 +50,9 @@ def clip_grad_(parameters, value, norm_type=2.0, mode='adaptive'):
       clip_value=value,
     )
   else:
-    raise ValueError(f'Unsupported mode={mode}, must be adaptive, norm or value')
+    raise ValueError(
+      f'Unsupported mode={mode}, must be adaptive, norm or value',
+    )
 
 
 class GradClipper:
@@ -74,11 +73,11 @@ class GradClipper:
       parameters=parameters,
       value=self.value,
       norm_type=self.norm_type,
-      mode=self.mode
+      mode=self.mode,
     )
 
   def state_dict(self):
     return self.__dict__
 
   def load_state_dict(self, dict):
-    self.__dict__.update(dict)
\ No newline at end of file
+    self.__dict__.update(dict)
diff --git a/yann/params.py b/yann/params.py
index d32f76d..a80380d 100644
--- a/yann/params.py
+++ b/yann/params.py
@@ -30,21 +30,23 @@
 
 """
 
+import logging
+import typing
 from abc import ABCMeta
 from collections import OrderedDict
 from copy import deepcopy
 from functools import wraps
 from typing import Dict
-import typing
-import logging
-from .utils import get_arg_parser
 
+from .utils import get_arg_parser
 
 log = logging.getLogger(__name__)
 
+
 class ValidationError(ValueError):
   pass
 
+
 class Field:
   def __init__(
     self,
@@ -54,7 +56,7 @@ class Field:
     type=None,
     required=False,
     default=None,
-    choices=None
+    choices=None,
   ):
     self.name = name
     self.help = help
@@ -67,18 +69,20 @@ class Field:
     try:
       if self.type and not isinstance(val, self.type):
         raise ValidationError(
-          f'Failed to validate {self.name}, the type ({type(val)} does is not a subclass of {self.type}')
+          f'Failed to validate {self.name}, the type ({type(val)} does is not a subclass of {self.type}',
+        )
     except TypeError as e:
-      log.debug(f'skipping type validation due to unresolved forwardref for {self.name} and expected type {self.type}')
+      log.debug(
+        f'skipping type validation due to unresolved forwardref for {self.name} and expected type {self.type}',
+      )
     if self.choices:
       assert val in self.choices
 
-
   def __repr__(self):
-    return f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    return f'{self.__class__.__name__}(type={self.type}, default={self.default})'
 
   def __str__(self):
-    return f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    return f'{self.__class__.__name__}(type={self.type}, default={self.default})'
 
 
 class Choice(Field):
@@ -108,7 +112,7 @@ class HyperParamsBase:
         setattr(self, k, v)
       else:
         raise ValueError(
-          f'Unknown parameter: {k}, should be one of {", ".join(self.__fields__)}'
+          f'Unknown parameter: {k}, should be one of {", ".join(self.__fields__)}',
         )
 
   def validate(self):
@@ -116,20 +120,19 @@ class HyperParamsBase:
       try:
         f.validate(getattr(self, k))
       except Exception as e:
-        raise Exception(f"{k} failed validation. {e}")
+        raise Exception(f'{k} failed validation. {e}')
 
   @classmethod
   def from_command(cls, cmd=None, validate=False, **kwargs):
     parser = get_arg_parser(cls.__fields__, **kwargs)
-    parsed = parser.parse_args(
-      cmd.split() if isinstance(cmd, str) else cmd
-    )
+    parsed = parser.parse_args(cmd.split() if isinstance(cmd, str) else cmd)
     params = cls(**vars(parsed))
 
     if validate:
       params.validate()
 
     return params
+
   #
   # @classmethod
   # def from_env(cls, prefix=''):
@@ -154,6 +157,7 @@ class HyperParamsBase:
 
   def save(self, path):
     import yann
+
     yann.save(dict(self), path)
 
   def on_change(self, callback):
@@ -180,24 +184,25 @@ class HyperParamsBase:
     return len(self.__fields__)
 
   def __eq__(self, other):
-    return len(self) == len(other) and self.keys() == other.keys(
-    ) and all(self[k] == other[k] for k in self.keys())
+    return (
+      len(self) == len(other)
+      and self.keys() == other.keys()
+      and all(self[k] == other[k] for k in self.keys())
+    )
 
   def fork(self, **args):
     return HyperParams(**{**self.items(), **args})
 
   def __repr__(self):
     return (
-      f'{self.__class__.__name__}('
-      f"{', '.join(f'{k}={v}' for k, v in self.items())}"
-      ')'
+      f'{self.__class__.__name__}({", ".join(f"{k}={v}" for k, v in self.items())})'
     )
 
   def __str__(self):
     return (
-      f'{self.__class__.__name__}(\n' +
-      ',\n'.join('  {}={}'.format(k, v) for k, v in self.items()) +
-      '\n)'
+      f'{self.__class__.__name__}(\n'
+      + ',\n'.join('  {}={}'.format(k, v) for k, v in self.items())
+      + '\n)'
     )
 
   def __contains__(self, key):
@@ -215,6 +220,21 @@ class HyperParamsBase:
   def items(self):
     return ((k, getattr(self, k)) for k in self.keys())
 
+  def update(self, other=None, **kwargs):
+    """Update parameters from dict or kwargs."""
+    if other is not None:
+      if hasattr(other, 'items'):
+        for k, v in other.items():
+          if k in self.__fields__:
+            setattr(self, k, v)
+      else:
+        for k in other:
+          if k in self.__fields__:
+            setattr(self, k, other[k])
+    for k, v in kwargs.items():
+      if k in self.__fields__:
+        setattr(self, k, v)
+
   def inject(self, scope=None, uppercase=True):
     scope = globals() if scope is None else scope
     for k, v in self.items():
@@ -226,7 +246,7 @@ class HyperParamsBase:
     scope=None,
     types=(int, str, float, bool),
     upper_only=True,
-    lowercase=True
+    lowercase=True,
   ):
     scope = globals() if scope is None else scope
 
@@ -242,7 +262,6 @@ class HyperParamsBase:
     return cls(**d)
 
 
-
 class MetaHyperParams(ABCMeta):
   def __new__(metaclass, class_name, bases, namespace):
     fields = OrderedDict()
@@ -251,17 +270,14 @@ class MetaHyperParams(ABCMeta):
       if issubclass(base, HyperParamsBase) and base != HyperParamsBase:
         fields.update(
           # deepcopy(
-            base.__fields__
+          base.__fields__,
           # )
         )
 
     # existing_attributes = set(dir(HyperParamsBase)) | set(fields)
 
     new_attributes = {
-      k: v
-      for (k, v) in namespace.items()
-      if not k.startswith('_') and
-      not callable(v)
+      k: v for (k, v) in namespace.items() if not k.startswith('_') and not callable(v)
     }
 
     for name, annotation in namespace.get('__annotations__', {}).items():
@@ -287,7 +303,7 @@ class MetaHyperParams(ABCMeta):
       {
         '__fields__': fields,
         **namespace,
-      }
+      },
     )
 
 
@@ -299,28 +315,26 @@ class HyperParams(HyperParamsBase, metaclass=MetaHyperParams):
     self.__dict__.update(state)
 
   def __reduce__(self):
-    return (self.__class__, )
-
+    return (self.__class__,)
 
 
 def to_argparse(params: HyperParams, **kwargs):
   return get_arg_parser(params.__fields__, **kwargs)
 
 
-
 def bind(params, mapping=None):
   def decorator(function):
-
     import inspect
+
     sig = inspect.signature(function)
 
     _mapping = mapping or {}
     for p in sig.parameters:
       if p not in _mapping and p in params:
         _mapping[p] = p
+
     @wraps(function)
     def bound(*args, **kwargs):
-
       for k, p in _mapping.items():
         if k in kwargs:
           params[p] = kwargs[k]
@@ -334,24 +348,25 @@ def bind(params, mapping=None):
   return decorator
 
 
-
 def from_signature(function, params: HyperParams = None):
   import inspect
+
   sig = inspect.signature(function)
 
   params = params or HyperParams()
   for k, p in sig.parameters.items():
-
     default = p.default if p.default is not p.empty else None
 
     params.__fields__[k] = Field(
       name=p.name,
       default=default,
-      type=p.annotation if p.annotation is not p.empty else (type(p.default) if p.default is not p.empty else None)
+      type=p.annotation
+      if p.annotation is not p.empty
+      else (type(p.default) if p.default is not p.empty else None),
     )
     params[k] = default
   return params
 
 
 def register():
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/perf.py b/yann/perf.py
index 1e3a62d..e597c9a 100644
--- a/yann/perf.py
+++ b/yann/perf.py
@@ -7,20 +7,22 @@ def optimize(
   quantize=False,
   freeze=False,
   example=None,
-  device=None
+  device=None,
 ):
   if benchmark:
     from torch.backends import cudnn
+
     cudnn.benchmark = True
 
   if model:
     if example:
       if isinstance(example, tuple):
         import torch
+
         example = torch.randn(*example)
 
       if jit:
         import torch.jit
+
         model = torch.jit.trace(model, example)
   return model
-
diff --git a/yann/schedule.py b/yann/schedule.py
index 4c79b40..e8fe044 100644
--- a/yann/schedule.py
+++ b/yann/schedule.py
@@ -1,9 +1,7 @@
-
 from functools import update_wrapper
 from typing import Any
 
 
-
 class Scheduler:
   def __init__(self):
     self.index = 0
@@ -60,10 +58,8 @@ def scheduled(*, get_step=None, **params):
   Returns:
 
   """
+
   def decorator(func):
     return Scheduled(func, **params, get_step=get_step)
 
   return decorator
-
-
-
diff --git a/yann/testing.py b/yann/testing.py
index 46a0262..81a323b 100644
--- a/yann/testing.py
+++ b/yann/testing.py
@@ -6,12 +6,12 @@ TODO: snapshot testing
 TODO: acceptance criteria / validation against test set
 """
 
+from contextlib import contextmanager
 
 import torch
-from contextlib import contextmanager
 
-from .utils.debug import iter_allocated_tensors
 from .exceptions import CheckFailure
+from .utils.debug import iter_allocated_tensors
 
 
 def check_tensor(
@@ -34,21 +34,18 @@ def check_tensor(
   gt=None,
   lte=None,
   gte=None,
-  none=False
+  none=False,
 ):
   if share_memory is not None:
     assert t.storage().data_ptr() == share_memory.storage().data_ptr()
   if not_share_memory is not None:
-    assert t.storage().data_ptr() != not_share_memory.storage(
-    ).data_ptr()
+    assert t.storage().data_ptr() != not_share_memory.storage().data_ptr()
   if different is not None:
     assert different is not t
   if same is not None:
     assert same is t
   if like is not None:
-    check_tensor(
-      t, device=like.device, shape=like.shape, dtype=like.shape
-    )
+    check_tensor(t, device=like.device, shape=like.shape, dtype=like.shape)
   if not none:
     assert t is not None
   if device:
@@ -109,12 +106,10 @@ def newly_allocated_tensors(count=None, max=None):
   diff = len(new_tensors)
 
   if count is not None and count != diff:
-    raise CheckFailure(
-      f'Expected {count} tensor allocations but got {diff}'
-    )
+    raise CheckFailure(f'Expected {count} tensor allocations but got {diff}')
   if max is not None:
     raise CheckFailure(
-      f'Expected at most {count} tensor allocations but got {diff}'
+      f'Expected at most {count} tensor allocations but got {diff}',
     )
 
 
@@ -164,7 +159,7 @@ def rand_image_tensor(
   min=0,
   max=1,
   dtype=None,
-  device=None
+  device=None,
 ):
   if width is None:
     width = height
@@ -180,13 +175,11 @@ def rand_image_batch(
   min=0,
   max=1,
   dtype=None,
-  device=None
+  device=None,
 ):
   if width is None:
     width = height
-  t = torch.rand(
-    num, channels, height, width, dtype=dtype, device=device
-  )
+  t = torch.rand(num, channels, height, width, dtype=dtype, device=device)
   return (min - max) * t + max
 
 
@@ -198,11 +191,11 @@ def check_model(
   output_shape=None,
   # loss=None,
   device=None,
-  dtype=None
+  dtype=None,
 ):
   device = device or model.device
-  input = input if input is not None else torch.rand(
-    input_shape, device=device, dtype=dtype
+  input = (
+    input if input is not None else torch.rand(input_shape, device=device, dtype=dtype)
   )
 
   output = model(input)
diff --git a/yann/train/__init__.py b/yann/train/__init__.py
index 263220c..68c75a9 100644
--- a/yann/train/__init__.py
+++ b/yann/train/__init__.py
@@ -1,16 +1,16 @@
-
 from pathlib import Path
-from ..data.io import load_json
+
 from ..data import flatten
+from ..data.io import load_json
+from .functional import step, train
 from .trainer import Trainer
 
-from .functional import train, step
-
 
 def collect_summaries(root='.', name='summary.json', pandas=True):
   s = [load_json(f) for f in Path(root).glob(f'**/*{name}')]
   if pandas:
     import pandas as pd
+
     return pd.DataFrame([flatten(x) for x in s])
   else:
     return s
diff --git a/yann/train/base.py b/yann/train/base.py
index 34b1a70..437a081 100644
--- a/yann/train/base.py
+++ b/yann/train/base.py
@@ -23,11 +23,3 @@ class BaseTrainer:
   @classmethod
   def from_checkpoint(cls, path):
     pass
-
-
-
-
-
-
-
-
diff --git a/yann/train/functional.py b/yann/train/functional.py
index c3c2f49..d7ddc3c 100644
--- a/yann/train/functional.py
+++ b/yann/train/functional.py
@@ -1,6 +1,11 @@
-
-
-def step(model, inputs, targets, optimizer, loss, callback: 'yann.callbacks.callback.Callback' = None):
+def step(
+  model,
+  inputs,
+  targets,
+  optimizer,
+  loss,
+  callback: 'yann.callbacks.callback.Callback' = None,
+):
   model.train()
   optimizer.zero_grad()
 
@@ -13,8 +18,21 @@ def step(model, inputs, targets, optimizer, loss, callback: 'yann.callbacks.call
   return inputs, targets, pred, loss
 
 
-def train(model, batches, optimizer, loss, device=None, step=step, callback: 'yann.callbacks.callback.Callback'=None):
-  for inputs, targets in batches:
+def train(
+  model,
+  batches,
+  optimizer,
+  loss,
+  device=None,
+  step=step,
+  callback: 'yann.callbacks.callback.Callback' = None,
+):
+  for batch in batches:
+    if isinstance(batch, dict):
+      inputs, targets = batch, batch  # Pass dict as both inputs and targets
+    else:
+      inputs, targets = batch  # Traditional tuple unpacking
+    
     if device:
       inputs, targets = inputs.to(device), targets.to(device)
-    yield step(model, inputs, targets, optimizer, loss)
\ No newline at end of file
+    yield step(model, inputs, targets, optimizer, loss)
diff --git a/yann/train/paths.py b/yann/train/paths.py
index b16d295..ad7a51e 100644
--- a/yann/train/paths.py
+++ b/yann/train/paths.py
@@ -1,6 +1,6 @@
 import pathlib
-from yann.utils import print_tree
 
+from yann.utils import print_tree
 
 
 class Paths:
@@ -51,7 +51,7 @@ class Paths:
 
   @property
   def profile(self):
-      return self.root / 'profile'
+    return self.root / 'profile'
 
   @property
   def git_diff(self):
@@ -62,4 +62,4 @@ class Paths:
     return self.root / 'requirements.txt'
 
   def tree(self, **kwargs):
-    print_tree(self.root, **kwargs)
\ No newline at end of file
+    print_tree(self.root, **kwargs)
diff --git a/yann/train/track.py b/yann/train/track.py
index b8a35a6..7523e86 100644
--- a/yann/train/track.py
+++ b/yann/train/track.py
@@ -1,4 +1,4 @@
-from typing import Mapping, Any
+from typing import Any, Mapping
 
 import yann
 
@@ -7,6 +7,7 @@ class Tracker:
   """
   Simple callable that takes a trainer instance as input and returns a dict of values to log
   """
+
   freq: int = None
 
   def __call__(self, trainer: 'yann.train.Trainer') -> Mapping[str, Any]:
@@ -15,17 +16,19 @@ class Tracker:
 
 class OptimizerState(Tracker):
   def __init__(
-      self,
-      optimizer=None,
-      prefix='',
-      keys=('lr', 'weight_decay', 'momentum', 'betas', 'alpha')):
+    self,
+    optimizer=None,
+    prefix='',
+    keys=('lr', 'weight_decay', 'momentum', 'betas', 'alpha'),
+  ):
     self.optimizer = optimizer
     self.keys = keys
     self.prefix = prefix
 
   def __call__(self, trainer: 'import yann.train.trainer.Trainer'):
     optim = self.optimizer or trainer.optimizer
-    if not optim: return {}
+    if not optim:
+      return {}
 
     values = {}
 
@@ -49,6 +52,7 @@ class ParamNorms(Tracker):
 
   def __call__(self, trainer):
     import yann
+
     model = self.model or trainer.model
 
     return {self.key: yann.param_norm(model)}
@@ -61,6 +65,7 @@ class GradNorms(Tracker):
 
   def __call__(self, trainer):
     import yann
+
     model = self.model or trainer.model
 
     return {self.key: yann.grad_norm(model)}
@@ -78,4 +83,3 @@ class Keys(Tracker):
       except:
         pass
     return values
-
diff --git a/yann/train/trainer.py b/yann/train/trainer.py
index 75eea4e..06ac3bd 100644
--- a/yann/train/trainer.py
+++ b/yann/train/trainer.py
@@ -1,31 +1,32 @@
-
 import datetime
 import inspect
 import logging
 import types
 from pathlib import Path
-from typing import Optional, Callable, Union, Dict, Mapping, Sequence
+from typing import Callable, Dict, Mapping, Optional, Sequence, Union
 
 import torch
 import torch.nn
-from torch import autocast
-from torch.cuda.amp import GradScaler, autocast
+from torch.amp import GradScaler, autocast
 from torch.optim.optimizer import Optimizer
-from torch.utils.data import Sampler, DataLoader
-from typing_extensions import Literal
-from typing_extensions import Unpack
+from torch.utils.data import DataLoader, Sampler
+from typing_extensions import Literal, Unpack
 
 import yann
-from yann.data import get_dataset_name, Classes
-from yann.datasets import TransformDataset, Subset
-from yann.distributed import Dist
 import yann.distributed
+from yann.data import Classes, get_dataset_name
+from yann.datasets import Subset, TransformDataset
+from yann.distributed import Dist
 from yann.export import export
 from yann.train.base import BaseTrainer
 from yann.train.paths import Paths
 from yann.utils import (
-  counter, timestr, hash_params, fully_qualified_name,
-  memorable_id, apply_known
+  apply_known,
+  counter,
+  fully_qualified_name,
+  hash_params,
+  memorable_id,
+  timestr,
 )
 from yann.utils.bash import git_diff, pip_freeze
 from yann.utils.timer import time
@@ -37,6 +38,7 @@ class Keys:
   """
   keys for data batch
   """
+
   ids = None
   inputs = 0
   targets = 1
@@ -95,19 +97,18 @@ class Params(yann.params.HyperParams):
     Callable,
     Mapping[str, Callable],
     Sequence[Callable],
-    None
+    None,
   ] = None
   transform_batch: Union[Callable, None] = None
 
   callbacks: Union[
     Sequence['yann.callbacks.Callback'],
     'yann.callbacks.Callbacks',
-    None
+    None,
   ] = None
   device: Union[torch.device, str, None] = None
   dtype: Optional[torch.dtype] = None
 
-
   val_dataset: Union[torch.utils.data.Dataset, float, str, None] = None
   val_subset: Optional[int] = None
   val_loader: Union[torch.utils.data.DataLoader, None] = None
@@ -115,21 +116,21 @@ class Params(yann.params.HyperParams):
     Callable,
     Mapping[str, Callable],
     Sequence[Callable],
-    None
+    None,
   ] = None
 
   metrics: Union[
     Dict[str, Callable],
     Sequence[Callable],
     Sequence[str],
-    None
+    None,
   ] = None
 
   dist: Optional[Dist] = None
   parallel: Union[None, Literal['dp', 'ddp']] = None
 
   amp: bool = False
-  grad_scaler: Optional[torch.cuda.amp.GradScaler] = None
+  grad_scaler: Optional[torch.amp.GradScaler] = None
 
   benchmark: bool = True
   jit: bool = False
@@ -145,7 +146,6 @@ class Params(yann.params.HyperParams):
   clip_grad: Union[Callable, 'yann.optim.clip.GradClipper', dict] = None
   seed: Optional[int] = None
 
-
   from_checkpoint: Optional[str] = None
 
 
@@ -157,8 +157,8 @@ class Trainer(TrainState, BaseTrainer):
 
 
   def train():
-    for epoch in self.epochs:
-      for batch in self.batches:
+    for epoch in self.epochs():
+      for batch in self.batches():
         self.step(batch)
       self.validate()
 
@@ -167,6 +167,7 @@ class Trainer(TrainState, BaseTrainer):
     self.update()
 
   """
+
   Params = Params
 
   params: Params
@@ -190,7 +191,6 @@ class Trainer(TrainState, BaseTrainer):
 
   history: 'yann.callbacks.History' = None
 
-
   # Dict with training run summary information
   summary: dict
 
@@ -199,227 +199,110 @@ class Trainer(TrainState, BaseTrainer):
   DataLoader = DataLoader
 
   @classmethod
-  def from_params(
-      cls,
-      params: Params,
-      **kwargs: Unpack[Params]
-  ):
-
-    return cls(
-      **{
-        **params,
-        **kwargs
-      },
-      params=params
-    )
+  def from_params(cls, params: Params, **kwargs: Unpack[Params]):
+    return cls(**{**params, **kwargs}, params=params)
 
   @time('Initialize Trainer')
   def __init__(
-      self,
-
-      # Metadata
-      id: Union[str, int, None] = None,
-      name: Union[str, None] = None,
-      description: Optional[str] = None,
-      project: Optional[str] = None,
-      meta: Optional[Dict] = None,
-
-
-      model: Union[torch.nn.Module, str, None] = None,
-      dataset: Union[torch.utils.data.Dataset, str, None] = None,
-      optimizer: Union[torch.optim.Optimizer, str, None] = None,
-      loss: Union[torch.nn.Module, str, None] = None,
-
-
-      # root directory where training runs are stored
-      root: Union[str, Path, Paths] = None,
-
-      subset: Optional[int] = None,
-      batch_size: int = None,
-      classes: Union[yann.data.Classes, Sequence[str], None] = None,
-
-      parameters: Union[torch.nn.ParameterList, Literal['trainable']] = None,
-      lr: Optional[float] = None,
-      weight_decay: Optional[float] = None,
-      momentum: Optional[float] = None,
-      lr_scheduler: Union[torch.optim.lr_scheduler._LRScheduler, None] = None,
-      lr_batch_step: bool = None,
-      none_grad: bool = None,
-
-      epochs: Optional[int] = None,
-
-      loader: Union[torch.utils.data.DataLoader, None] = None,
-      num_workers: int = 8,
-      collate: Union[Callable, None] = None,
-      pin_memory: bool = None,
-      sampler: Union[torch.utils.data.Sampler, None] = None,
-      batch_sampler: Union[torch.utils.data.BatchSampler, None] = None,
-      prefetch_factor: Optional[int] = None,
-      persistent_workers: Optional[bool] = True,
-
-      transform: Union[
-        Callable,
-        Mapping[str, Callable],
-        Sequence[Callable],
-        None
-      ] = None,
-      transform_batch: Union[Callable, None] = None,
-
-      callbacks: Union[
-        Sequence['yann.callbacks.Callback'],
-        'yann.callbacks.Callbacks',
-        None,
-        bool
-      ] = None,
-      device: Union[torch.device, str, None] = None,
-      dtype: Optional[torch.dtype] = None,
-
-      val_dataset: Union[torch.utils.data.Dataset, str, float, None] = None,
-      val_subset: Optional[int] = None,
-      val_loader: Union[torch.utils.data.DataLoader, None] = None,
-      val_transform: Union[
-        Callable,
-        Mapping[str, Callable],
-        Sequence[Callable],
-        None
-      ] = None,
-
-      metrics: Union[
-        Dict[str, Callable],
-        Sequence[Callable],
-        Sequence[str],
-        None
-      ] = None,
-
-      dist: Optional[Dist] = None,
-      parallel: Union[None, Literal['dp', 'ddp']] = None,
-
-      amp: bool = None,
-      grad_scaler: Optional[torch.cuda.amp.GradScaler] = None,
-
-      benchmark: bool = None,
-      jit: bool = None,
-      memory_format: Optional[str] = None,
-      aot_autograd: bool = None,
-      cuda_graph: bool = None,
-
-      compile: bool = None,
-      tf32: bool = None,
-
-      step: Optional[Callable] = None,
-      place: Optional[Union[Callable, dict, tuple, yann.data.place.Place]] = None,
-      clip_grad: Union[Callable, 'yann.optim.clip.GradClipper', dict] = None,
-      seed: Optional[int] = None,
-      keys: Keys = None,
-
-      from_checkpoint: Optional[str] = None,
-
-      params: Union[Params, str, None] = None,
-      **extra
+    self,
+    /,
+    params: Union[Params, str, None] = None,
+    **kwargs: Unpack[Params],
   ):
     super().__init__()
 
-    kwargs = {**locals()}
-    kwargs.pop('self')
-    kwargs.pop('params')
-    kwargs.pop('__class__')
-
-    self.params = params or self.Params()
+    self.params = (
+      params
+      if isinstance(params, self.Params)
+      else self.Params(params) if params else self.Params()
+    )
+    self.params.update(kwargs)
 
-    if seed is not None:
-      yann.seed(seed)
+    if self.params.seed is not None:
+      yann.seed(self.params.seed)
 
-    self.id = id or memorable_id()
-    self.name = name
-    self.description = description
-    self.project = project
-    self.meta = meta
+    self.id = self.params.id or memorable_id()
+    self.name = self.params.name
+    self.description = self.params.description
+    self.project = self.params.project
+    self.meta = self.params.meta
     self.summary = {}
 
     self.time_created = datetime.datetime.utcnow()
 
-    self._epochs = epochs
+    self._epochs = self.params.epochs
 
-    if benchmark:
+    if self.params.benchmark:
       yann.benchmark()
 
-    self.dist = dist or Dist()
+    self.dist = self.params.dist or Dist()
     self.dist.initialize()
 
+    device = self.params.device
     if self.dist.is_enabled:
       device = device or self.dist.device
 
     device = yann.default.device if device is None else device
     self.device = torch.device(device) if isinstance(device, str) else device
 
-    self.memory_format = yann.memory_formats.get(
-      memory_format,
-      memory_format
-    )
-    self.dtype = dtype
+    self.memory_format = yann.memory_formats.get(self.params.memory_format, self.params.memory_format)
+    self.dtype = self.params.dtype
 
-    self.parallel = parallel
-    self.lr_batch_step = lr_batch_step
-    self.none_grad = none_grad
+    self.lr_batch_step = self.params.lr_batch_step
+    self.none_grad = self.params.none_grad
 
-    self.model = yann.resolve.model(
-      model,
-      required=False,
-      validate=callable
-    )
+    self.model = yann.resolve.model(self.params.model, required=False, validate=callable)
 
-    if tf32:
+    if self.params.tf32:
       torch.backends.cuda.matmul.allow_tf32 = True
 
-    if compile:
-      if isinstance(compile, dict):
-        self.model = torch.compile(self.model, **compile)
+    compile_arg = self.params.compile
+    if compile_arg:
+      if isinstance(compile_arg, dict):
+        self.model = torch.compile(self.model, **compile_arg)
       else:
         self.model = torch.compile(self.model)
 
-    if jit:
+    if self.params.jit:
       self.model = torch.jit.script(self.model)
-    if aot_autograd:
+    if self.params.aot_autograd:
       try:
         from functorch.compile import memory_efficient_fusion
       except ImportError:
         raise ValueError('functorch must be installed for aot_autograd support')
       self.model = memory_efficient_fusion(self.model)
 
-    self.loss = yann.resolve.loss(
-      loss,
-      required=False,
-      validate=callable
-    )
+    self.loss = yann.resolve.loss(self.params.loss, required=False, validate=callable)
 
-    self._init_parallel(**kwargs)
-    self._init_optim(**kwargs)
-    self._init_data_loaders(**kwargs)
-    self._init_amp(**kwargs)
-    self._init_callbacks(**kwargs)
+    self._init_parallel()
+    self._init_optim()
+    self._init_data_loaders()
+    self._init_amp()
+    self._init_callbacks()
 
-    if step is not None:
-      self.override(self.step, step)
+    if self.params.step is not None:
+      self.override(self.step, self.params.step)
 
-    if place is not None:
-      if isinstance(place, Callable):
-        self.place = place
+    if self.params.place is not None:
+      if isinstance(self.params.place, Callable):
+        self.place = self.params.place
       else:
         from yann.data.place import Place
-        self.place = Place(place)
+        self.place = Place(self.params.place)
 
     self.to(device=self.device, memory_format=self.memory_format)
 
-    self.name = name or (
-      f"{get_dataset_name(self.loader)}-{yann.get_model_name(self.model)}"
+    self.name = self.name or (
+      f'{get_dataset_name(self.loader)}-{yann.get_model_name(self.model)}'
     )
 
+    root = self.params.root
     if isinstance(root, Paths):
       self.paths = root
     else:
       self.paths = Paths(
-        Path(root or yann.default.train_root) / self.name / timestr(
-          self.time_created))
+        Path(root or yann.default.train_root) / self.name / timestr(self.time_created),
+      )
     self.paths.create()
 
     if self.dist.is_main:
@@ -429,238 +312,188 @@ class Trainer(TrainState, BaseTrainer):
         pass  # not in git repo
       yann.save.txt(pip_freeze(), self.paths.requirements)
 
-    if from_checkpoint:
-      self.load_checkpoint(from_checkpoint)
+    if self.params.from_checkpoint:
+      self.load_checkpoint(self.params.from_checkpoint)
 
     self.update_summary()
     self.save_summary()
 
-    # self.callbacks.on_init(trainer=self, kwargs=kwargs)
-
-  def _init_callbacks(
-      self,
-      metrics=None,
-      callbacks=None,
-      **kwargs
-  ):
+  def _init_callbacks(self, **kwargs):
     from yann.callbacks import get_callbacks
     from yann.callbacks.callbacks import Callbacks
 
-    # metrics = metrics or ()
-    # if not self.dist.is_main:
-    #   # NOTE: disable most callbacks if not on main
-    #   self.history = yann.callbacks.History(**metrics) \
-    #     if isinstance(metrics, dict) \
-    #     else yann.callbacks.History(*metrics)
-    #   self.callbacks = Callbacks(self.history)
-    #   return
-
-    callbacks = get_callbacks() if callbacks is True else callbacks
-    callbacks = [c for c in callbacks if yann.distributed.matches(c.dist_placement, self.dist)]
+    callbacks = get_callbacks() if self.params.callbacks is True else self.params.callbacks
+    callbacks = callbacks or []
+    callbacks = [
+      c for c in callbacks if yann.distributed.matches(c.dist_placement, self.dist)
+    ]
 
     self.callbacks = Callbacks(callbacks)
 
     if 'history' not in self.callbacks:
+      metrics = self.params.metrics
       metrics = (metrics,) if isinstance(metrics, str) else metrics
-      self.callbacks.history = yann.callbacks.History(**metrics) \
-        if isinstance(metrics, dict) \
-        else yann.callbacks.History(*metrics)
+      self.callbacks.history = (
+        yann.callbacks.History(**metrics)
+        if isinstance(metrics, dict)
+        else yann.callbacks.History(*metrics or ())
+      )
 
     self.history = self.callbacks.history
     self.callbacks.move_to_start('history')
 
-  def _init_parallel(self, parallel=None, **kwargs):
+  def _init_parallel(self, **kwargs):
     if self.model is not None:
-      if parallel == 'dp':
+      if self.params.parallel == 'dp':
         if not isinstance(self.model, torch.nn.parallel.DataParallel):
           self.model = torch.nn.DataParallel(self.model)
-      elif parallel == 'ddp' or (parallel is None and self.dist.is_enabled):
-        self.parallel = 'ddp'
-        if not isinstance(self.model, torch.nn.parallel.DistributedDataParallel):
+      elif self.params.parallel == 'ddp' or (self.params.parallel is None and self.dist.is_enabled):
+        self.parallel = 'ddp'  # Store the effective parallel type
+        if not isinstance(
+          self.model,
+          torch.nn.parallel.DistributedDataParallel,
+        ):
           self.model.to(self.device)
           self.model = torch.nn.parallel.DistributedDataParallel(
             self.model,
             device_ids=[self.dist.local_rank],
             output_device=self.dist.local_rank,
-            find_unused_parameters=yann.default.ddp_find_unused_parameters
+            find_unused_parameters=yann.default.ddp_find_unused_parameters,
           )
 
   @time('Initialize Data Loading')
-  def _init_data_loaders(
-      self,
-      dataset=None,
-      batch_size=None,
-      subset=None,
-      classes=None,
-      transform=None,
-      transform_batch=None,
-      loader=None,
-      sampler=None,
-      batch_sampler=None,
-      pin_memory=None,
-      collate=None,
-      num_workers=None,
-      prefetch_factor=2,
-      persistent_workers=True,
-      val_dataset=None,
-      val_subset=None,
-      val_transform=None,
-      val_loader=None,
-      **kwargs
-  ):
-    self.batch_size = batch_size or yann.default.batch_size
-
-    self.dataset = yann.resolve.dataset(dataset, required=False)
-    if self.dataset and subset is not None:
+  def _init_data_loaders(self, **kwargs):
+    self.dataset = yann.resolve.dataset(self.params.dataset, required=False)
+    if self.dataset and self.params.subset is not None:
       self.dataset = yann.datasets.Subset(
         self.dataset,
-        *subset
-        if isinstance(subset, tuple)
-        else (subset,)
+        *self.params.subset if isinstance(self.params.subset, tuple) else (self.params.subset,),
       )
 
+    val_dataset = self.params.val_dataset # Temporary for split logic
     if isinstance(val_dataset, float):
       split = 1 - val_dataset
-      val_dataset = Subset(dataset, split, 1.0)
-      self.dataset = Subset(dataset, 0, split)
+      self.val_dataset_resolved = Subset(self.dataset, split, 1.0) # Resolved val dataset
+      self.dataset = Subset(self.dataset, 0, split) # Updated train dataset
+    else:
+      self.val_dataset_resolved = yann.resolve.dataset(val_dataset, required=False)
 
+    classes = self.params.classes
     if classes:
-      self.classes = (
-        classes if isinstance(classes, Classes)
-        else Classes(classes)
-      )
-    elif hasattr(self.dataset, 'classes') and \
-        isinstance(self.dataset.classes, Classes):
+      self.classes = classes if isinstance(classes, Classes) else Classes(classes)
+    elif hasattr(self.dataset, 'classes') and isinstance(
+      self.dataset.classes,
+      Classes,
+    ):
       self.classes = self.dataset.classes
     else:
       self.classes = None
 
-    if transform:
-      self.dataset = TransformDataset(self.dataset, transform)
-    self.transform = transform
+    if self.params.transform:
+      self.dataset = TransformDataset(self.dataset, self.params.transform)
 
-    self.transform_batch = transform_batch
-
-    self.sampler = sampler
-    if not self.sampler and self.dist.is_enabled:
+    self.sampler = self.params.sampler
+    if not self.sampler and self.dist.is_enabled and self.dataset:
       self.sampler = torch.utils.data.distributed.DistributedSampler(
         self.dataset,
         num_replicas=self.dist.world_size,
-        rank=self.dist.rank
+        rank=self.dist.rank,
       )
 
-    if loader is not None:
-      self.loader = loader
+    if self.params.loader is not None:
+      self.loader = self.params.loader
     elif self.dataset is not None:
-      if batch_sampler is not None:
+      if self.params.batch_sampler is not None:
         loader_signature = inspect.signature(self.DataLoader)
         if 'batch_sampler' not in loader_signature.parameters:
           raise ValueError(
-            'batch_sampler provided but DataLoader does not support it, might need to upgrade pytorch to newer version')
+            'batch_sampler provided but DataLoader does not support it, might need to upgrade pytorch to newer version',
+          )
         self.loader = self.DataLoader(
           dataset=self.dataset,
-          batch_sampler=batch_sampler,
-          pin_memory=pin_memory,
-          num_workers=num_workers,
-          persistent_workers=persistent_workers and num_workers > 0,
-          prefetch_factor=prefetch_factor,
-          **({'collate_fn': collate} if collate else {})
+          batch_sampler=self.params.batch_sampler,
+          pin_memory=self.params.pin_memory,
+          num_workers=self.params.num_workers,
+          persistent_workers=self.params.persistent_workers and self.params.num_workers > 0,
+          prefetch_factor=self.params.prefetch_factor,
+          **({'collate_fn': self.params.collate} if self.params.collate else {}),
         )
       else:
         self.loader = self.DataLoader(
           dataset=self.dataset,
-          batch_size=self.batch_size,
-          pin_memory=pin_memory,
+          batch_size=self.params.batch_size or yann.default.batch_size,
+          pin_memory=self.params.pin_memory,
           shuffle=False if self.sampler else True,
           sampler=self.sampler,
-          num_workers=num_workers,
-          persistent_workers=persistent_workers and num_workers > 0,
-          prefetch_factor=prefetch_factor,
-          **({'collate_fn': collate} if collate else {})
+          num_workers=self.params.num_workers,
+          persistent_workers=self.params.persistent_workers and self.params.num_workers > 0,
+          prefetch_factor=self.params.prefetch_factor,
+          **({'collate_fn': self.params.collate} if self.params.collate else {}),
         )
 
-    self.val_dataset = yann.resolve.dataset(
-      val_dataset,
-      required=False,
-    )
+    if self.val_dataset_resolved and self.params.val_subset is not None:
+      self.val_dataset_resolved = yann.datasets.Subset(
+        self.val_dataset_resolved,
+        *self.params.val_subset if isinstance(self.params.val_subset, tuple) else (self.params.val_subset,),
+      )
 
-    if self.val_dataset and val_subset is not None:
-      self.val_dataset = yann.datasets.Subset(
-        self.val_dataset,
-        *val_subset
-        if isinstance(val_subset, tuple)
-        else (val_subset,)
+    resolved_val_transform = self.params.val_transform or self.params.transform
+    if resolved_val_transform and self.val_dataset_resolved:
+        self.val_dataset_resolved = TransformDataset(self.val_dataset_resolved, resolved_val_transform)
+
+    self.val_loader = self.params.val_loader or (
+      self.val_dataset_resolved
+      and self.DataLoader(
+        self.val_dataset_resolved,
+        batch_size=self.params.batch_size or yann.default.batch_size,
+        shuffle=False,
+        pin_memory=self.params.pin_memory,
+        num_workers=self.params.num_workers,
       )
+    )
 
-    self.val_transform = val_transform or transform
-    if self.val_transform:
-      self.val_dataset = TransformDataset(self.val_dataset, self.val_transform)
-
-    self.val_loader = val_loader or (val_dataset and self.DataLoader(
-      self.val_dataset,
-      batch_size=batch_size,
-      shuffle=False,
-      pin_memory=pin_memory,
-      num_workers=num_workers
-    ))
-
-  def _init_optim(
-      self,
-      parameters='trainable',
-      optimizer=None,
-      lr_scheduler=None,
-      lr_batch_step=None,
-      clip_grad=None,
-      lr=None,
-      weight_decay=None,
-      momentum=None,
-      **kwargs
-  ):
-    parameters = parameters
+  def _init_optim(self, **kwargs):
+    parameters = self.params.parameters
     if parameters == 'trainable' and self.model:
       parameters = yann.trainable(self.model.parameters())
 
     self.optimizer = yann.resolve.optimizer(
-      optimizer,
-      args=(parameters or self.model.parameters(),),
+      self.params.optimizer,
+      args=(parameters or (self.model.parameters() if self.model else None),),
       kwargs={
-        k: v for k, v in dict(
-          lr=lr,
-          weight_decay=weight_decay,
-          momentum=momentum
-        ).items() if v is not None
+        k: v
+        for k, v in dict(
+          lr=self.params.lr,
+          weight_decay=self.params.weight_decay,
+          momentum=self.params.momentum,
+        ).items()
+        if v is not None
       },
       required=False,
-      validate=lambda x: hasattr(x, 'step')
+      validate=lambda x: hasattr(x, 'step'),
     )
 
     self.lr_scheduler = yann.resolve.lr_scheduler(
-      lr_scheduler,
-      kwargs=dict(optimizer=self.optimizer)
+      self.params.lr_scheduler,
+      kwargs=dict(optimizer=self.optimizer),
     )
 
-    self.lr_batch_step = lr_batch_step
-
-    if clip_grad:
+    self.clip_grad = None
+    if self.params.clip_grad:
       from yann.optim import GradClipper
-      if clip_grad is True:
+
+      if self.params.clip_grad is True:
         self.clip_grad = GradClipper(value=1)
-      elif isinstance(clip_grad, dict):
-        self.clip_grad = GradClipper(**clip_grad)
-      else:
-        self.clip_grad = clip_grad
+      elif isinstance(self.params.clip_grad, dict):
+        self.clip_grad = GradClipper(**self.params.clip_grad)
+      elif callable(self.params.clip_grad):
+        self.clip_grad = self.params.clip_grad
 
-  def _init_amp(
-      self,
-      amp=None,
-      grad_scaler=None,
-      **kwargs
-  ):
-    self.amp = amp
-    if grad_scaler is False:
+  def _init_amp(self, **kwargs):
+    if self.params.grad_scaler is False:
       self.grad_scaler = None
     else:
-      self.grad_scaler = grad_scaler or (GradScaler() if self.amp else None)
+      self.grad_scaler = self.params.grad_scaler or (GradScaler() if self.params.amp else None)
 
   @property
   def root(self):
@@ -669,28 +502,35 @@ class Trainer(TrainState, BaseTrainer):
 
   def __setattr__(self, key, value):
     if key == 'optimizer':
-      if hasattr(self, 'lr_scheduler') and hasattr(self.lr_scheduler,
-                                                   'optimizer'):
+      if hasattr(self, 'lr_scheduler') and hasattr(
+        self.lr_scheduler,
+        'optimizer',
+      ):
         self.lr_scheduler.optimizer = value
     if key == 'loader':
       if hasattr(self, 'dataset') and hasattr(value, 'dataset'):
         super(Trainer, self).__setattr__('dataset', value.dataset)
       if hasattr(value, 'batch_size'):
-        super(Trainer, self).__setattr__('batch_size', value.batch_size)
-    if key == 'batch_size' and hasattr(self,
-                                       'batch_size') and self.batch_size != key:
-      if hasattr(self, 'loader') and self.loader:
-        raise ValueError(
-          'Cannot modify batch_size because a loader is defined '
-          'and modifying batch size of a loader is not supported, '
-          'try creating and setting a new loader instead'
-        )
-      if key == 'dataset' and hasattr(self, 'dataset') and self.dataset \
-          and hasattr(self, 'loader') and self.loader:
+        pass # batch_size is now only on params
+    if key == 'batch_size':
+      if self.params.batch_size != value:
+        if hasattr(self, 'loader') and self.loader:
+          raise ValueError(
+            'Cannot modify batch_size because a loader is defined '
+            'and modifying batch size of a loader is not supported, '
+            'try creating and setting a new loader instead',
+          )
+      if (
+        key == 'dataset'
+        and hasattr(self, 'dataset')
+        and self.dataset
+        and hasattr(self, 'loader')
+        and self.loader
+      ):
         raise ValueError(
           'Cannot modify dataset because a loader is defined '
           'and modifying dataset of a loader is not supported, '
-          'try creating and setting a new loader instead'
+          'try creating and setting a new loader instead',
         )
 
     log.debug(f"setting '{key}' to {value}")
@@ -707,7 +547,7 @@ class Trainer(TrainState, BaseTrainer):
       device=self.device,
       memory_format=self.memory_format,
       dtype=self.dtype,
-      **kwargs
+      **kwargs,
     )
     return self
 
@@ -727,23 +567,24 @@ class Trainer(TrainState, BaseTrainer):
             batch[self.keys.inputs],
             device=self.device,
             memory_format=self.memory_format,
-            **kwargs
+            **kwargs,
           ),
-          *yann.to(batch[1:], device=self.device, **kwargs)
+          *yann.to(batch[1:], device=self.device, **kwargs),
         )
 
     return yann.to(
       batch,
       device=self.device,
       memory_format=self.memory_format,
-      **kwargs
+      **kwargs,
     )
 
   def on(self, event, callback=None):
     if self.callbacks is not None:
       return self.callbacks.on(event, callback)
     log.warning(
-      '.on() callback registration ignored because callbacks are not defined')
+      '.on() callback registration ignored because callbacks are not defined',
+    )
 
   @property
   def training(self):
@@ -765,8 +606,8 @@ class Trainer(TrainState, BaseTrainer):
 
   def batches(self, device=None):
     for batch in self.loader:
-      if self.transform_batch:
-        batch = self.transform_batch(*batch)
+      if self.params.transform_batch:
+        batch = self.params.transform_batch(*batch)
 
       yield self.place(batch, device=device)
 
@@ -780,7 +621,8 @@ class Trainer(TrainState, BaseTrainer):
     method = method if isinstance(method, str) else method.__name__
     if not hasattr(self, method):
       raise AttributeError(
-        f"Can't override method '{method}' because it's not defined")
+        f"Can't override method '{method}' because it's not defined",
+      )
     if function is False:
       # assume it's used as a decorator
       # @train.override('step')
@@ -799,22 +641,18 @@ class Trainer(TrainState, BaseTrainer):
     if not self.training:
       self.train_mode()
 
-    outputs, loss = self.forward(
-      inputs=inputs,
-      targets=targets
-    )
+    outputs, loss = self.forward(inputs=inputs, targets=targets)
 
-    self.update(
-      loss=loss,
-      inputs=inputs,
-      targets=targets,
-      outputs=outputs
-    )
+    self.update(loss=loss, inputs=inputs, targets=targets, outputs=outputs)
 
     return outputs, loss
 
   def forward(self, inputs=None, targets=None):
-    with autocast(enabled=self.amp):
+    with autocast(
+      device_type=self.device.type,
+      dtype=self.dtype,
+      enabled=self.params.amp,
+    ):
       outputs = self.model(inputs)
       if self.loss:
         loss = self.loss(outputs, targets)
@@ -861,16 +699,16 @@ class Trainer(TrainState, BaseTrainer):
     if loader is not None:
       with torch.inference_mode():
         for inputs, targets, outputs in yann.evaluate(
-            model=self.model,
-            batches=loader,
-            device=device
+          model=self.model,
+          batches=loader,
+          device=device,
         ):
           if self.callbacks:
             self.callbacks.on_validation_batch(
               inputs=inputs,
               targets=targets,
               outputs=outputs,
-              trainer=self
+              trainer=self,
             )
           ts.append(targets)
           os.append(outputs)
@@ -885,7 +723,7 @@ class Trainer(TrainState, BaseTrainer):
         targets=ts,
         outputs=os,
         loss=loss,
-        trainer=self
+        trainer=self,
       )
 
     return loss
@@ -900,20 +738,22 @@ class Trainer(TrainState, BaseTrainer):
         self.callbacks.on_train_start(trainer=self)
 
         for epoch_idx in self.epochs(num=epochs):
-          self.callbacks.on_epoch_start(
-            epoch=self.num_epochs,
-            trainer=self
-          )
+          self.callbacks.on_epoch_start(epoch=self.num_epochs, trainer=self)
 
           if self.sampler is not None and hasattr(self.sampler, 'set_epoch'):
             self.sampler.set_epoch(epoch_idx)
 
-          for inputs, targets in self.batches():
+          for batch in self.batches():
+            if isinstance(batch, dict):
+              inputs, targets = batch, batch  # Pass dict as both inputs and targets
+            else:
+              inputs, targets = batch  # Traditional tuple unpacking
+            
             self.callbacks.on_step_start(
               index=self.num_steps,
               inputs=inputs,
               targets=targets,
-              trainer=self
+              trainer=self,
             )
             try:
               outputs, loss = self.step(inputs=inputs, targets=targets)
@@ -924,9 +764,10 @@ class Trainer(TrainState, BaseTrainer):
               self.callbacks.on_step_error(
                 index=self.num_steps,
                 error=e,
-                trainer=self
+                trainer=self,
               )
-              if self._stop: break
+              if self._stop:
+                break
               raise e
 
             self.callbacks.on_step_end(
@@ -935,7 +776,7 @@ class Trainer(TrainState, BaseTrainer):
               targets=targets,
               outputs=outputs,
               loss=loss,
-              trainer=self
+              trainer=self,
             )
 
             if self.lr_scheduler and self.lr_batch_step:
@@ -944,15 +785,18 @@ class Trainer(TrainState, BaseTrainer):
             self.num_steps += 1
             self.num_samples += len(inputs)
 
-            if self._stop: break
-          if self._stop: break
+            if self._stop:
+              break
+          if self._stop:
+            break
 
           val_loss = self.validate() if self.val_loader else None
           if self.lr_scheduler and not self.lr_batch_step:
             self._lr_scheduler_step(
               step=epoch_idx,
               metric=self.history.metrics.running_mean('loss')
-              if val_loss is None else val_loss,
+              if val_loss is None
+              else val_loss,
             )
 
           self.callbacks.on_epoch_end(epoch=epoch_idx, trainer=self)
@@ -968,24 +812,29 @@ class Trainer(TrainState, BaseTrainer):
         self.save_summary()
     else:
       for epoch_idx in self.epochs(num=epochs):
-
         if self.sampler is not None and hasattr(self.sampler, 'set_epoch'):
           self.sampler.set_epoch(epoch_idx)
 
-        for inputs, targets in self.batches():
+        for batch in self.batches():
+          if isinstance(batch, dict):
+            inputs, targets = batch, batch  # Pass dict as both inputs and targets
+          else:
+            inputs, targets = batch  # Traditional tuple unpacking
+          
           outputs, loss = self.step(inputs=inputs, targets=targets)
 
           if self.lr_scheduler and self.lr_batch_step:
             self.lr_scheduler.step(epoch=self.num_steps)
 
           self.num_steps += 1
-          self.num_samples += len(inputs)
+          self.num_samples += len(inputs) if not isinstance(inputs, dict) else len(next(iter(inputs.values())))
 
         val_loss = self.validate() if self.val_loader else None
         self._lr_scheduler_step(
           step=epoch_idx,
           metric=self.history.metrics.running_mean('loss')
-          if val_loss is None else val_loss
+          if val_loss is None
+          else val_loss,
         )
       self.update_summary()
       self.save_summary()
@@ -1008,20 +857,21 @@ class Trainer(TrainState, BaseTrainer):
   def checkpoint(self, name=None) -> Path:
     state = self.state_dict()
     path = self.paths.checkpoints / (
-      f"{name}.th" if name else
-      f"{timestr()}-epoch-{self.num_epochs:03d}-steps-{self.num_steps:05d}.th"
+      f'{name}.th'
+      if name
+      else f'{timestr()}-epoch-{self.num_epochs:03d}-steps-{self.num_steps:05d}.th'
     )
     torch.save(state, str(path))
     print(f'Saved checkpoint at {path}')
     return path
 
   def load_checkpoint(
-      self,
-      path,
-      metadata=True,
-      map_location=None,
-      strict: bool = True,
-      keys=None
+    self,
+    path,
+    metadata=True,
+    map_location=None,
+    strict: bool = True,
+    keys=None,
   ):
     # TODO: add 'latest', 'best' support
     log.info(f'Attempting to load checkpoint {path}')
@@ -1032,7 +882,7 @@ class Trainer(TrainState, BaseTrainer):
     path = path or self.paths.exports / timestr()
     export(
       model=self.model,
-      preprocess=self.val_transform,
+      preprocess=self.params.val_transform or self.params.transform,
       postprocess=postprocess,
       classes=self.classes,
       trace=trace and next(iter(self.loader)),
@@ -1042,13 +892,16 @@ class Trainer(TrainState, BaseTrainer):
         root=str(self.paths.root),
         dataset=get_dataset_name(self.dataset),
         num_steps=self.num_steps,
-        **(meta or {})
-      )
+        num_samples=self.num_samples,
+        num_epochs=self.num_epochs,
+        batch_size=self.params.batch_size,
+        time_created=self.time_created,
+        param_hash=hash_params(self.model),
+      ),
     )
 
     return path
 
-
   def state_dict(self):
     data = {
       'metadata': {
@@ -1057,24 +910,28 @@ class Trainer(TrainState, BaseTrainer):
         'num_steps': self.num_steps,
         'num_samples': self.num_samples,
         'num_epochs': self.num_epochs,
-        'batch_size': self.batch_size,
-
+        'batch_size': self.params.batch_size,
         'time_created': self.time_created,
-        'param_hash': hash_params(self.model)
-      }
+        'param_hash': hash_params(self.model),
+      },
     }
 
     for k, v in self.__dict__.items():
       if hasattr(v, 'state_dict'):
         data[k] = {
           'state_dict': v.state_dict(),
-          'class': fully_qualified_name(v)
+          'class': fully_qualified_name(v),
         }
 
     return data
 
-  def load_state_dict(self, data, metadata=True, strict: bool = True,
-                      keys=None):
+  def load_state_dict(
+    self,
+    data,
+    metadata=True,
+    strict: bool = True,
+    keys=None,
+  ):
     """
     TODO: add a way to specify which parts should be loaded (ex: model only)
     """
@@ -1083,17 +940,17 @@ class Trainer(TrainState, BaseTrainer):
     from inspect import getfullargspec
 
     for k, v in data.items():
-      if keys and k not in keys: continue
+      if keys and k not in keys:
+        continue
       if 'state_dict' in v and hasattr(self, k):
-
         entry = getattr(self, k)
         if 'strict' in getfullargspec(entry.load_state_dict).args:
           entry.load_state_dict(v['state_dict'], strict=strict)
         else:
           entry.load_state_dict(v['state_dict'])
-        log.debug(f"loaded {k}")
+        log.debug(f'loaded {k}')
       else:
-        log.debug(f"skipped loading {k}")
+        log.debug(f'skipped loading {k}')
         skipped.add(k)
 
     if metadata and 'metadata' in data:
@@ -1111,18 +968,20 @@ class Trainer(TrainState, BaseTrainer):
     return yann.utils.env_info()
 
   def update_summary(self):
-    self.summary.update(dict(
-      id=self.id,
-      name=self.name,
-      path=str(self.paths.root),
-      num_steps=self.num_steps,
-      num_samples=self.num_samples,
-      num_epochs=self.num_epochs,
-      batch_size=self.batch_size,
-      device=str(self.device),
-      time_created=self.time_created,
-      # params={k: str(v) for k, v in self.params.items()},
-    ))
+    self.summary.update(
+      dict(
+        id=self.id,
+        name=self.name,
+        path=str(self.paths.root),
+        num_steps=self.num_steps,
+        num_samples=self.num_samples,
+        num_epochs=self.num_epochs,
+        batch_size=self.params.batch_size,
+        device=str(self.device),
+        time_created=self.time_created,
+        # params={k: str(v) for k, v in self.params.items()},
+      ),
+    )
 
     if 'env' not in self.summary:
       self.summary['env'] = self._get_env()
@@ -1130,27 +989,35 @@ class Trainer(TrainState, BaseTrainer):
     if self.dataset:
       if 'dataset' not in self.summary:
         self.summary['dataset'] = {}
-      self.summary['dataset'].update(dict(
-        name=get_dataset_name(self.dataset),
-        size=len(self.dataset),
-        num_classes=len(self.dataset.classes) if hasattr(self.dataset,
-                                                         'classes') else None
-      ))
+      self.summary['dataset'].update(
+        dict(
+          name=get_dataset_name(self.dataset),
+          size=len(self.dataset),
+          num_classes=len(self.dataset.classes)
+          if hasattr(self.dataset, 'classes')
+          else None,
+        ),
+      )
     if self.model:
       if 'model' not in self.summary:
         self.summary['model'] = {}
-      self.summary['model'].update(dict(
-        name=yann.get_model_name(self.model),
-        param_count=yann.param_count(self.model),
-        trainable_param_count=yann.param_count(
-          yann.trainable(self.model.parameters()))
-      ))
+      self.summary['model'].update(
+        dict(
+          name=yann.get_model_name(self.model),
+          param_count=yann.param_count(self.model),
+          trainable_param_count=yann.param_count(
+            yann.trainable(self.model.parameters()),
+          ),
+        ),
+      )
 
   def save_summary(self):
     try:
       yann.save(self.summary, self.paths.summary)
     except Exception as e:
-      log.warning(f'Failed to save summary, most likely due to unserializable params, {e}')
+      log.warning(
+        f'Failed to save summary, most likely due to unserializable params, {e}',
+      )
     return self.paths.summary
 
   def __str__(self):
@@ -1158,7 +1025,7 @@ class Trainer(TrainState, BaseTrainer):
 id: {self.id}
 name: {self.name}
 root: {self.root}
-batch_size: {self.batch_size}
+batch_size: {self.params.batch_size}
 device: {self.device}
 
 PARAMS
@@ -1212,13 +1079,13 @@ samples: {self.num_samples}
 
   def __repr__(self):
     return (
-      f"Trainer("
-      f"\n  id={self.id},"
-      f"\n  name={self.name},"
-      f"\n  root={self.root},"
-      f"\n  batch_size={self.batch_size},"
-      f"\n  device={self.device}"
-      "\n)"
+      f'Trainer('
+      f'\n  id={self.id},'
+      f'\n  name={self.name},'
+      f'\n  root={self.root},'
+      f'\n  batch_size={self.params.batch_size},'
+      f'\n  device={self.device}'
+      '\n)'
     )
 
   def __getstate__(self):
diff --git a/yann/transforms/__init__.py b/yann/transforms/__init__.py
index 6b68d92..02e6144 100644
--- a/yann/transforms/__init__.py
+++ b/yann/transforms/__init__.py
@@ -1,16 +1,23 @@
 import base64
-import os
-from typing import Protocol, Any
 import io
-import numpy as np
+import os
 import pathlib
-import torch
 import random
+from typing import Any, Protocol
+
+import numpy as np
+import torch
 from PIL import Image
-from timm.data.mixup import rand_bbox
+from torchvision import transforms
 from torchvision import transforms as tvt
 from torchvision.transforms.functional import to_pil_image
-from torchvision import transforms
+
+try:
+  from timm.data.mixup import rand_bbox
+except ImportError:
+  HAS_TIMM = False
+else:
+  HAS_TIMM = True
 
 from ..utils import truthy
 
@@ -21,17 +28,11 @@ class Transform(Protocol):
 
 
 class FittableTransform(Transform):
-  def fit(self, x: Any):
-    ...
-
-  def transform(self, x: Any) -> Any:
-    ...
-
-  def fit_transform(self, x: Any) -> Any:
-    ...
-
+  def fit(self, x: Any): ...
 
+  def transform(self, x: Any) -> Any: ...
 
+  def fit_transform(self, x: Any) -> Any: ...
 
 
 class Transforms:
@@ -58,40 +59,38 @@ class Transforms:
 
   def __repr__(self):
     return (
-      f"{self.__class__.__name__}(\n"
-      f"  load={str(self.load)}\n"
-      f"  transform={str(self.transform)}\n"
-      f"  to_tensor={str(self.to_tensor)}\n"
-      ")")
-
-
-
+      f'{self.__class__.__name__}(\n'
+      f'  load={str(self.load)}\n'
+      f'  transform={str(self.transform)}\n'
+      f'  to_tensor={str(self.to_tensor)}\n'
+      ')'
+    )
 
 
- # for backwards compatibility after rename, to avoid confusion with "Transformers"
+# for backwards compatibility after rename, to avoid confusion with "Transformers"
 Transformer = Transforms
 
 
 class ImageTransforms(Transforms):
   def __init__(
-      self,
-      load=None,
-      resize=None,
-      rotate=None,
-      crop=None,
-      warp=None,
-      mirror=None,
-      mean=None,
-      std=None,
-      color_jitter=None,
-      interpolation=None,
-      color_space=None,
-      transform=None,
-      to_tensor=None,
-      autoaugment=None,
-      randaugment=None,
-      trivialaugment=None,
-      erase=None
+    self,
+    load=None,
+    resize=None,
+    rotate=None,
+    crop=None,
+    warp=None,
+    mirror=None,
+    mean=None,
+    std=None,
+    color_jitter=None,
+    interpolation=None,
+    color_space=None,
+    transform=None,
+    to_tensor=None,
+    autoaugment=None,
+    randaugment=None,
+    trivialaugment=None,
+    erase=None,
   ):
     interpolation = interpolation or Image.ANTIALIAS
     self.resize = resize and tvt.Resize(resize, interpolation=interpolation)
@@ -102,10 +101,11 @@ class ImageTransforms(Transforms):
       else tvt.CenterCrop(crop)
     )
     self.mirror = mirror and tvt.RandomHorizontalFlip(
-      .5 if mirror is True else mirror)
+      0.5 if mirror is True else mirror,
+    )
 
     if color_jitter is True:
-      color_jitter = (.4, .2, .1, .05)
+      color_jitter = (0.4, 0.2, 0.1, 0.05)
     self.color_jitter = color_jitter and tvt.ColorJitter(*color_jitter)
 
     self.normalize = (mean or std) and tvt.Normalize(mean=mean, std=std)
@@ -120,25 +120,25 @@ class ImageTransforms(Transforms):
     self.randaugment = randaugment and tvt.RandAugment()
     self.trivialaugment = trivialaugment and tvt.TrivialAugmentWide()
 
-
     super().__init__(
       load=load or GetImage(color_space),
-      transform=tvt.Compose(truthy([
-        self.resize,
-        transform,
-        self.autoagument,
-        self.randaugment,
-        self.trivialaugment,
-        self.rotate,
-        self.crop,
-        self.mirror,
-        self.color_jitter,
-      ])),
-      to_tensor=to_tensor or tvt.Compose(truthy([
-        tvt.ToTensor(),
-        self.normalize,
-        self.erase
-      ]))
+      transform=tvt.Compose(
+        truthy(
+          [
+            self.resize,
+            transform,
+            self.autoagument,
+            self.randaugment,
+            self.trivialaugment,
+            self.rotate,
+            self.crop,
+            self.mirror,
+            self.color_jitter,
+          ],
+        ),
+      ),
+      to_tensor=to_tensor
+      or tvt.Compose(truthy([tvt.ToTensor(), self.normalize, self.erase])),
     )
 
   def state_dict(self):
@@ -148,7 +148,7 @@ class ImageTransforms(Transforms):
     pass
 
 
- # for backwards compatibility after rename, to avoid confusion with "Transformers"
+# for backwards compatibility after rename, to avoid confusion with "Transformers"
 ImageTransformer = ImageTransforms
 
 
@@ -158,8 +158,7 @@ class DictTransforms:
 
   def __call__(self, data: dict):
     return {
-      k: (self.transforms[k](v) if k in self.transforms else v)
-      for k, v in data.items()
+      k: (self.transforms[k](v) if k in self.transforms else v) for k, v in data.items()
     }
 
 
@@ -171,7 +170,6 @@ class BatchTransforms:
     return [self.transform(x) for x in items]
 
 
-
 class GetImage:
   def __init__(self, space=None):
     self.color_space = space
@@ -199,6 +197,7 @@ def get_image(x, space=None) -> Image.Image:
   if isinstance(x, str):
     if x.startswith('http') or x.startswith('www.'):
       import requests
+
       x = requests.get(x).content
     elif x.startswith('data') and 'base64,' in x:
       # data header for base64 encoded
@@ -233,7 +232,7 @@ def mixup(inputs, targets, alpha=1):
   fraction = np.random.beta(alpha, alpha)
   return (
     fraction * inputs + (1 - fraction) * inputs[shuffled_indices],
-    fraction * targets + (1 - fraction) * targets[shuffled_indices]
+    fraction * targets + (1 - fraction) * targets[shuffled_indices],
   )
 
 
@@ -245,7 +244,7 @@ class Mixup:
     return mixup(inputs=inputs, targets=targets, alpha=self.alpha)
 
 
-def cutout(img, percent=.3, value=0):
+def cutout(img, percent=0.3, value=0):
   pil_img = False
   if isinstance(img, Image.Image):
     img = np.array(img)
@@ -258,33 +257,38 @@ def cutout(img, percent=.3, value=0):
   start_h = random.randint(0, (height - mask_height))
   start_w = random.randint(0, (width - mask_width))
 
-  img[start_h:start_h + mask_height, start_w:start_w + mask_width] = value
+  img[start_h : start_h + mask_height, start_w : start_w + mask_width] = value
   return Image.fromarray(img) if pil_img else img
 
 
-def cutmix(inputs, targets, beta):
-  lam = np.random.beta(beta, beta)
-  rand_index = torch.randperm(inputs.size()[0]).cuda()
-  target_a = targets
-  target_b = targets[rand_index]
-  bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)
-  inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]
-  # adjust lambda to exactly match pixel ratio
-  lam = 1 - (
-        (bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))
+if HAS_TIMM:
 
+  def cutmix(inputs, targets, beta):
+    lam = np.random.beta(beta, beta)
+    rand_index = torch.randperm(inputs.size()[0]).cuda()
+    target_a = targets
+    target_b = targets[rand_index]
+    bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)
+    inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[
+      rand_index,
+      :,
+      bbx1:bbx2,
+      bby1:bby2,
+    ]
+    # adjust lambda to exactly match pixel ratio
+    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))
 
 
 def get_imagenet_transforms(
-    size=224,
-    crop_scale=(.5, 1.2),
-    val_size=None,
-    resize=256,
-    fixres=False,
-    trivial=False,
-
+  size=224,
+  crop_scale=(0.5, 1.2),
+  val_size=None,
+  resize=256,
+  fixres=False,
+  trivial=False,
 ):
-  augment = transforms.Compose([
+  augment = transforms.Compose(
+    [
       transforms.RandomResizedCrop(
         size,
         scale=crop_scale,
@@ -294,32 +298,41 @@ def get_imagenet_transforms(
       #     .3, .3, .3
       # ),
       transforms.RandomHorizontalFlip(),
-    ])
+    ],
+  )
 
   train_transform = Transforms(
     load=GetImage('RGB'),
     transform=augment,
-    to_tensor=transforms.Compose([
-      transforms.ToTensor(),
-      # transforms.RandomErasing(),
-      transforms.Normalize(
+    to_tensor=transforms.Compose(
+      [
+        transforms.ToTensor(),
+        # transforms.RandomErasing(),
+        transforms.Normalize(
           mean=[0.485, 0.456, 0.406],
-          std=[0.229, 0.224, 0.225])
-    ])
+          std=[0.229, 0.224, 0.225],
+        ),
+      ],
+    ),
   )
 
   test_transform = Transforms(
     load=train_transform.load,
-    transform=transforms.Compose([
-      transforms.Resize(val_size or size, interpolation=Image.ANTIALIAS),
-      transforms.CenterCrop(val_size or size)
-    ]),
-    to_tensor=transforms.Compose([
-      transforms.ToTensor(),
-      transforms.Normalize(
+    transform=transforms.Compose(
+      [
+        transforms.Resize(val_size or size, interpolation=Image.ANTIALIAS),
+        transforms.CenterCrop(val_size or size),
+      ],
+    ),
+    to_tensor=transforms.Compose(
+      [
+        transforms.ToTensor(),
+        transforms.Normalize(
           mean=[0.485, 0.456, 0.406],
-          std=[0.229, 0.224, 0.225])
-    ])
+          std=[0.229, 0.224, 0.225],
+        ),
+      ],
+    ),
   )
 
   return train_transform, test_transform
diff --git a/yann/typedefs.py b/yann/typedefs.py
index ffab150..8e2ed7c 100644
--- a/yann/typedefs.py
+++ b/yann/typedefs.py
@@ -1,6 +1,6 @@
-import torch
 import typing
 
+import torch
 
 Logits = torch.FloatTensor
 Embedding = torch.FloatTensor
@@ -12,7 +12,6 @@ LogProbabilities = torch.FloatTensor
 Batch = typing.NewType('Batch', torch.Tensor)
 
 ClassIndices = torch.LongTensor
-OneHot = torch.FloatTensor
 MultiLabelOneHot = OneHot
 
 ImageTensor = torch.ShortTensor
diff --git a/yann/utils/__init__.py b/yann/utils/__init__.py
index 04a4fbe..908c2c2 100644
--- a/yann/utils/__init__.py
+++ b/yann/utils/__init__.py
@@ -1,13 +1,15 @@
 import argparse
+import datetime
 import re
+import subprocess
 import sys
-from typing import Union, Optional, Dict, Any, TYPE_CHECKING
 import typing
 from contextlib import contextmanager
+from typing import TYPE_CHECKING, Any, Dict, Optional, Union
+
 import numpy as np
 import torch
 from PIL import Image
-import datetime
 
 if TYPE_CHECKING:
   import yann
@@ -15,13 +17,80 @@ if TYPE_CHECKING:
 from .ids import memorable_id
 
 
+def get_env_info(save_to_path=None):
+  """Get environment dependencies and optionally save to files.
+  
+  Args:
+    save_to_path: Optional directory path to save environment files
+                  (env.yml for conda, requirements.txt for pip)
+  
+  Returns:
+    dict: Environment information including packages and versions
+  """
+  import os
+  
+  env_data = {}
+  
+  # Try to get conda environment info
+  try:
+    result = subprocess.run(
+      ['conda', 'env', 'export'],
+      capture_output=True,
+      text=True,
+      check=True,
+    )
+    env_data['conda'] = result.stdout
+    if save_to_path:
+      with open(os.path.join(save_to_path, 'env.yml'), 'w') as f:
+        f.write(result.stdout)
+  except (FileNotFoundError, subprocess.CalledProcessError):
+    pass  # Conda not available
+  
+  # Try to get pip requirements
+  requirements = None
+  
+  # Try uv pip freeze first
+  try:
+    result = subprocess.run(
+      ['uv', 'pip', 'freeze'],
+      capture_output=True,
+      text=True,
+      check=True,
+    )
+    requirements = result.stdout
+    env_data['pip'] = requirements
+    env_data['pip_tool'] = 'uv'
+  except (FileNotFoundError, subprocess.CalledProcessError):
+    # Try regular pip freeze
+    try:
+      result = subprocess.run(
+        ['pip', 'freeze'],
+        capture_output=True,
+        text=True,
+        check=True,
+      )
+      requirements = result.stdout
+      env_data['pip'] = requirements
+      env_data['pip_tool'] = 'pip'
+    except (FileNotFoundError, subprocess.CalledProcessError):
+      pass  # Neither uv nor pip available
+  
+  # Save requirements.txt if we got pip info
+  if save_to_path and requirements:
+    with open(os.path.join(save_to_path, 'requirements.txt'), 'w') as f:
+      f.write(requirements)
+  
+  return env_data
+
+
 def env_info():
-  import sys
   import os
   import socket
-  from .bash import git_hash
+  import sys
+
   import yann
 
+  from .bash import git_hash
 
   try:
     gith = git_hash()
@@ -32,18 +101,15 @@ def env_info():
     cwd=os.getcwd(),
     arguments=sys.argv,
     git_hash=gith,
-    python=dict(
-      executable=sys.executable,
-      version=sys.version,
-      path=sys.path
-    ),
+    python=dict(executable=sys.executable, version=sys.version, path=sys.path),
     torch_version=torch.__version__,
     yann_version=yann.__version__,
-    hostname=socket.gethostname()
+    hostname=socket.gethostname(),
   )
 
+
 def timestr(d=None):
-  return f"{(d or datetime.datetime.utcnow()).strftime('%y-%m-%dT%H:%M:%S')}"
+  return f'{(d or datetime.datetime.utcnow()).strftime("%y-%m-%dT%H:%M:%S")}'
 
 
 def camel_to_snake(text):
@@ -52,7 +118,7 @@ def camel_to_snake(text):
 
 
 def abbreviate(text):
-  return re.sub(r"([a-zA-Z])[a-z]*[^A-Za-z]*", r"\1", text).lower()
+  return re.sub(r'([a-zA-Z])[a-z]*[^A-Za-z]*', r'\1', text).lower()
 
 
 def str2bool(v):
@@ -64,8 +130,8 @@ def str2bool(v):
     return False
   else:
     import argparse
-    raise argparse.ArgumentTypeError('Boolean value expected.')
 
+    raise argparse.ArgumentTypeError('Boolean value expected.')
 
 
 def supports_primitive_types(t: type, types=(bool, str, int, float)):
@@ -77,6 +143,7 @@ def supports_primitive_types(t: type, types=(bool, str, int, float)):
 
   return True
 
+
 def get_primitive_type(t: type, types=(bool, str, int, float)):
   if t in types:
     return t
@@ -85,18 +152,26 @@ def get_primitive_type(t: type, types=(bool, str, int, float)):
       if x in t.__args__:
         return x
 
+
 def get_arg_parser(
-    x: Union[Dict[str, Dict[str, Any],], Dict[str, 'yann.params.Field']],
-    description=None,
-    epilog=None,
-    parser: Optional[argparse.ArgumentParser]=None,
-    **kwargs
+  x: Union[
+    Dict[
+      str,
+      Dict[str, Any],
+    ],
+    Dict[str, 'yann.params.Field'],
+  ],
+  description=None,
+  epilog=None,
+  parser: Optional[argparse.ArgumentParser] = None,
+  **kwargs,
 ):
   from ..params import Field
+
   parser = parser or argparse.ArgumentParser(
     description=description,
     epilog=epilog,
-    **kwargs
+    **kwargs,
   )
 
   abbreviations = {'h'}
@@ -115,9 +190,9 @@ def get_arg_parser(
     names = []
     abbreviated = abbreviate(k)
     if abbreviated not in abbreviations:
-      names.append(f"-{abbreviated}")
+      names.append(f'-{abbreviated}')
       abbreviations.add(abbreviated)
-    names.append(f"--{camel_to_snake(k)}")
+    names.append(f'--{camel_to_snake(k)}')
 
     if isinstance(v, dict):
       parser.add_argument(
@@ -128,14 +203,14 @@ def get_arg_parser(
         help=v.get('help'),
         required=v.get('required'),
         choices=v.get('choices'),
-        dest=v.get('dest')
+        dest=v.get('dest'),
       )
     elif isinstance(v, Field):
       parser.add_argument(
         *names,
         default=v.default,
         type=str2bool if prim_type is bool else prim_type,
-        help=f"{v.help or k} (default: {v.default}, type: {prim_type.__name__})",
+        help=f'{v.help or k} (default: {v.default}, type: {prim_type.__name__})',
         required=v.required,
         choices=getattr(v, 'choices', None),
       )
@@ -174,12 +249,12 @@ def progress(it, num=None):
 
   if num:
     for n, x in enumerate(it, 1):
-      sys.stdout.write(f"\r{n} / {num}")
+      sys.stdout.write(f'\r{n} / {num}')
       sys.stdout.flush()
       yield x
   else:
     for n, x in enumerate(it, 1):
-      sys.stdout.write(f"\r{n}")
+      sys.stdout.write(f'\r{n}')
       sys.stdout.flush()
       yield x
 
@@ -242,8 +317,9 @@ def pretty_size(bytes):
 
 
 def print_tree(root, indent=2, depth=None, filter=None):
-  from pathlib import Path
   from datetime import datetime
+  from pathlib import Path
+
   root = Path(root)
   for path in sorted((root, *root.rglob('*'))):
     d = len(path.relative_to(root).parts)
@@ -257,7 +333,7 @@ def print_tree(root, indent=2, depth=None, filter=None):
       print(
         f'{" " * (d * indent)}  - {path.name:25} '
         f'{f"({pretty_size(path.stat().st_size)})":15} '
-        f'{datetime.fromtimestamp(path.stat().st_mtime)}'
+        f'{datetime.fromtimestamp(path.stat().st_mtime)}',
       )
 
 
@@ -271,6 +347,7 @@ def fully_qualified_name(x):
 
 def hash_params(module):
   from hashlib import sha1
+
   s = sha1()
   for p in module.parameters():
     s.update(to_numpy(p).tobytes())
@@ -284,15 +361,16 @@ def dynamic_import(qualified_name: str):
     qualified_name: fully qualified name (ex: `torch.nn.Linear`)
   """
   import importlib
+
   module_name, obj_name = qualified_name.rsplit('.', maxsplit=1)
   module = importlib.import_module(module_name)
   return getattr(module, obj_name)
 
 
 def source_file_import(
-    path: Union[str, 'pathlib.Path'],
-    module_name: Optional[str] = None
-) -> "types.ModuleType":
+  path: Union[str, 'pathlib.Path'],
+  module_name: Optional[str] = None,
+) -> 'types.ModuleType':
   """
   Import python module from a source file
   Args:
@@ -306,6 +384,7 @@ def source_file_import(
 
   if module_name is None:
     from pathlib import Path
+
     module_name = Path(path).stem.replace('-', '_')
 
   spec = importlib.util.spec_from_file_location(module_name, str(path))
@@ -314,7 +393,7 @@ def source_file_import(
   return module
 
 
-def source_string_import(code: str, module_name: str) -> "types.ModuleType":
+def source_string_import(code: str, module_name: str) -> 'types.ModuleType':
   """
   Import code from source code string
   Args:
@@ -325,6 +404,7 @@ def source_string_import(code: str, module_name: str) -> "types.ModuleType":
     imported module
   """
   import importlib.util
+
   spec = importlib.util.spec_from_loader(module_name, loader=None)
   module = importlib.util.module_from_spec(spec)
   exec(code, module.__dict__)
@@ -344,10 +424,10 @@ def is_notebook() -> bool:
     return False
 
 
-
 @contextmanager
 def timeout(seconds, message='Exceeded time'):
   import signal
+
   def error():
     raise TimeoutError(message)
 
@@ -365,9 +445,6 @@ def apply_known(function: typing.Callable, arguments: dict):
     that are defined in the signature
   """
   import inspect
+
   sig = inspect.signature(function)
-  return function(
-    **{k: arguments[k]
-       for k in sig.parameters
-       if k in arguments
-       })
\ No newline at end of file
+  return function(**{k: arguments[k] for k in sig.parameters if k in arguments})
diff --git a/yann/utils/bash.py b/yann/utils/bash.py
index 70e8da7..d5a8718 100644
--- a/yann/utils/bash.py
+++ b/yann/utils/bash.py
@@ -1,5 +1,7 @@
+import shutil
 import subprocess
 
+
 def run(command):
   out = subprocess.check_output(command, shell=True)
   if out:
@@ -7,7 +9,7 @@ def run(command):
 
 
 def git_hash():
-  return run('git rev-parse HEAD')
+  return run('git rev-parse --short HEAD')
 
 
 def git_commit(files='.', message='automated commit', branch=None):
@@ -36,9 +38,23 @@ def nvidia_smi():
 
 
 def pip_freeze():
-  return run('pip freeze')
+  try:
+    # Try standard pip first
+    return run('pip freeze')
+  except subprocess.CalledProcessError as e:
+    if e.returncode == 127:
+      # If pip is not found, try uv pip freeze
+      try:
+        return run('uv pip freeze')
+      except subprocess.CalledProcessError as uv_e:
+        print(
+          f"Warning: 'pip freeze' and 'uv pip freeze' failed. Requirements will not be saved. Error: {uv_e}",
+        )
+        return 'Could not determine requirements.'
+    else:
+      # Re-raise other pip errors
+      raise e
 
 
 def conda_list(explicit=False):
   return run(f'conda list {"--explicit" if explicit else ""}')
-
diff --git a/yann/utils/debug.py b/yann/utils/debug.py
index 8bbfc9b..353cc9e 100644
--- a/yann/utils/debug.py
+++ b/yann/utils/debug.py
@@ -1,6 +1,7 @@
-import torch
 import gc
+
 import numpy as np
+import torch
 from torch import nn
 
 
diff --git a/yann/utils/decorators.py b/yann/utils/decorators.py
index 9e05af7..31a87d7 100644
--- a/yann/utils/decorators.py
+++ b/yann/utils/decorators.py
@@ -1,5 +1,6 @@
 from functools import wraps
 
+
 class lazy:
   __slots__ = 'method', 'name'
 
@@ -34,6 +35,7 @@ def robust(x=None, exceptions=Exception, default=None):
   if callable(x):
     return RobustFunction(func=x, exceptions=exceptions, default=default)
   else:
+
     def decorator(x):
       return RobustFunction(func=x, exceptions=exceptions, default=default)
 
@@ -58,7 +60,8 @@ def track(x=None, sanitize=None):
   if callable(x):
     return FunctionTracker(func=x)
   else:
+
     def decorator(x):
       return FunctionTracker(func=x, sanitize=sanitize)
 
-    return decorator
\ No newline at end of file
+    return decorator
diff --git a/yann/utils/ids.py b/yann/utils/ids.py
index 3237e67..fad0199 100644
--- a/yann/utils/ids.py
+++ b/yann/utils/ids.py
@@ -1,4054 +1,4057 @@
 import random
 
 adjectives = [
-  "able",
-  "about",
-  "above",
-  "abuzz",
-  "ace",
-  "achy",
-  "acid",
-  "acned",
-  "acute",
-  "adept",
-  "adorable",
-  "adult",
-  "adventurous",
-  "afire",
-  "afoot",
-  "afoul",
-  "aft",
-  "after",
-  "aged",
-  "aggressive",
-  "agile",
-  "aging",
-  "aglow",
-  "ago",
-  "agreeable",
-  "ahead",
-  "aided",
-  "airy",
-  "ajar",
-  "akin",
-  "alert",
-  "alien",
-  "alike",
-  "alive",
-  "all",
-  "alone",
-  "aloof",
-  "alpha",
-  "alto",
-  "amber",
-  "ample",
-  "amused",
-  "angry",
-  "annoyed",
-  "annoying",
-  "anti",
-  "antic",
-  "antsy",
-  "anxious",
-  "any",
-  "apart",
-  "apish",
-  "apt",
-  "arced",
-  "arch",
-  "arid",
-  "armed",
-  "arrogant",
-  "ashamed",
-  "ashen",
-  "ashy",
-  "askew",
-  "astir",
-  "atrip",
-  "attic",
-  "attractive",
-  "average",
-  "avian",
-  "avid",
-  "awake",
-  "aware",
-  "awash",
-  "away",
-  "awed",
-  "awful",
-  "awing",
-  "awned",
-  "awry",
-  "axial",
-  "azure",
-  "back",
-  "bad",
-  "baggy",
-  "baked",
-  "bald",
-  "balmy",
-  "bandy",
-  "bare",
-  "bared",
-  "basal",
-  "base",
-  "based",
-  "basic",
-  "bated",
-  "bats",
-  "batty",
-  "bay",
-  "beady",
-  "beamy",
-  "beat",
-  "beautiful",
-  "beefy",
-  "beery",
-  "beige",
-  "bent",
-  "best",
-  "beta",
-  "better",
-  "bewildered",
-  "bias",
-  "birch",
-  "bitty",
-  "black",
-  "blame",
-  "bland",
-  "blank",
-  "bleak",
-  "blear",
-  "blind",
-  "blond",
-  "bloody",
-  "blown",
-  "blue",
-  "blue-eyed",
-  "bluff",
-  "blunt",
-  "blushing",
-  "boggy",
-  "bogus",
-  "bold",
-  "bone",
-  "boned",
-  "bonny",
-  "bony",
-  "boon",
-  "boozy",
-  "bored",
-  "born",
-  "boss",
-  "bossy",
-  "both",
-  "bound",
-  "bowed",
-  "boxed",
-  "boxy",
-  "brag",
-  "brainy",
-  "brash",
-  "brave",
-  "breakable",
-  "brief",
-  "bright",
-  "briny",
-  "brisk",
-  "broad",
-  "broke",
-  "brown",
-  "brute",
-  "buff",
-  "buggy",
-  "built",
-  "bulgy",
-  "bulky",
-  "bully",
-  "bum",
-  "bumpy",
-  "burly",
-  "burnt",
-  "bush",
-  "bushy",
-  "bust",
-  "busty",
-  "busy",
-  "butch",
-  "calm",
-  "camp",
-  "campy",
-  "careful",
-  "catty",
-  "cautious",
-  "charming",
-  "cheap",
-  "cheerful",
-  "chewy",
-  "chic",
-  "chief",
-  "civic",
-  "civil",
-  "clean",
-  "clear",
-  "cleft",
-  "clever",
-  "close",
-  "cloudy",
-  "clumsy",
-  "cocky",
-  "cod",
-  "cold",
-  "color",
-  "colorful",
-  "combative",
-  "comfortable",
-  "comfy",
-  "comic",
-  "concerned",
-  "condemned",
-  "confused",
-  "cool",
-  "cooperative",
-  "coral",
-  "corny",
-  "cosy",
-  "courageous",
-  "coy",
-  "cozy",
-  "crazy",
-  "creepy",
-  "crisp",
-  "cross",
-  "crowded",
-  "cruel",
-  "cubic",
-  "cured",
-  "curious",
-  "curly",
-  "curt",
-  "curvy",
-  "cushy",
-  "cut",
-  "cute",
-  "cyan",
-  "daft",
-  "daily",
-  "damn",
-  "damp",
-  "dandy",
-  "dangerous",
-  "dank",
-  "dark",
-  "dated",
-  "dazed",
-  "dead",
-  "deaf",
-  "dear",
-  "deep",
-  "defeated",
-  "defiant",
-  "deft",
-  "deist",
-  "delightful",
-  "dense",
-  "depressed",
-  "determined",
-  "dewy",
-  "dicey",
-  "different",
-  "difficult",
-  "dim",
-  "dingy",
-  "dinky",
-  "dire",
-  "dirt",
-  "dirty",
-  "disgusted",
-  "distinct",
-  "disturbed",
-  "dizzy",
-  "dodgy",
-  "domed",
-  "done",
-  "doped",
-  "dopey",
-  "dopy",
-  "dormy",
-  "dosed",
-  "doubtful",
-  "down",
-  "downy",
-  "dozen",
-  "drab",
-  "drawn",
-  "dread",
-  "drear",
-  "dress",
-  "dried",
-  "droll",
-  "drunk",
-  "dry",
-  "dual",
-  "dud",
-  "due",
-  "dull",
-  "dumb",
-  "dummy",
-  "dusky",
-  "dusty",
-  "dyed",
-  "dying",
-  "each",
-  "eager",
-  "early",
-  "eased",
-  "east",
-  "easy",
-  "edged",
-  "edgy",
-  "eerie",
-  "eight",
-  "elated",
-  "elder",
-  "elect",
-  "elegant",
-  "elfin",
-  "elite",
-  "embarrassed",
-  "empty",
-  "enchanting",
-  "encouraging",
-  "ended",
-  "energetic",
-  "enthusiastic",
-  "envious",
-  "epic",
-  "equal",
-  "even",
-  "every",
-  "evil",
-  "exact",
-  "excited",
-  "expensive",
-  "extra",
-  "exuberant",
-  "eyed",
-  "fab",
-  "faced",
-  "faded",
-  "faint",
-  "fair",
-  "faithful",
-  "fake",
-  "false",
-  "famed",
-  "famous",
-  "fancy",
-  "fantastic",
-  "far",
-  "fast",
-  "fat",
-  "fatal",
-  "fated",
-  "fazed",
-  "feral",
-  "few",
-  "fewer",
-  "fierce",
-  "fiery",
-  "fifth",
-  "fifty",
-  "filmy",
-  "filthy",
-  "final",
-  "fine",
-  "finer",
-  "fired",
-  "firm",
-  "first",
-  "fishy",
-  "fit",
-  "five",
-  "fixed",
-  "fizzy",
-  "flaky",
-  "flash",
-  "flat",
-  "fleet",
-  "flint",
-  "flip",
-  "fluid",
-  "flush",
-  "fly",
-  "foamy",
-  "focal",
-  "foggy",
-  "fond",
-  "foolish",
-  "fore",
-  "foul",
-  "found",
-  "four",
-  "foxy",
-  "fragile",
-  "frail",
-  "frank",
-  "frantic",
-  "free",
-  "fresh",
-  "fried",
-  "friendly",
-  "frightened",
-  "front",
-  "full",
-  "fumed",
-  "funky",
-  "funny",
-  "furry",
-  "fused",
-  "fussy",
-  "fuzzy",
-  "game",
-  "gaudy",
-  "gaunt",
-  "gawky",
-  "gentle",
-  "giant",
-  "giddy",
-  "gifted",
-  "gimpy",
-  "glad",
-  "glamorous",
-  "gleaming",
-  "glorious",
-  "glum",
-  "godly",
-  "going",
-  "gold",
-  "gone",
-  "good",
-  "gooey",
-  "goofy",
-  "gorgeous",
-  "gory",
-  "graceful",
-  "grand",
-  "great",
-  "green",
-  "grey",
-  "grieving",
-  "grim",
-  "grimy",
-  "gross",
-  "grotesque",
-  "grown",
-  "gruff",
-  "grumpy",
-  "gummy",
-  "gushy",
-  "gusty",
-  "gutsy",
-  "hairy",
-  "hale",
-  "half",
-  "halt",
-  "hammy",
-  "handsome",
-  "handy",
-  "happy",
-  "hard",
-  "hardy",
-  "harsh",
-  "hasty",
-  "hated",
-  "hazel",
-  "hazy",
-  "healthy",
-  "heard",
-  "heavy",
-  "hefty",
-  "held",
-  "helpful",
-  "helpless",
-  "here",
-  "hex",
-  "hexed",
-  "high",
-  "hilarious",
-  "hilly",
-  "hind",
-  "hip",
-  "hired",
-  "hoar",
-  "hoary",
-  "hokey",
-  "holey",
-  "holy",
-  "home",
-  "homeless",
-  "homely",
-  "homey",
-  "honey",
-  "horny",
-  "horrible",
-  "hot",
-  "huffy",
-  "huge",
-  "human",
-  "humid",
-  "hungry",
-  "hurt",
-  "husky",
-  "icky",
-  "icy",
-  "ideal",
-  "idle",
-  "iffy",
-  "ill",
-  "important",
-  "impossible",
-  "inane",
-  "inept",
-  "inert",
-  "inexpensive",
-  "inky",
-  "inner",
-  "innocent",
-  "inquisitive",
-  "ionic",
-  "irate",
-  "iron",
-  "itchy",
-  "jade",
-  "jaded",
-  "jaggy",
-  "jawed",
-  "jazzy",
-  "jealous",
-  "jet",
-  "jittery",
-  "joint",
-  "jolly",
-  "jowly",
-  "joyous",
-  "juicy",
-  "jumbo",
-  "jumpy",
-  "just",
-  "kempt",
-  "key",
-  "keyed",
-  "khaki",
-  "kin",
-  "kind",
-  "kinky",
-  "known",
-  "kooky",
-  "laced",
-  "lacy",
-  "laid",
-  "lame",
-  "lank",
-  "lanky",
-  "large",
-  "last",
-  "late",
-  "later",
-  "lax",
-  "lay",
-  "lazy",
-  "leafy",
-  "leaky",
-  "lean",
-  "least",
-  "left",
-  "legal",
-  "less",
-  "level",
-  "light",
-  "like",
-  "liked",
-  "limp",
-  "lined",
-  "lit",
-  "live",
-  "lively",
-  "liver",
-  "livid",
-  "loamy",
-  "local",
-  "loco",
-  "lofty",
-  "lone",
-  "lonely",
-  "long",
-  "loony",
-  "loopy",
-  "loose",
-  "lossy",
-  "lost",
-  "loud",
-  "lousy",
-  "loved",
-  "lovely",
-  "low",
-  "lowly",
-  "loyal",
-  "lucid",
-  "lucky",
-  "lumpy",
-  "lunar",
-  "lurid",
-  "lush",
-  "lusty",
-  "lyric",
-  "macho",
-  "macro",
-  "mad",
-  "made",
-  "magic",
-  "magnificent",
-  "main",
-  "major",
-  "male",
-  "mangy",
-  "manic",
-  "manly",
-  "many",
-  "mass",
-  "matt",
-  "matte",
-  "mauve",
-  "mealy",
-  "mean",
-  "meaty",
-  "meek",
-  "meet",
-  "mere",
-  "merry",
-  "messy",
-  "metal",
-  "micro",
-  "mild",
-  "milky",
-  "mimic",
-  "mined",
-  "mini",
-  "minor",
-  "mint",
-  "minty",
-  "minus",
-  "mired",
-  "mirky",
-  "misty",
-  "mixed",
-  "mock",
-  "mod",
-  "modal",
-  "model",
-  "modern",
-  "moist",
-  "molar",
-  "moldy",
-  "mono",
-  "moody",
-  "moony",
-  "moot",
-  "moral",
-  "more",
-  "mossy",
-  "most",
-  "mothy",
-  "motionless",
-  "motor",
-  "mousy",
-  "moved",
-  "mown",
-  "much",
-  "mucky",
-  "muddy",
-  "muggy",
-  "mum",
-  "mural",
-  "murky",
-  "mushy",
-  "musky",
-  "must",
-  "musty",
-  "mute",
-  "muted",
-  "mysterious",
-  "naive",
-  "nary",
-  "nasal",
-  "nasty",
-  "natal",
-  "natty",
-  "naughty",
-  "naval",
-  "near",
-  "neat",
-  "needy",
-  "nervous",
-  "nervy",
-  "new",
-  "newsy",
-  "next",
-  "nice",
-  "nifty",
-  "nigh",
-  "nine",
-  "ninth",
-  "noble",
-  "noisy",
-  "none",
-  "north",
-  "nosed",
-  "noted",
-  "novel",
-  "nubby",
-  "numb",
-  "nuts",
-  "nutty",
-  "oaken",
-  "oaten",
-  "obedient",
-  "obese",
-  "obnoxious",
-  "odd",
-  "oiled",
-  "oily",
-  "okay",
-  "old",
-  "old-fashioned",
-  "olden",
-  "older",
-  "olive",
-  "one",
-  "only",
-  "oozy",
-  "open",
-  "optic",
-  "oral",
-  "other",
-  "out",
-  "outer",
-  "outrageous",
-  "outstanding",
-  "oval",
-  "over",
-  "overt",
-  "owing",
-  "own",
-  "owned",
-  "pagan",
-  "paid",
-  "pale",
-  "palmy",
-  "panicky",
-  "pass",
-  "past",
-  "pasty",
-  "pat",
-  "paved",
-  "peaky",
-  "peaty",
-  "pedal",
-  "pent",
-  "peppy",
-  "perfect",
-  "perky",
-  "pert",
-  "pesky",
-  "pet",
-  "petty",
-  "phony",
-  "piano",
-  "picky",
-  "pied",
-  "piggy",
-  "pilar",
-  "pink",
-  "plain",
-  "plane",
-  "pleasant",
-  "plumb",
-  "plump",
-  "plus",
-  "plush",
-  "poised",
-  "polar",
-  "poor",
-  "pop",
-  "port",
-  "posed",
-  "posh",
-  "potty",
-  "powerful",
-  "precious",
-  "prickly",
-  "pricy",
-  "prim",
-  "prior",
-  "privy",
-  "prize",
-  "prone",
-  "proof",
-  "prosy",
-  "proud",
-  "pubic",
-  "pudgy",
-  "puff",
-  "puffy",
-  "pulpy",
-  "punk",
-  "puny",
-  "pupal",
-  "pure",
-  "pushy",
-  "putrid",
-  "puzzled",
-  "quack",
-  "quaint",
-  "quasi",
-  "quick",
-  "quiet",
-  "rabid",
-  "radio",
-  "rainy",
-  "rapid",
-  "rare",
-  "rash",
-  "raspy",
-  "ratty",
-  "raw",
-  "ready",
-  "real",
-  "rear",
-  "red",
-  "regal",
-  "relieved",
-  "repulsive",
-  "retro",
-  "rich",
-  "rife",
-  "right",
-  "rigid",
-  "riled",
-  "ripe",
-  "risen",
-  "risky",
-  "ritzy",
-  "roast",
-  "robed",
-  "rocky",
-  "roomy",
-  "ropey",
-  "rose",
-  "rosy",
-  "rough",
-  "round",
-  "rowdy",
-  "royal",
-  "ruby",
-  "rude",
-  "ruled",
-  "rum",
-  "rummy",
-  "runic",
-  "runny",
-  "runty",
-  "rural",
-  "rush",
-  "rushy",
-  "rust",
-  "rusty",
-  "rutty",
-  "sad",
-  "safe",
-  "sage",
-  "said",
-  "salt",
-  "salty",
-  "same",
-  "sandy",
-  "sane",
-  "sappy",
-  "sassy",
-  "saute",
-  "saved",
-  "scaly",
-  "scant",
-  "scary",
-  "scrub",
-  "seamy",
-  "sear",
-  "seedy",
-  "self",
-  "selfish",
-  "sent",
-  "seven",
-  "sewed",
-  "sewn",
-  "shady",
-  "shaky",
-  "sham",
-  "sharp",
-  "shed",
-  "sheer",
-  "shiny",
-  "short",
-  "shot",
-  "showy",
-  "shut",
-  "shy",
-  "sick",
-  "side",
-  "sign",
-  "silky",
-  "silly",
-  "silty",
-  "sissy",
-  "six",
-  "sixth",
-  "sixty",
-  "size",
-  "sized",
-  "skew",
-  "skim",
-  "slack",
-  "slain",
-  "slaty",
-  "slav",
-  "sleek",
-  "sleepy",
-  "slick",
-  "slim",
-  "slimy",
-  "slow",
-  "sly",
-  "small",
-  "smart",
-  "smiling",
-  "smoggy",
-  "smoky",
-  "smug",
-  "snaky",
-  "sneak",
-  "snide",
-  "snowy",
-  "snub",
-  "snuff",
-  "snug",
-  "soapy",
-  "sober",
-  "soft",
-  "soggy",
-  "solar",
-  "sold",
-  "sole",
-  "solid",
-  "solo",
-  "some",
-  "sooty",
-  "sore",
-  "sorry",
-  "sound",
-  "soupy",
-  "sour",
-  "south",
-  "sown",
-  "spare",
-  "sparkling",
-  "spent",
-  "spicy",
-  "spiky",
-  "spiny",
-  "splay",
-  "splendid",
-  "split",
-  "spotless",
-  "spry",
-  "spumy",
-  "squab",
-  "squat",
-  "stagy",
-  "stale",
-  "star",
-  "stark",
-  "steep",
-  "stern",
-  "stiff",
-  "still",
-  "stock",
-  "stone",
-  "stony",
-  "stormy",
-  "stout",
-  "strange",
-  "straw",
-  "stray",
-  "stuck",
-  "stung",
-  "stupid",
-  "suave",
-  "successful",
-  "such",
-  "sudsy",
-  "sulky",
-  "sunk",
-  "sunny",
-  "super",
-  "sure",
-  "surly",
-  "sweet",
-  "swell",
-  "swept",
-  "swift",
-  "swish",
-  "sworn",
-  "tabby",
-  "taboo",
-  "tacky",
-  "taken",
-  "talented",
-  "talky",
-  "tall",
-  "tame",
-  "tamed",
-  "tan",
-  "tangy",
-  "taped",
-  "tardy",
-  "tart",
-  "tasty",
-  "tawny",
-  "teal",
-  "ten",
-  "tender",
-  "tenor",
-  "tense",
-  "tenth",
-  "tepid",
-  "terrible",
-  "terse",
-  "testy",
-  "thankful",
-  "thick",
-  "thin",
-  "third",
-  "thoughtful",
-  "thoughtless",
-  "three",
-  "tidal",
-  "tidy",
-  "tied",
-  "tight",
-  "tiled",
-  "timed",
-  "timid",
-  "tinny",
-  "tiny",
-  "tipsy",
-  "tired",
-  "toed",
-  "token",
-  "tonal",
-  "toned",
-  "tonic",
-  "top",
-  "tops",
-  "torn",
-  "total",
-  "tough",
-  "toxic",
-  "tried",
-  "trig",
-  "trim",
-  "trite",
-  "troubled",
-  "true",
-  "tubby",
-  "tubed",
-  "tumid",
-  "twee",
-  "twin",
-  "two",
-  "ugliest",
-  "ugly",
-  "ultra",
-  "uncut",
-  "under",
-  "undue",
-  "unfed",
-  "unfit",
-  "uninterested",
-  "union",
-  "unlit",
-  "unsightly",
-  "unusual",
-  "unwed",
-  "upper",
-  "upset",
-  "uptight",
-  "urban",
-  "used",
-  "usual",
-  "utter",
-  "vague",
-  "vain",
-  "valid",
-  "vapid",
-  "vast",
-  "victorious",
-  "viral",
-  "vital",
-  "vivacious",
-  "vivid",
-  "vocal",
-  "void",
-  "wacky",
-  "wandering",
-  "warm",
-  "wary",
-  "washy",
-  "waste",
-  "wavy",
-  "waxed",
-  "waxen",
-  "waxy",
-  "weak",
-  "weary",
-  "weedy",
-  "weeny",
-  "weepy",
-  "weird",
-  "well",
-  "welsh",
-  "west",
-  "wet",
-  "whiny",
-  "white",
-  "whole",
-  "wicked",
-  "wide",
-  "wide-eyed",
-  "wild",
-  "wily",
-  "wimpy",
-  "windy",
-  "wired",
-  "wiry",
-  "wise",
-  "wispy",
-  "witty",
-  "wonky",
-  "woody",
-  "wooly",
-  "woozy",
-  "wordy",
-  "wormy",
-  "worn",
-  "worried",
-  "worrisome",
-  "worse",
-  "worst",
-  "worth",
-  "wound",
-  "woven",
-  "wrong",
-  "wroth",
-  "wry",
-  "young",
-  "yucky",
-  "yummy",
-  "zany",
-  "zealous",
-  "zero",
-  "zesty",
-  "zippy",
-  "zonal",
+  'able',
+  'about',
+  'above',
+  'abuzz',
+  'ace',
+  'achy',
+  'acid',
+  'acned',
+  'acute',
+  'adept',
+  'adorable',
+  'adult',
+  'adventurous',
+  'afire',
+  'afoot',
+  'afoul',
+  'aft',
+  'after',
+  'aged',
+  'aggressive',
+  'agile',
+  'aging',
+  'aglow',
+  'ago',
+  'agreeable',
+  'ahead',
+  'aided',
+  'airy',
+  'ajar',
+  'akin',
+  'alert',
+  'alien',
+  'alike',
+  'alive',
+  'all',
+  'alone',
+  'aloof',
+  'alpha',
+  'alto',
+  'amber',
+  'ample',
+  'amused',
+  'angry',
+  'annoyed',
+  'annoying',
+  'anti',
+  'antic',
+  'antsy',
+  'anxious',
+  'any',
+  'apart',
+  'apish',
+  'apt',
+  'arced',
+  'arch',
+  'arid',
+  'armed',
+  'arrogant',
+  'ashamed',
+  'ashen',
+  'ashy',
+  'askew',
+  'astir',
+  'atrip',
+  'attic',
+  'attractive',
+  'average',
+  'avian',
+  'avid',
+  'awake',
+  'aware',
+  'awash',
+  'away',
+  'awed',
+  'awful',
+  'awing',
+  'awned',
+  'awry',
+  'axial',
+  'azure',
+  'back',
+  'bad',
+  'baggy',
+  'baked',
+  'bald',
+  'balmy',
+  'bandy',
+  'bare',
+  'bared',
+  'basal',
+  'base',
+  'based',
+  'basic',
+  'bated',
+  'bats',
+  'batty',
+  'bay',
+  'beady',
+  'beamy',
+  'beat',
+  'beautiful',
+  'beefy',
+  'beery',
+  'beige',
+  'bent',
+  'best',
+  'beta',
+  'better',
+  'bewildered',
+  'bias',
+  'birch',
+  'bitty',
+  'black',
+  'blame',
+  'bland',
+  'blank',
+  'bleak',
+  'blear',
+  'blind',
+  'blond',
+  'bloody',
+  'blown',
+  'blue',
+  'blue-eyed',
+  'bluff',
+  'blunt',
+  'blushing',
+  'boggy',
+  'bogus',
+  'bold',
+  'bone',
+  'boned',
+  'bonny',
+  'bony',
+  'boon',
+  'boozy',
+  'bored',
+  'born',
+  'boss',
+  'bossy',
+  'both',
+  'bound',
+  'bowed',
+  'boxed',
+  'boxy',
+  'brag',
+  'brainy',
+  'brash',
+  'brave',
+  'breakable',
+  'brief',
+  'bright',
+  'briny',
+  'brisk',
+  'broad',
+  'broke',
+  'brown',
+  'brute',
+  'buff',
+  'buggy',
+  'built',
+  'bulgy',
+  'bulky',
+  'bully',
+  'bum',
+  'bumpy',
+  'burly',
+  'burnt',
+  'bush',
+  'bushy',
+  'bust',
+  'busty',
+  'busy',
+  'butch',
+  'calm',
+  'camp',
+  'campy',
+  'careful',
+  'catty',
+  'cautious',
+  'charming',
+  'cheap',
+  'cheerful',
+  'chewy',
+  'chic',
+  'chief',
+  'civic',
+  'civil',
+  'clean',
+  'clear',
+  'cleft',
+  'clever',
+  'close',
+  'cloudy',
+  'clumsy',
+  'cocky',
+  'cod',
+  'cold',
+  'color',
+  'colorful',
+  'combative',
+  'comfortable',
+  'comfy',
+  'comic',
+  'concerned',
+  'condemned',
+  'confused',
+  'cool',
+  'cooperative',
+  'coral',
+  'corny',
+  'cosy',
+  'courageous',
+  'coy',
+  'cozy',
+  'crazy',
+  'creepy',
+  'crisp',
+  'cross',
+  'crowded',
+  'cruel',
+  'cubic',
+  'cured',
+  'curious',
+  'curly',
+  'curt',
+  'curvy',
+  'cushy',
+  'cut',
+  'cute',
+  'cyan',
+  'daft',
+  'daily',
+  'damn',
+  'damp',
+  'dandy',
+  'dangerous',
+  'dank',
+  'dark',
+  'dated',
+  'dazed',
+  'dead',
+  'deaf',
+  'dear',
+  'deep',
+  'defeated',
+  'defiant',
+  'deft',
+  'deist',
+  'delightful',
+  'dense',
+  'depressed',
+  'determined',
+  'dewy',
+  'dicey',
+  'different',
+  'difficult',
+  'dim',
+  'dingy',
+  'dinky',
+  'dire',
+  'dirt',
+  'dirty',
+  'disgusted',
+  'distinct',
+  'disturbed',
+  'dizzy',
+  'dodgy',
+  'domed',
+  'done',
+  'doped',
+  'dopey',
+  'dopy',
+  'dormy',
+  'dosed',
+  'doubtful',
+  'down',
+  'downy',
+  'dozen',
+  'drab',
+  'drawn',
+  'dread',
+  'drear',
+  'dress',
+  'dried',
+  'droll',
+  'drunk',
+  'dry',
+  'dual',
+  'dud',
+  'due',
+  'dull',
+  'dumb',
+  'dummy',
+  'dusky',
+  'dusty',
+  'dyed',
+  'dying',
+  'each',
+  'eager',
+  'early',
+  'eased',
+  'east',
+  'easy',
+  'edged',
+  'edgy',
+  'eerie',
+  'eight',
+  'elated',
+  'elder',
+  'elect',
+  'elegant',
+  'elfin',
+  'elite',
+  'embarrassed',
+  'empty',
+  'enchanting',
+  'encouraging',
+  'ended',
+  'energetic',
+  'enthusiastic',
+  'envious',
+  'epic',
+  'equal',
+  'even',
+  'every',
+  'evil',
+  'exact',
+  'excited',
+  'expensive',
+  'extra',
+  'exuberant',
+  'eyed',
+  'fab',
+  'faced',
+  'faded',
+  'faint',
+  'fair',
+  'faithful',
+  'fake',
+  'false',
+  'famed',
+  'famous',
+  'fancy',
+  'fantastic',
+  'far',
+  'fast',
+  'fat',
+  'fatal',
+  'fated',
+  'fazed',
+  'feral',
+  'few',
+  'fewer',
+  'fierce',
+  'fiery',
+  'fifth',
+  'fifty',
+  'filmy',
+  'filthy',
+  'final',
+  'fine',
+  'finer',
+  'fired',
+  'firm',
+  'first',
+  'fishy',
+  'fit',
+  'five',
+  'fixed',
+  'fizzy',
+  'flaky',
+  'flash',
+  'flat',
+  'fleet',
+  'flint',
+  'flip',
+  'fluid',
+  'flush',
+  'fly',
+  'foamy',
+  'focal',
+  'foggy',
+  'fond',
+  'foolish',
+  'fore',
+  'foul',
+  'found',
+  'four',
+  'foxy',
+  'fragile',
+  'frail',
+  'frank',
+  'frantic',
+  'free',
+  'fresh',
+  'fried',
+  'friendly',
+  'frightened',
+  'front',
+  'full',
+  'fumed',
+  'funky',
+  'funny',
+  'furry',
+  'fused',
+  'fussy',
+  'fuzzy',
+  'game',
+  'gaudy',
+  'gaunt',
+  'gawky',
+  'gentle',
+  'giant',
+  'giddy',
+  'gifted',
+  'gimpy',
+  'glad',
+  'glamorous',
+  'gleaming',
+  'glorious',
+  'glum',
+  'godly',
+  'going',
+  'gold',
+  'gone',
+  'good',
+  'gooey',
+  'goofy',
+  'gorgeous',
+  'gory',
+  'graceful',
+  'grand',
+  'great',
+  'green',
+  'grey',
+  'grieving',
+  'grim',
+  'grimy',
+  'gross',
+  'grotesque',
+  'grown',
+  'gruff',
+  'grumpy',
+  'gummy',
+  'gushy',
+  'gusty',
+  'gutsy',
+  'hairy',
+  'hale',
+  'half',
+  'halt',
+  'hammy',
+  'handsome',
+  'handy',
+  'happy',
+  'hard',
+  'hardy',
+  'harsh',
+  'hasty',
+  'hated',
+  'hazel',
+  'hazy',
+  'healthy',
+  'heard',
+  'heavy',
+  'hefty',
+  'held',
+  'helpful',
+  'helpless',
+  'here',
+  'hex',
+  'hexed',
+  'high',
+  'hilarious',
+  'hilly',
+  'hind',
+  'hip',
+  'hired',
+  'hoar',
+  'hoary',
+  'hokey',
+  'holey',
+  'holy',
+  'home',
+  'homeless',
+  'homely',
+  'homey',
+  'honey',
+  'horny',
+  'horrible',
+  'hot',
+  'huffy',
+  'huge',
+  'human',
+  'humid',
+  'hungry',
+  'hurt',
+  'husky',
+  'icky',
+  'icy',
+  'ideal',
+  'idle',
+  'iffy',
+  'ill',
+  'important',
+  'impossible',
+  'inane',
+  'inept',
+  'inert',
+  'inexpensive',
+  'inky',
+  'inner',
+  'innocent',
+  'inquisitive',
+  'ionic',
+  'irate',
+  'iron',
+  'itchy',
+  'jade',
+  'jaded',
+  'jaggy',
+  'jawed',
+  'jazzy',
+  'jealous',
+  'jet',
+  'jittery',
+  'joint',
+  'jolly',
+  'jowly',
+  'joyous',
+  'juicy',
+  'jumbo',
+  'jumpy',
+  'just',
+  'kempt',
+  'key',
+  'keyed',
+  'khaki',
+  'kin',
+  'kind',
+  'kinky',
+  'known',
+  'kooky',
+  'laced',
+  'lacy',
+  'laid',
+  'lame',
+  'lank',
+  'lanky',
+  'large',
+  'last',
+  'late',
+  'later',
+  'lax',
+  'lay',
+  'lazy',
+  'leafy',
+  'leaky',
+  'lean',
+  'least',
+  'left',
+  'legal',
+  'less',
+  'level',
+  'light',
+  'like',
+  'liked',
+  'limp',
+  'lined',
+  'lit',
+  'live',
+  'lively',
+  'liver',
+  'livid',
+  'loamy',
+  'local',
+  'loco',
+  'lofty',
+  'lone',
+  'lonely',
+  'long',
+  'loony',
+  'loopy',
+  'loose',
+  'lossy',
+  'lost',
+  'loud',
+  'lousy',
+  'loved',
+  'lovely',
+  'low',
+  'lowly',
+  'loyal',
+  'lucid',
+  'lucky',
+  'lumpy',
+  'lunar',
+  'lurid',
+  'lush',
+  'lusty',
+  'lyric',
+  'macho',
+  'macro',
+  'mad',
+  'made',
+  'magic',
+  'magnificent',
+  'main',
+  'major',
+  'male',
+  'mangy',
+  'manic',
+  'manly',
+  'many',
+  'mass',
+  'matt',
+  'matte',
+  'mauve',
+  'mealy',
+  'mean',
+  'meaty',
+  'meek',
+  'meet',
+  'mere',
+  'merry',
+  'messy',
+  'metal',
+  'micro',
+  'mild',
+  'milky',
+  'mimic',
+  'mined',
+  'mini',
+  'minor',
+  'mint',
+  'minty',
+  'minus',
+  'mired',
+  'mirky',
+  'misty',
+  'mixed',
+  'mock',
+  'mod',
+  'modal',
+  'model',
+  'modern',
+  'moist',
+  'molar',
+  'moldy',
+  'mono',
+  'moody',
+  'moony',
+  'moot',
+  'moral',
+  'more',
+  'mossy',
+  'most',
+  'mothy',
+  'motionless',
+  'motor',
+  'mousy',
+  'moved',
+  'mown',
+  'much',
+  'mucky',
+  'muddy',
+  'muggy',
+  'mum',
+  'mural',
+  'murky',
+  'mushy',
+  'musky',
+  'must',
+  'musty',
+  'mute',
+  'muted',
+  'mysterious',
+  'naive',
+  'nary',
+  'nasal',
+  'nasty',
+  'natal',
+  'natty',
+  'naughty',
+  'naval',
+  'near',
+  'neat',
+  'needy',
+  'nervous',
+  'nervy',
+  'new',
+  'newsy',
+  'next',
+  'nice',
+  'nifty',
+  'nigh',
+  'nine',
+  'ninth',
+  'noble',
+  'noisy',
+  'none',
+  'north',
+  'nosed',
+  'noted',
+  'novel',
+  'nubby',
+  'numb',
+  'nuts',
+  'nutty',
+  'oaken',
+  'oaten',
+  'obedient',
+  'obese',
+  'obnoxious',
+  'odd',
+  'oiled',
+  'oily',
+  'okay',
+  'old',
+  'old-fashioned',
+  'olden',
+  'older',
+  'olive',
+  'one',
+  'only',
+  'oozy',
+  'open',
+  'optic',
+  'oral',
+  'other',
+  'out',
+  'outer',
+  'outrageous',
+  'outstanding',
+  'oval',
+  'over',
+  'overt',
+  'owing',
+  'own',
+  'owned',
+  'pagan',
+  'paid',
+  'pale',
+  'palmy',
+  'panicky',
+  'pass',
+  'past',
+  'pasty',
+  'pat',
+  'paved',
+  'peaky',
+  'peaty',
+  'pedal',
+  'pent',
+  'peppy',
+  'perfect',
+  'perky',
+  'pert',
+  'pesky',
+  'pet',
+  'petty',
+  'phony',
+  'piano',
+  'picky',
+  'pied',
+  'piggy',
+  'pilar',
+  'pink',
+  'plain',
+  'plane',
+  'pleasant',
+  'plumb',
+  'plump',
+  'plus',
+  'plush',
+  'poised',
+  'polar',
+  'poor',
+  'pop',
+  'port',
+  'posed',
+  'posh',
+  'potty',
+  'powerful',
+  'precious',
+  'prickly',
+  'pricy',
+  'prim',
+  'prior',
+  'privy',
+  'prize',
+  'prone',
+  'proof',
+  'prosy',
+  'proud',
+  'pubic',
+  'pudgy',
+  'puff',
+  'puffy',
+  'pulpy',
+  'punk',
+  'puny',
+  'pupal',
+  'pure',
+  'pushy',
+  'putrid',
+  'puzzled',
+  'quack',
+  'quaint',
+  'quasi',
+  'quick',
+  'quiet',
+  'rabid',
+  'radio',
+  'rainy',
+  'rapid',
+  'rare',
+  'rash',
+  'raspy',
+  'ratty',
+  'raw',
+  'ready',
+  'real',
+  'rear',
+  'red',
+  'regal',
+  'relieved',
+  'repulsive',
+  'retro',
+  'rich',
+  'rife',
+  'right',
+  'rigid',
+  'riled',
+  'ripe',
+  'risen',
+  'risky',
+  'ritzy',
+  'roast',
+  'robed',
+  'rocky',
+  'roomy',
+  'ropey',
+  'rose',
+  'rosy',
+  'rough',
+  'round',
+  'rowdy',
+  'royal',
+  'ruby',
+  'rude',
+  'ruled',
+  'rum',
+  'rummy',
+  'runic',
+  'runny',
+  'runty',
+  'rural',
+  'rush',
+  'rushy',
+  'rust',
+  'rusty',
+  'rutty',
+  'sad',
+  'safe',
+  'sage',
+  'said',
+  'salt',
+  'salty',
+  'same',
+  'sandy',
+  'sane',
+  'sappy',
+  'sassy',
+  'saute',
+  'saved',
+  'scaly',
+  'scant',
+  'scary',
+  'scrub',
+  'seamy',
+  'sear',
+  'seedy',
+  'self',
+  'selfish',
+  'sent',
+  'seven',
+  'sewed',
+  'sewn',
+  'shady',
+  'shaky',
+  'sham',
+  'sharp',
+  'shed',
+  'sheer',
+  'shiny',
+  'short',
+  'shot',
+  'showy',
+  'shut',
+  'shy',
+  'sick',
+  'side',
+  'sign',
+  'silky',
+  'silly',
+  'silty',
+  'sissy',
+  'six',
+  'sixth',
+  'sixty',
+  'size',
+  'sized',
+  'skew',
+  'skim',
+  'slack',
+  'slain',
+  'slaty',
+  'slav',
+  'sleek',
+  'sleepy',
+  'slick',
+  'slim',
+  'slimy',
+  'slow',
+  'sly',
+  'small',
+  'smart',
+  'smiling',
+  'smoggy',
+  'smoky',
+  'smug',
+  'snaky',
+  'sneak',
+  'snide',
+  'snowy',
+  'snub',
+  'snuff',
+  'snug',
+  'soapy',
+  'sober',
+  'soft',
+  'soggy',
+  'solar',
+  'sold',
+  'sole',
+  'solid',
+  'solo',
+  'some',
+  'sooty',
+  'sore',
+  'sorry',
+  'sound',
+  'soupy',
+  'sour',
+  'south',
+  'sown',
+  'spare',
+  'sparkling',
+  'spent',
+  'spicy',
+  'spiky',
+  'spiny',
+  'splay',
+  'splendid',
+  'split',
+  'spotless',
+  'spry',
+  'spumy',
+  'squab',
+  'squat',
+  'stagy',
+  'stale',
+  'star',
+  'stark',
+  'steep',
+  'stern',
+  'stiff',
+  'still',
+  'stock',
+  'stone',
+  'stony',
+  'stormy',
+  'stout',
+  'strange',
+  'straw',
+  'stray',
+  'stuck',
+  'stung',
+  'stupid',
+  'suave',
+  'successful',
+  'such',
+  'sudsy',
+  'sulky',
+  'sunk',
+  'sunny',
+  'super',
+  'sure',
+  'surly',
+  'sweet',
+  'swell',
+  'swept',
+  'swift',
+  'swish',
+  'sworn',
+  'tabby',
+  'taboo',
+  'tacky',
+  'taken',
+  'talented',
+  'talky',
+  'tall',
+  'tame',
+  'tamed',
+  'tan',
+  'tangy',
+  'taped',
+  'tardy',
+  'tart',
+  'tasty',
+  'tawny',
+  'teal',
+  'ten',
+  'tender',
+  'tenor',
+  'tense',
+  'tenth',
+  'tepid',
+  'terrible',
+  'terse',
+  'testy',
+  'thankful',
+  'thick',
+  'thin',
+  'third',
+  'thoughtful',
+  'thoughtless',
+  'three',
+  'tidal',
+  'tidy',
+  'tied',
+  'tight',
+  'tiled',
+  'timed',
+  'timid',
+  'tinny',
+  'tiny',
+  'tipsy',
+  'tired',
+  'toed',
+  'token',
+  'tonal',
+  'toned',
+  'tonic',
+  'top',
+  'tops',
+  'torn',
+  'total',
+  'tough',
+  'toxic',
+  'tried',
+  'trig',
+  'trim',
+  'trite',
+  'troubled',
+  'true',
+  'tubby',
+  'tubed',
+  'tumid',
+  'twee',
+  'twin',
+  'two',
+  'ugliest',
+  'ugly',
+  'ultra',
+  'uncut',
+  'under',
+  'undue',
+  'unfed',
+  'unfit',
+  'uninterested',
+  'union',
+  'unlit',
+  'unsightly',
+  'unusual',
+  'unwed',
+  'upper',
+  'upset',
+  'uptight',
+  'urban',
+  'used',
+  'usual',
+  'utter',
+  'vague',
+  'vain',
+  'valid',
+  'vapid',
+  'vast',
+  'victorious',
+  'viral',
+  'vital',
+  'vivacious',
+  'vivid',
+  'vocal',
+  'void',
+  'wacky',
+  'wandering',
+  'warm',
+  'wary',
+  'washy',
+  'waste',
+  'wavy',
+  'waxed',
+  'waxen',
+  'waxy',
+  'weak',
+  'weary',
+  'weedy',
+  'weeny',
+  'weepy',
+  'weird',
+  'well',
+  'welsh',
+  'west',
+  'wet',
+  'whiny',
+  'white',
+  'whole',
+  'wicked',
+  'wide',
+  'wide-eyed',
+  'wild',
+  'wily',
+  'wimpy',
+  'windy',
+  'wired',
+  'wiry',
+  'wise',
+  'wispy',
+  'witty',
+  'wonky',
+  'woody',
+  'wooly',
+  'woozy',
+  'wordy',
+  'wormy',
+  'worn',
+  'worried',
+  'worrisome',
+  'worse',
+  'worst',
+  'worth',
+  'wound',
+  'woven',
+  'wrong',
+  'wroth',
+  'wry',
+  'young',
+  'yucky',
+  'yummy',
+  'zany',
+  'zealous',
+  'zero',
+  'zesty',
+  'zippy',
+  'zonal',
 ]
 
 nouns = [
-  "ace",
-  "ache",
-  "acid",
-  "acme",
-  "acorn",
-  "acre",
-  "act",
-  "actor",
-  "add",
-  "adder",
-  "adept",
-  "advil",
-  "afro",
-  "agave",
-  "age",
-  "aged",
-  "agent",
-  "agony",
-  "ailey",
-  "aim",
-  "aioli",
-  "air",
-  "aisle",
-  "akron",
-  "alarm",
-  "album",
-  "ale",
-  "alert",
-  "algae",
-  "alias",
-  "alibi",
-  "alien",
-  "alley",
-  "alloy",
-  "ally",
-  "aloe",
-  "alpha",
-  "alps",
-  "altar",
-  "amber",
-  "amigo",
-  "amino",
-  "amish",
-  "ammo",
-  "amp",
-  "angel",
-  "anger",
-  "angle",
-  "angst",
-  "angus",
-  "anime",
-  "ankle",
-  "annex",
-  "anole",
-  "ant",
-  "ante",
-  "antic",
-  "anvil",
-  "ape",
-  "apex",
-  "aphid",
-  "apple",
-  "april",
-  "apron",
-  "aqua",
-  "arbor",
-  "arc",
-  "arch",
-  "area",
-  "arena",
-  "argon",
-  "argus",
-  "ark",
-  "arm",
-  "armor",
-  "arms",
-  "army",
-  "aroma",
-  "array",
-  "arrow",
-  "arson",
-  "art",
-  "ascot",
-  "aspen",
-  "asset",
-  "ate",
-  "atom",
-  "attic",
-  "audio",
-  "audit",
-  "auger",
-  "aunt",
-  "aunty",
-  "aura",
-  "auto",
-  "award",
-  "awe",
-  "awl",
-  "axe",
-  "axiom",
-  "axis",
-  "axle",
-  "azure",
-  "baby",
-  "back",
-  "bacon",
-  "bad",
-  "badge",
-  "bag",
-  "bagel",
-  "bail",
-  "bait",
-  "baker",
-  "bale",
-  "balk",
-  "ball",
-  "balm",
-  "ban",
-  "band",
-  "bane",
-  "banjo",
-  "bank",
-  "banks",
-  "bar",
-  "barb",
-  "bard",
-  "barge",
-  "bark",
-  "barn",
-  "baron",
-  "bars",
-  "base",
-  "bash",
-  "basic",
-  "basil",
-  "basin",
-  "basis",
-  "bass",
-  "bat",
-  "batch",
-  "bath",
-  "baton",
-  "bay",
-  "bayou",
-  "beach",
-  "bead",
-  "beads",
-  "beak",
-  "beam",
-  "bean",
-  "bear",
-  "beard",
-  "beast",
-  "beat",
-  "beats",
-  "bed",
-  "bee",
-  "beech",
-  "beef",
-  "beep",
-  "beer",
-  "beet",
-  "begin",
-  "beige",
-  "being",
-  "belch",
-  "bell",
-  "belly",
-  "belt",
-  "bench",
-  "bend",
-  "bends",
-  "bent",
-  "beret",
-  "berry",
-  "bet",
-  "beta",
-  "bevel",
-  "bevy",
-  "bias",
-  "bib",
-  "bible",
-  "bid",
-  "bidet",
-  "bike",
-  "biker",
-  "bill",
-  "bin",
-  "bind",
-  "bingo",
-  "biome",
-  "biped",
-  "birch",
-  "bird",
-  "birth",
-  "bison",
-  "bit",
-  "bite",
-  "biter",
-  "black",
-  "blade",
-  "blame",
-  "blank",
-  "blast",
-  "blaze",
-  "blend",
-  "blimp",
-  "blind",
-  "bling",
-  "blink",
-  "blip",
-  "bliss",
-  "blitz",
-  "bloat",
-  "blob",
-  "block",
-  "blog",
-  "bloke",
-  "blond",
-  "blood",
-  "bloom",
-  "blow",
-  "blue",
-  "blues",
-  "bluff",
-  "blur",
-  "blurb",
-  "blush",
-  "boa",
-  "boar",
-  "board",
-  "boast",
-  "boat",
-  "bod",
-  "body",
-  "bog",
-  "bogey",
-  "boil",
-  "bold",
-  "bolt",
-  "bomb",
-  "bond",
-  "bone",
-  "boner",
-  "bones",
-  "bong",
-  "bongo",
-  "bonus",
-  "boo",
-  "book",
-  "boom",
-  "boon",
-  "boost",
-  "boot",
-  "booth",
-  "booty",
-  "booze",
-  "bore",
-  "borer",
-  "born",
-  "boss",
-  "bot",
-  "botch",
-  "bound",
-  "bow",
-  "bowel",
-  "bowl",
-  "bowls",
-  "box",
-  "boxer",
-  "boy",
-  "bra",
-  "brace",
-  "brag",
-  "braid",
-  "brail",
-  "brain",
-  "brake",
-  "bran",
-  "brand",
-  "brass",
-  "brat",
-  "brave",
-  "bravo",
-  "brawl",
-  "brawn",
-  "bread",
-  "break",
-  "breed",
-  "brew",
-  "briar",
-  "bribe",
-  "brick",
-  "bride",
-  "brie",
-  "brief",
-  "brim",
-  "brine",
-  "brink",
-  "brit",
-  "brits",
-  "britt",
-  "broad",
-  "broil",
-  "brood",
-  "brook",
-  "broom",
-  "broth",
-  "brow",
-  "brown",
-  "brunt",
-  "brush",
-  "brute",
-  "buck",
-  "bud",
-  "buddy",
-  "budge",
-  "buff",
-  "bug",
-  "buggy",
-  "bugle",
-  "build",
-  "bulb",
-  "bulge",
-  "bulk",
-  "bull",
-  "bully",
-  "bum",
-  "bump",
-  "bun",
-  "bunch",
-  "bung",
-  "bunk",
-  "bunny",
-  "buns",
-  "bunt",
-  "buoy",
-  "bur",
-  "burn",
-  "burns",
-  "burp",
-  "burst",
-  "bus",
-  "bush",
-  "buss",
-  "bust",
-  "buy",
-  "buyer",
-  "buzz",
-  "bye",
-  "bylaw",
-  "byte",
-  "cab",
-  "cabin",
-  "cable",
-  "cabot",
-  "cache",
-  "caddy",
-  "cadet",
-  "cafe",
-  "cage",
-  "cager",
-  "cake",
-  "calf",
-  "call",
-  "calm",
-  "cam",
-  "camel",
-  "camp",
-  "can",
-  "canal",
-  "candy",
-  "cane",
-  "cap",
-  "cape",
-  "caper",
-  "car",
-  "carat",
-  "card",
-  "cards",
-  "care",
-  "caret",
-  "cargo",
-  "carp",
-  "carry",
-  "cart",
-  "case",
-  "cash",
-  "cask",
-  "cast",
-  "caste",
-  "cat",
-  "catch",
-  "caulk",
-  "cause",
-  "cave",
-  "cavil",
-  "caw",
-  "cease",
-  "cedar",
-  "cell",
-  "cello",
-  "cent",
-  "chaff",
-  "chain",
-  "chair",
-  "chalk",
-  "champ",
-  "chant",
-  "chaos",
-  "chap",
-  "chard",
-  "charm",
-  "chart",
-  "chase",
-  "chasm",
-  "chat",
-  "cheat",
-  "check",
-  "cheek",
-  "cheep",
-  "cheer",
-  "chef",
-  "chess",
-  "chest",
-  "chew",
-  "chic",
-  "chick",
-  "chief",
-  "child",
-  "chill",
-  "chime",
-  "chimp",
-  "chin",
-  "chip",
-  "chips",
-  "chirp",
-  "chit",
-  "chive",
-  "chock",
-  "choir",
-  "choke",
-  "choky",
-  "chomp",
-  "chop",
-  "chord",
-  "chore",
-  "chow",
-  "chuck",
-  "chug",
-  "chum",
-  "chump",
-  "chunk",
-  "churn",
-  "chute",
-  "cider",
-  "cigar",
-  "cinch",
-  "cite",
-  "city",
-  "clack",
-  "claim",
-  "clam",
-  "clamp",
-  "clams",
-  "clan",
-  "clang",
-  "clank",
-  "clap",
-  "clash",
-  "clasp",
-  "class",
-  "clay",
-  "clean",
-  "clear",
-  "cleat",
-  "cleft",
-  "clerk",
-  "click",
-  "cliff",
-  "climb",
-  "cling",
-  "clip",
-  "cloak",
-  "clock",
-  "clog",
-  "clone",
-  "close",
-  "clot",
-  "cloth",
-  "cloud",
-  "clout",
-  "clove",
-  "clown",
-  "club",
-  "cluck",
-  "clue",
-  "clump",
-  "clunk",
-  "coach",
-  "coal",
-  "coast",
-  "coat",
-  "cobra",
-  "cocoa",
-  "cod",
-  "code",
-  "cog",
-  "coil",
-  "coin",
-  "coke",
-  "cola",
-  "cold",
-  "colon",
-  "color",
-  "colt",
-  "coma",
-  "comb",
-  "combo",
-  "come",
-  "comet",
-  "comic",
-  "comma",
-  "conch",
-  "condo",
-  "cone",
-  "coney",
-  "conk",
-  "cook",
-  "cool",
-  "coot",
-  "cop",
-  "cope",
-  "copy",
-  "coral",
-  "cord",
-  "cords",
-  "core",
-  "cork",
-  "corn",
-  "corp",
-  "corps",
-  "cost",
-  "costs",
-  "cosy",
-  "cot",
-  "couch",
-  "cough",
-  "count",
-  "court",
-  "cove",
-  "coven",
-  "cover",
-  "cow",
-  "cowl",
-  "cows",
-  "cozy",
-  "crab",
-  "crabs",
-  "crack",
-  "craft",
-  "cramp",
-  "crane",
-  "crank",
-  "crash",
-  "crate",
-  "crawl",
-  "craze",
-  "crazy",
-  "creak",
-  "cream",
-  "cred",
-  "cree",
-  "creed",
-  "creek",
-  "creep",
-  "crepe",
-  "cress",
-  "crest",
-  "crew",
-  "crib",
-  "crime",
-  "crimp",
-  "crisp",
-  "croak",
-  "crock",
-  "crook",
-  "crop",
-  "cross",
-  "crow",
-  "crowd",
-  "crown",
-  "crud",
-  "crude",
-  "crumb",
-  "crush",
-  "crust",
-  "crux",
-  "cry",
-  "crypt",
-  "cub",
-  "cubby",
-  "cube",
-  "cubit",
-  "cue",
-  "cuff",
-  "cull",
-  "cult",
-  "cup",
-  "curb",
-  "curd",
-  "cure",
-  "curl",
-  "curry",
-  "curse",
-  "curve",
-  "cut",
-  "cyan",
-  "cycle",
-  "cynic",
-  "dab",
-  "daily",
-  "dairy",
-  "daisy",
-  "dame",
-  "damp",
-  "dance",
-  "dandy",
-  "dane",
-  "dare",
-  "dark",
-  "dart",
-  "darts",
-  "dash",
-  "data",
-  "date",
-  "dawn",
-  "day",
-  "days",
-  "daze",
-  "dead",
-  "deaf",
-  "deal",
-  "dean",
-  "dear",
-  "death",
-  "debit",
-  "debt",
-  "debut",
-  "decal",
-  "decay",
-  "deck",
-  "decor",
-  "decoy",
-  "deed",
-  "deeds",
-  "deep",
-  "deer",
-  "delay",
-  "deli",
-  "delta",
-  "demo",
-  "demon",
-  "denim",
-  "dent",
-  "depot",
-  "depth",
-  "derby",
-  "desk",
-  "detox",
-  "deuce",
-  "devil",
-  "dew",
-  "dial",
-  "diary",
-  "dibs",
-  "dice",
-  "diet",
-  "dig",
-  "digit",
-  "digs",
-  "dill",
-  "dime",
-  "diner",
-  "ding",
-  "dip",
-  "dirt",
-  "disc",
-  "disco",
-  "dish",
-  "disk",
-  "ditch",
-  "ditto",
-  "dive",
-  "diver",
-  "dock",
-  "dodge",
-  "dog",
-  "dogma",
-  "doll",
-  "dolly",
-  "dolt",
-  "dome",
-  "donor",
-  "donut",
-  "doom",
-  "door",
-  "dope",
-  "dork",
-  "dorm",
-  "dot",
-  "doubt",
-  "dough",
-  "dove",
-  "dowel",
-  "down",
-  "dozen",
-  "dozer",
-  "draft",
-  "drag",
-  "drain",
-  "drama",
-  "drape",
-  "draw",
-  "dread",
-  "dream",
-  "dress",
-  "drew",
-  "drier",
-  "drift",
-  "drill",
-  "drink",
-  "drip",
-  "drive",
-  "drone",
-  "drool",
-  "drop",
-  "drove",
-  "drug",
-  "druid",
-  "drum",
-  "dry",
-  "dryer",
-  "duck",
-  "duct",
-  "due",
-  "duel",
-  "duet",
-  "dug",
-  "dunce",
-  "dune",
-  "dunk",
-  "dusk",
-  "dust",
-  "duty",
-  "dye",
-  "dyer",
-  "dying",
-  "eager",
-  "eagle",
-  "ear",
-  "earth",
-  "ease",
-  "easel",
-  "east",
-  "eater",
-  "eats",
-  "echo",
-  "edge",
-  "eel",
-  "egg",
-  "eggs",
-  "ego",
-  "eight",
-  "elbow",
-  "elder",
-  "elect",
-  "elf",
-  "elite",
-  "elk",
-  "elm",
-  "elves",
-  "email",
-  "ember",
-  "empty",
-  "emu",
-  "end",
-  "enemy",
-  "entry",
-  "envy",
-  "epic",
-  "epoxy",
-  "equal",
-  "era",
-  "error",
-  "essay",
-  "eve",
-  "even",
-  "event",
-  "evil",
-  "exam",
-  "exile",
-  "exit",
-  "extra",
-  "eye",
-  "eyes",
-  "fable",
-  "face",
-  "facet",
-  "fact",
-  "fad",
-  "fade",
-  "faint",
-  "fair",
-  "fairy",
-  "faith",
-  "fake",
-  "fall",
-  "falls",
-  "fame",
-  "fan",
-  "fancy",
-  "fang",
-  "far",
-  "farce",
-  "fare",
-  "farm",
-  "fast",
-  "fat",
-  "fate",
-  "fault",
-  "favor",
-  "fawn",
-  "fax",
-  "fear",
-  "feast",
-  "feat",
-  "fed",
-  "fee",
-  "feed",
-  "feel",
-  "felt",
-  "femur",
-  "fence",
-  "fern",
-  "ferry",
-  "fetch",
-  "feud",
-  "fever",
-  "few",
-  "fib",
-  "fiber",
-  "field",
-  "fiend",
-  "fifth",
-  "fifty",
-  "fig",
-  "fight",
-  "file",
-  "filet",
-  "fill",
-  "film",
-  "filth",
-  "final",
-  "finch",
-  "find",
-  "fine",
-  "fire",
-  "firm",
-  "first",
-  "fish",
-  "fist",
-  "fit",
-  "five",
-  "fiver",
-  "fives",
-  "fix",
-  "fixer",
-  "fizz",
-  "flag",
-  "flair",
-  "flak",
-  "flake",
-  "flame",
-  "flank",
-  "flap",
-  "flaps",
-  "flare",
-  "flash",
-  "flask",
-  "flat",
-  "flats",
-  "flaw",
-  "flea",
-  "fleet",
-  "flesh",
-  "flex",
-  "flick",
-  "flier",
-  "flies",
-  "fling",
-  "flint",
-  "flip",
-  "flirt",
-  "float",
-  "flock",
-  "flood",
-  "floor",
-  "flop",
-  "floss",
-  "flour",
-  "flow",
-  "flu",
-  "flub",
-  "fluff",
-  "fluid",
-  "fluke",
-  "flume",
-  "flush",
-  "flute",
-  "flux",
-  "fly",
-  "flyer",
-  "foam",
-  "focus",
-  "fog",
-  "foil",
-  "fold",
-  "folk",
-  "folks",
-  "folly",
-  "font",
-  "food",
-  "fool",
-  "foot",
-  "force",
-  "forge",
-  "fork",
-  "form",
-  "fort",
-  "forth",
-  "forty",
-  "forum",
-  "foul",
-  "found",
-  "four",
-  "fowl",
-  "fox",
-  "foyer",
-  "frail",
-  "frame",
-  "frat",
-  "fraud",
-  "fray",
-  "freak",
-  "free",
-  "freon",
-  "fret",
-  "friar",
-  "fries",
-  "frill",
-  "frisk",
-  "frizz",
-  "frog",
-  "front",
-  "frost",
-  "froth",
-  "frown",
-  "fruit",
-  "fry",
-  "fryer",
-  "fudge",
-  "fuel",
-  "full",
-  "fume",
-  "fumes",
-  "fun",
-  "fund",
-  "funds",
-  "fungi",
-  "funk",
-  "funny",
-  "fur",
-  "fury",
-  "fuse",
-  "fuss",
-  "futon",
-  "fuze",
-  "fuzz",
-  "gag",
-  "gage",
-  "gain",
-  "game",
-  "gamma",
-  "gap",
-  "gape",
-  "gas",
-  "gash",
-  "gasp",
-  "gate",
-  "gates",
-  "gator",
-  "gauge",
-  "gavel",
-  "gawk",
-  "gaze",
-  "gear",
-  "gecko",
-  "geek",
-  "gel",
-  "gem",
-  "gene",
-  "genie",
-  "genoa",
-  "genre",
-  "gent",
-  "germ",
-  "ghost",
-  "ghoul",
-  "giant",
-  "gift",
-  "gild",
-  "gimp",
-  "gin",
-  "gipsy",
-  "girl",
-  "gist",
-  "give",
-  "given",
-  "giver",
-  "gizmo",
-  "glad",
-  "glade",
-  "gland",
-  "glans",
-  "glare",
-  "glass",
-  "glaze",
-  "gleam",
-  "glee",
-  "glide",
-  "glint",
-  "globe",
-  "gloom",
-  "glory",
-  "gloss",
-  "glove",
-  "glow",
-  "glue",
-  "gnat",
-  "gnome",
-  "goal",
-  "goat",
-  "going",
-  "gold",
-  "golem",
-  "golf",
-  "goner",
-  "goo",
-  "good",
-  "goof",
-  "goofy",
-  "goon",
-  "goose",
-  "goth",
-  "gouge",
-  "gown",
-  "grab",
-  "grace",
-  "grad",
-  "grade",
-  "graft",
-  "grail",
-  "grain",
-  "gram",
-  "grand",
-  "grant",
-  "grape",
-  "graph",
-  "grasp",
-  "grass",
-  "grate",
-  "grave",
-  "gravy",
-  "gray",
-  "graze",
-  "great",
-  "greed",
-  "green",
-  "grey",
-  "grid",
-  "grief",
-  "grill",
-  "grime",
-  "grin",
-  "grind",
-  "grip",
-  "gripe",
-  "grit",
-  "grits",
-  "groan",
-  "groom",
-  "gross",
-  "group",
-  "grove",
-  "growl",
-  "grub",
-  "gruel",
-  "grump",
-  "grunt",
-  "guard",
-  "guess",
-  "guest",
-  "guide",
-  "guild",
-  "guilt",
-  "gulch",
-  "gulf",
-  "gull",
-  "gulp",
-  "gum",
-  "gun",
-  "guppy",
-  "guru",
-  "gush",
-  "gust",
-  "gut",
-  "guts",
-  "guy",
-  "gym",
-  "habit",
-  "hack",
-  "hag",
-  "hail",
-  "hair",
-  "half",
-  "hall",
-  "halo",
-  "halt",
-  "ham",
-  "hand",
-  "hands",
-  "handy",
-  "hang",
-  "hare",
-  "harp",
-  "hash",
-  "haste",
-  "hat",
-  "hatch",
-  "hate",
-  "hater",
-  "haunt",
-  "have",
-  "haven",
-  "havoc",
-  "hawk",
-  "hay",
-  "haze",
-  "hazel",
-  "head",
-  "heap",
-  "heaps",
-  "heart",
-  "heat",
-  "heavy",
-  "hedge",
-  "heed",
-  "heel",
-  "heft",
-  "heir",
-  "helix",
-  "hell",
-  "hello",
-  "helm",
-  "help",
-  "hem",
-  "hemp",
-  "hen",
-  "herb",
-  "herd",
-  "here",
-  "hero",
-  "hex",
-  "hick",
-  "hide",
-  "high",
-  "hike",
-  "hiker",
-  "hill",
-  "hilt",
-  "hind",
-  "hinge",
-  "hint",
-  "hip",
-  "hippo",
-  "hippy",
-  "hire",
-  "hiss",
-  "hit",
-  "hitch",
-  "hive",
-  "hives",
-  "hoagy",
-  "hoard",
-  "hoax",
-  "hob",
-  "hobby",
-  "hobo",
-  "hog",
-  "hoist",
-  "hold",
-  "hole",
-  "home",
-  "honey",
-  "honk",
-  "honor",
-  "hoof",
-  "hook",
-  "hooks",
-  "hoop",
-  "hoops",
-  "hoot",
-  "hop",
-  "hope",
-  "hops",
-  "horde",
-  "horn",
-  "horse",
-  "hose",
-  "host",
-  "hotel",
-  "hound",
-  "hour",
-  "hours",
-  "house",
-  "howl",
-  "hub",
-  "hue",
-  "huff",
-  "hug",
-  "hula",
-  "hulk",
-  "hull",
-  "hum",
-  "human",
-  "humor",
-  "hump",
-  "humus",
-  "hunch",
-  "hunk",
-  "hunt",
-  "hurl",
-  "hurry",
-  "hurt",
-  "hush",
-  "husk",
-  "husky",
-  "hut",
-  "hydra",
-  "hyena",
-  "hymn",
-  "hype",
-  "ibis",
-  "ice",
-  "icing",
-  "icon",
-  "idea",
-  "ideal",
-  "idiom",
-  "idiot",
-  "idle",
-  "idler",
-  "idol",
-  "igloo",
-  "iglu",
-  "ill",
-  "image",
-  "imp",
-  "inch",
-  "index",
-  "info",
-  "ingot",
-  "ink",
-  "inlet",
-  "inn",
-  "input",
-  "intro",
-  "ion",
-  "iris",
-  "iron",
-  "irony",
-  "isle",
-  "issue",
-  "itch",
-  "ivory",
-  "ivy",
-  "jab",
-  "jack",
-  "jacks",
-  "jail",
-  "jam",
-  "jamb",
-  "jar",
-  "java",
-  "jaw",
-  "jay",
-  "jazz",
-  "jean",
-  "jeans",
-  "jeep",
-  "jeer",
-  "jello",
-  "jelly",
-  "jest",
-  "jet",
-  "jetty",
-  "jewel",
-  "jig",
-  "jive",
-  "job",
-  "jock",
-  "jog",
-  "join",
-  "joint",
-  "joist",
-  "joke",
-  "joker",
-  "jolly",
-  "jolt",
-  "joust",
-  "joy",
-  "judge",
-  "jug",
-  "juice",
-  "juke",
-  "jump",
-  "junk",
-  "junky",
-  "juror",
-  "jury",
-  "kale",
-  "kayak",
-  "kazoo",
-  "kebab",
-  "keen",
-  "keep",
-  "keg",
-  "kelp",
-  "key",
-  "kick",
-  "kid",
-  "kiddy",
-  "kiln",
-  "kilo",
-  "kilt",
-  "kin",
-  "kind",
-  "king",
-  "kiss",
-  "kit",
-  "kite",
-  "kitty",
-  "kiwi",
-  "knack",
-  "knee",
-  "kneel",
-  "knell",
-  "knife",
-  "knit",
-  "knob",
-  "knock",
-  "knot",
-  "know",
-  "koala",
-  "krill",
-  "lab",
-  "label",
-  "labor",
-  "lace",
-  "lack",
-  "lad",
-  "ladle",
-  "lady",
-  "lag",
-  "lair",
-  "lake",
-  "lamb",
-  "lame",
-  "lamp",
-  "lance",
-  "land",
-  "lane",
-  "lap",
-  "lapel",
-  "lapse",
-  "lard",
-  "large",
-  "larva",
-  "laser",
-  "lash",
-  "lass",
-  "lasso",
-  "last",
-  "lat",
-  "latch",
-  "latex",
-  "lathe",
-  "latte",
-  "laugh",
-  "lava",
-  "law",
-  "lawn",
-  "laws",
-  "lay",
-  "layer",
-  "layup",
-  "leach",
-  "lead",
-  "leaf",
-  "leak",
-  "lean",
-  "leap",
-  "lear",
-  "lease",
-  "leash",
-  "least",
-  "leave",
-  "ledge",
-  "leech",
-  "leeds",
-  "leek",
-  "leer",
-  "left",
-  "lefty",
-  "leg",
-  "lego",
-  "legs",
-  "lemon",
-  "lemur",
-  "lens",
-  "lent",
-  "let",
-  "level",
-  "lever",
-  "liar",
-  "libel",
-  "lick",
-  "lid",
-  "lie",
-  "lied",
-  "life",
-  "lift",
-  "light",
-  "like",
-  "lilac",
-  "limb",
-  "limbo",
-  "lime",
-  "limit",
-  "limp",
-  "line",
-  "linen",
-  "liner",
-  "link",
-  "links",
-  "lint",
-  "lion",
-  "lip",
-  "lisp",
-  "list",
-  "lit",
-  "liter",
-  "liver",
-  "llama",
-  "loach",
-  "load",
-  "loads",
-  "loaf",
-  "loan",
-  "lob",
-  "lobby",
-  "lobe",
-  "local",
-  "lock",
-  "lodge",
-  "loft",
-  "log",
-  "logic",
-  "logo",
-  "loner",
-  "look",
-  "loom",
-  "loon",
-  "loony",
-  "loop",
-  "loot",
-  "lord",
-  "loser",
-  "loss",
-  "lost",
-  "lot",
-  "lots",
-  "lotto",
-  "lotus",
-  "love",
-  "lover",
-  "low",
-  "lower",
-  "luck",
-  "lump",
-  "lunch",
-  "lung",
-  "lure",
-  "lush",
-  "lying",
-  "mace",
-  "macro",
-  "madam",
-  "mafia",
-  "magi",
-  "magic",
-  "magma",
-  "maid",
-  "mail",
-  "main",
-  "major",
-  "maker",
-  "male",
-  "malt",
-  "mam",
-  "mama",
-  "mamba",
-  "mambo",
-  "mamma",
-  "man",
-  "mane",
-  "mango",
-  "mania",
-  "manor",
-  "map",
-  "maple",
-  "march",
-  "mare",
-  "mark",
-  "marks",
-  "mars",
-  "marsh",
-  "mash",
-  "mask",
-  "mass",
-  "mast",
-  "mat",
-  "match",
-  "mate",
-  "mates",
-  "math",
-  "maths",
-  "max",
-  "maxim",
-  "may",
-  "mayo",
-  "mayor",
-  "maze",
-  "meal",
-  "mean",
-  "means",
-  "meat",
-  "medal",
-  "medic",
-  "meet",
-  "meld",
-  "melee",
-  "melon",
-  "melt",
-  "memo",
-  "men",
-  "mend",
-  "menu",
-  "meow",
-  "mercy",
-  "merit",
-  "mesh",
-  "mess",
-  "metal",
-  "meter",
-  "meth",
-  "metro",
-  "might",
-  "mile",
-  "milk",
-  "mill",
-  "mills",
-  "mimer",
-  "mimic",
-  "min",
-  "mince",
-  "mind",
-  "mine",
-  "miner",
-  "mini",
-  "mink",
-  "minor",
-  "mint",
-  "minus",
-  "miser",
-  "miss",
-  "mist",
-  "mite",
-  "miter",
-  "mitt",
-  "mix",
-  "mixer",
-  "moan",
-  "moat",
-  "mob",
-  "mocha",
-  "mock",
-  "mod",
-  "modal",
-  "mode",
-  "model",
-  "modem",
-  "mogul",
-  "mojo",
-  "molar",
-  "mold",
-  "mole",
-  "molt",
-  "mom",
-  "momma",
-  "mommy",
-  "money",
-  "monk",
-  "month",
-  "moo",
-  "mooch",
-  "mood",
-  "moody",
-  "moon",
-  "moose",
-  "mop",
-  "mope",
-  "moped",
-  "moral",
-  "morse",
-  "moss",
-  "motel",
-  "moth",
-  "motor",
-  "motto",
-  "mould",
-  "mound",
-  "mount",
-  "mouse",
-  "mouth",
-  "move",
-  "mover",
-  "movie",
-  "mow",
-  "mucus",
-  "mud",
-  "muff",
-  "mug",
-  "mulch",
-  "mule",
-  "mum",
-  "mummy",
-  "munch",
-  "mural",
-  "muse",
-  "mush",
-  "music",
-  "musk",
-  "must",
-  "mute",
-  "mutt",
-  "mylar",
-  "nacho",
-  "name",
-  "namer",
-  "names",
-  "nanna",
-  "nap",
-  "nasal",
-  "navy",
-  "neck",
-  "need",
-  "needy",
-  "neon",
-  "nepal",
-  "nerd",
-  "nerve",
-  "nest",
-  "net",
-  "news",
-  "newt",
-  "nick",
-  "niece",
-  "night",
-  "nine",
-  "niner",
-  "ninja",
-  "ninth",
-  "noble",
-  "nod",
-  "node",
-  "noise",
-  "nomad",
-  "none",
-  "nook",
-  "noon",
-  "noose",
-  "north",
-  "nose",
-  "notch",
-  "note",
-  "noun",
-  "nudge",
-  "nuke",
-  "nun",
-  "nurse",
-  "nut",
-  "nylon",
-  "oaf",
-  "oak",
-  "oar",
-  "oasis",
-  "oat",
-  "oates",
-  "oath",
-  "ocean",
-  "octet",
-  "odds",
-  "ode",
-  "odor",
-  "offer",
-  "ogre",
-  "oil",
-  "oiler",
-  "oink",
-  "okay",
-  "old",
-  "oldie",
-  "olive",
-  "omega",
-  "omen",
-  "one",
-  "onion",
-  "onset",
-  "ooze",
-  "open",
-  "optic",
-  "oral",
-  "orange",
-  "orb",
-  "orbit",
-  "orca",
-  "order",
-  "ore",
-  "oreo",
-  "organ",
-  "ounce",
-  "out",
-  "oval",
-  "oven",
-  "over",
-  "owl",
-  "owner",
-  "oxbow",
-  "oxen",
-  "ozone",
-  "pace",
-  "pacer",
-  "pack",
-  "pact",
-  "pad",
-  "page",
-  "pager",
-  "pail",
-  "pain",
-  "pains",
-  "paint",
-  "pair",
-  "pal",
-  "pale",
-  "palm",
-  "pan",
-  "panda",
-  "pane",
-  "panel",
-  "panic",
-  "pansy",
-  "pant",
-  "pants",
-  "papa",
-  "paper",
-  "par",
-  "park",
-  "parks",
-  "part",
-  "parts",
-  "party",
-  "pass",
-  "past",
-  "pasta",
-  "paste",
-  "pat",
-  "patch",
-  "path",
-  "patio",
-  "pause",
-  "pave",
-  "paw",
-  "pawn",
-  "pay",
-  "payer",
-  "peace",
-  "peach",
-  "peak",
-  "pear",
-  "pearl",
-  "pecan",
-  "pedal",
-  "peek",
-  "peel",
-  "peer",
-  "peg",
-  "pelt",
-  "pen",
-  "penny",
-  "perch",
-  "peril",
-  "perk",
-  "pesto",
-  "pet",
-  "petal",
-  "petty",
-  "phase",
-  "phone",
-  "photo",
-  "piano",
-  "pick",
-  "pie",
-  "piece",
-  "pier",
-  "pig",
-  "piggy",
-  "pigmy",
-  "pike",
-  "pile",
-  "piles",
-  "pill",
-  "pimp",
-  "pin",
-  "pinch",
-  "pine",
-  "ping",
-  "pink",
-  "pinky",
-  "pinot",
-  "pint",
-  "pipe",
-  "pit",
-  "pita",
-  "pitch",
-  "pitt",
-  "pity",
-  "pivot",
-  "pixel",
-  "pizza",
-  "place",
-  "plaid",
-  "plain",
-  "plan",
-  "plane",
-  "plank",
-  "plant",
-  "plate",
-  "play",
-  "plaza",
-  "plea",
-  "plier",
-  "plot",
-  "plow",
-  "ploy",
-  "pluck",
-  "plug",
-  "plum",
-  "plumb",
-  "plume",
-  "plump",
-  "plus",
-  "plush",
-  "plyer",
-  "pod",
-  "poem",
-  "poet",
-  "point",
-  "poke",
-  "poker",
-  "pole",
-  "poll",
-  "polls",
-  "pond",
-  "pong",
-  "pony",
-  "pooch",
-  "poof",
-  "pool",
-  "poor",
-  "pop",
-  "poppy",
-  "porch",
-  "pore",
-  "pork",
-  "port",
-  "pose",
-  "poser",
-  "post",
-  "pot",
-  "pouch",
-  "pound",
-  "power",
-  "prank",
-  "prawn",
-  "press",
-  "prey",
-  "price",
-  "pride",
-  "prime",
-  "prism",
-  "prize",
-  "pro",
-  "probe",
-  "prom",
-  "promo",
-  "proof",
-  "prop",
-  "props",
-  "prose",
-  "prowl",
-  "prune",
-  "pry",
-  "pub",
-  "puck",
-  "puff",
-  "pug",
-  "pull",
-  "pulp",
-  "pulse",
-  "puma",
-  "pump",
-  "pun",
-  "punch",
-  "punk",
-  "punks",
-  "punt",
-  "pup",
-  "pupil",
-  "puppy",
-  "purge",
-  "purse",
-  "push",
-  "put",
-  "putt",
-  "putty",
-  "quack",
-  "quad",
-  "quake",
-  "qualm",
-  "quart",
-  "queen",
-  "query",
-  "quest",
-  "quick",
-  "quid",
-  "quiet",
-  "quilt",
-  "quirk",
-  "quirt",
-  "quiz",
-  "quota",
-  "quote",
-  "race",
-  "racer",
-  "rad",
-  "radar",
-  "radio",
-  "raft",
-  "rafts",
-  "rag",
-  "rage",
-  "raid",
-  "rail",
-  "rails",
-  "rain",
-  "raise",
-  "rake",
-  "rally",
-  "ram",
-  "ramp",
-  "ranch",
-  "range",
-  "rank",
-  "rant",
-  "rap",
-  "rapid",
-  "rash",
-  "rat",
-  "rate",
-  "rates",
-  "ratio",
-  "raw",
-  "ray",
-  "razor",
-  "razz",
-  "reach",
-  "read",
-  "ready",
-  "real",
-  "realm",
-  "ream",
-  "rear",
-  "rebel",
-  "red",
-  "reed",
-  "reef",
-  "reek",
-  "reel",
-  "reign",
-  "relay",
-  "relic",
-  "rent",
-  "reply",
-  "reset",
-  "resin",
-  "rest",
-  "retro",
-  "revel",
-  "rhino",
-  "rhyme",
-  "rib",
-  "rice",
-  "ricer",
-  "rich",
-  "ride",
-  "rider",
-  "ridge",
-  "riff",
-  "rifle",
-  "rift",
-  "rig",
-  "right",
-  "rim",
-  "rind",
-  "ring",
-  "rings",
-  "rink",
-  "rinse",
-  "riot",
-  "rip",
-  "rise",
-  "riser",
-  "risk",
-  "rite",
-  "rival",
-  "river",
-  "roach",
-  "road",
-  "roads",
-  "roar",
-  "roast",
-  "robe",
-  "robin",
-  "robot",
-  "rock",
-  "rod",
-  "rodeo",
-  "rogue",
-  "role",
-  "roll",
-  "room",
-  "rooms",
-  "roost",
-  "root",
-  "roots",
-  "rope",
-  "rose",
-  "rot",
-  "rotor",
-  "rouge",
-  "rough",
-  "round",
-  "route",
-  "rover",
-  "row",
-  "rowdy",
-  "rower",
-  "royal",
-  "rub",
-  "rube",
-  "ruby",
-  "rug",
-  "rugby",
-  "ruin",
-  "rule",
-  "ruler",
-  "rum",
-  "rummy",
-  "rumor",
-  "run",
-  "rune",
-  "rung",
-  "runt",
-  "ruse",
-  "rush",
-  "rust",
-  "rut",
-  "saber",
-  "safe",
-  "sag",
-  "saga",
-  "sage",
-  "sail",
-  "saint",
-  "salad",
-  "sale",
-  "salem",
-  "sales",
-  "salon",
-  "salsa",
-  "salt",
-  "same",
-  "sand",
-  "sands",
-  "sang",
-  "sash",
-  "sass",
-  "sauce",
-  "sauna",
-  "save",
-  "saver",
-  "savor",
-  "saw",
-  "say",
-  "scale",
-  "scan",
-  "scar",
-  "scare",
-  "scarf",
-  "scene",
-  "scent",
-  "scold",
-  "scone",
-  "scoop",
-  "scope",
-  "score",
-  "scorn",
-  "scout",
-  "scrap",
-  "sea",
-  "seal",
-  "seam",
-  "seat",
-  "seats",
-  "sect",
-  "sedan",
-  "see",
-  "seed",
-  "seek",
-  "seer",
-  "self",
-  "sell",
-  "sense",
-  "serum",
-  "serve",
-  "servo",
-  "set",
-  "setup",
-  "seven",
-  "shack",
-  "shade",
-  "shake",
-  "sham",
-  "shame",
-  "shank",
-  "shape",
-  "shard",
-  "share",
-  "shark",
-  "sharp",
-  "shave",
-  "shawl",
-  "shed",
-  "sheep",
-  "sheet",
-  "shelf",
-  "shell",
-  "shift",
-  "shill",
-  "shim",
-  "shin",
-  "ship",
-  "shirt",
-  "shoe",
-  "shoes",
-  "shop",
-  "shore",
-  "shot",
-  "shove",
-  "show",
-  "shred",
-  "shrub",
-  "shrug",
-  "shy",
-  "sick",
-  "siege",
-  "sigh",
-  "sight",
-  "sign",
-  "silk",
-  "silks",
-  "silly",
-  "silo",
-  "sin",
-  "sink",
-  "sinus",
-  "sip",
-  "sir",
-  "siren",
-  "six",
-  "sixer",
-  "sixth",
-  "sixty",
-  "size",
-  "ski",
-  "skid",
-  "skier",
-  "skill",
-  "skim",
-  "skin",
-  "skip",
-  "skirt",
-  "skit",
-  "skull",
-  "skunk",
-  "sky",
-  "slab",
-  "slack",
-  "slag",
-  "slain",
-  "slam",
-  "slang",
-  "slant",
-  "slap",
-  "slash",
-  "slate",
-  "slave",
-  "slaw",
-  "sled",
-  "sleep",
-  "sleet",
-  "slew",
-  "slews",
-  "slice",
-  "slick",
-  "slide",
-  "slime",
-  "sling",
-  "slip",
-  "slit",
-  "slob",
-  "slope",
-  "slot",
-  "sloth",
-  "slug",
-  "slum",
-  "slump",
-  "slur",
-  "slush",
-  "smack",
-  "small",
-  "smart",
-  "smash",
-  "smear",
-  "smell",
-  "smelt",
-  "smile",
-  "smirk",
-  "smith",
-  "smock",
-  "smog",
-  "smoke",
-  "snack",
-  "snag",
-  "snail",
-  "snake",
-  "snap",
-  "snare",
-  "snarl",
-  "sneak",
-  "sniff",
-  "snipe",
-  "snore",
-  "snort",
-  "snot",
-  "snow",
-  "snug",
-  "soak",
-  "soap",
-  "soar",
-  "sob",
-  "sock",
-  "sofa",
-  "softy",
-  "soil",
-  "sole",
-  "solid",
-  "son",
-  "sonar",
-  "song",
-  "sonny",
-  "soot",
-  "sooth",
-  "sore",
-  "sort",
-  "soul",
-  "sound",
-  "soup",
-  "sour",
-  "south",
-  "spa",
-  "space",
-  "spade",
-  "spam",
-  "span",
-  "spar",
-  "spare",
-  "spark",
-  "spasm",
-  "spat",
-  "spawn",
-  "speed",
-  "spell",
-  "spelt",
-  "spice",
-  "spike",
-  "spill",
-  "spin",
-  "spit",
-  "spite",
-  "splat",
-  "split",
-  "spoil",
-  "spoke",
-  "spoof",
-  "spook",
-  "spool",
-  "spoon",
-  "spore",
-  "sport",
-  "spot",
-  "spots",
-  "spout",
-  "spray",
-  "spree",
-  "spud",
-  "spur",
-  "spurt",
-  "spy",
-  "squat",
-  "squid",
-  "stab",
-  "stack",
-  "staff",
-  "stag",
-  "stage",
-  "stain",
-  "stair",
-  "stake",
-  "stalk",
-  "stall",
-  "stamp",
-  "stand",
-  "star",
-  "stare",
-  "start",
-  "stash",
-  "state",
-  "stay",
-  "stays",
-  "steak",
-  "steal",
-  "steam",
-  "steed",
-  "steel",
-  "steer",
-  "stem",
-  "step",
-  "steps",
-  "stern",
-  "stew",
-  "stick",
-  "stiff",
-  "still",
-  "stilt",
-  "sting",
-  "stink",
-  "stint",
-  "stir",
-  "stock",
-  "stoic",
-  "stomp",
-  "stone",
-  "stool",
-  "stoop",
-  "stop",
-  "stops",
-  "store",
-  "stork",
-  "storm",
-  "story",
-  "stove",
-  "strap",
-  "straw",
-  "stray",
-  "strip",
-  "strum",
-  "strut",
-  "stub",
-  "stud",
-  "study",
-  "stuff",
-  "stump",
-  "stunt",
-  "style",
-  "sub",
-  "suds",
-  "sugar",
-  "suit",
-  "suite",
-  "sum",
-  "sumer",
-  "sun",
-  "sung",
-  "super",
-  "surf",
-  "surge",
-  "sushi",
-  "sutra",
-  "swab",
-  "swag",
-  "swamp",
-  "swan",
-  "swap",
-  "swarm",
-  "sway",
-  "sweat",
-  "sweep",
-  "sweet",
-  "swell",
-  "swift",
-  "swim",
-  "swine",
-  "swing",
-  "swipe",
-  "swirl",
-  "swish",
-  "syrup",
-  "table",
-  "tack",
-  "taco",
-  "tact",
-  "tad",
-  "taffy",
-  "tag",
-  "tail",
-  "tails",
-  "take",
-  "taker",
-  "tale",
-  "talk",
-  "talks",
-  "tall",
-  "tally",
-  "talon",
-  "tan",
-  "tank",
-  "tap",
-  "tape",
-  "taps",
-  "tar",
-  "tarp",
-  "tart",
-  "task",
-  "taste",
-  "taunt",
-  "tax",
-  "taxer",
-  "taxi",
-  "taxis",
-  "tea",
-  "teach",
-  "teal",
-  "team",
-  "tear",
-  "tears",
-  "tease",
-  "teen",
-  "teens",
-  "teeth",
-  "tell",
-  "temp",
-  "tempo",
-  "ten",
-  "tense",
-  "tent",
-  "tenth",
-  "term",
-  "terms",
-  "test",
-  "text",
-  "thaw",
-  "theft",
-  "theme",
-  "then",
-  "there",
-  "theta",
-  "thick",
-  "thief",
-  "thigh",
-  "thing",
-  "think",
-  "third",
-  "thorn",
-  "three",
-  "throw",
-  "thud",
-  "thug",
-  "thumb",
-  "tick",
-  "tide",
-  "tidy",
-  "tie",
-  "tier",
-  "tiger",
-  "tilde",
-  "tile",
-  "till",
-  "time",
-  "timer",
-  "times",
-  "timid",
-  "tin",
-  "tint",
-  "tip",
-  "tire",
-  "titan",
-  "title",
-  "toad",
-  "toady",
-  "toast",
-  "today",
-  "toe",
-  "toil",
-  "token",
-  "toll",
-  "tomb",
-  "tome",
-  "ton",
-  "tone",
-  "toner",
-  "tongs",
-  "tonic",
-  "tons",
-  "tool",
-  "toon",
-  "toot",
-  "tooth",
-  "top",
-  "topic",
-  "torch",
-  "torso",
-  "toss",
-  "total",
-  "tote",
-  "totem",
-  "touch",
-  "tough",
-  "tour",
-  "tours",
-  "tow",
-  "towel",
-  "tower",
-  "town",
-  "towny",
-  "toxin",
-  "toy",
-  "trace",
-  "track",
-  "trade",
-  "trail",
-  "train",
-  "trait",
-  "trap",
-  "trash",
-  "tray",
-  "tread",
-  "treat",
-  "tree",
-  "trek",
-  "trend",
-  "triad",
-  "trial",
-  "trick",
-  "trim",
-  "trio",
-  "trip",
-  "troll",
-  "troop",
-  "trot",
-  "trout",
-  "truce",
-  "truck",
-  "true",
-  "trump",
-  "trunk",
-  "trust",
-  "truth",
-  "try",
-  "tub",
-  "tuba",
-  "tube",
-  "tuck",
-  "tug",
-  "tulip",
-  "tummy",
-  "tumor",
-  "tuna",
-  "tune",
-  "tuner",
-  "tunic",
-  "turf",
-  "turn",
-  "tush",
-  "tusk",
-  "tutor",
-  "twine",
-  "twins",
-  "twirl",
-  "twist",
-  "two",
-  "tying",
-  "type",
-  "typo",
-  "udder",
-  "ulcer",
-  "uncle",
-  "union",
-  "unit",
-  "unity",
-  "upper",
-  "upset",
-  "urn",
-  "usage",
-  "use",
-  "user",
-  "usher",
-  "using",
-  "valet",
-  "valor",
-  "value",
-  "valve",
-  "van",
-  "vase",
-  "vat",
-  "vault",
-  "vegan",
-  "veil",
-  "vein",
-  "venom",
-  "vent",
-  "venue",
-  "verb",
-  "verge",
-  "vest",
-  "vet",
-  "vial",
-  "vibe",
-  "vibes",
-  "vice",
-  "video",
-  "view",
-  "vigil",
-  "vine",
-  "vinyl",
-  "viola",
-  "viper",
-  "virgo",
-  "virus",
-  "visit",
-  "visor",
-  "vista",
-  "vocal",
-  "vodka",
-  "vogue",
-  "voice",
-  "void",
-  "volt",
-  "vote",
-  "voter",
-  "vow",
-  "vowel",
-  "wacko",
-  "wad",
-  "wade",
-  "wader",
-  "wads",
-  "wafer",
-  "waft",
-  "wag",
-  "wage",
-  "wager",
-  "wages",
-  "wagon",
-  "wail",
-  "wain",
-  "waist",
-  "wait",
-  "wake",
-  "walk",
-  "wall",
-  "waltz",
-  "wane",
-  "want",
-  "war",
-  "ward",
-  "ware",
-  "warp",
-  "wart",
-  "wash",
-  "wasp",
-  "waste",
-  "watch",
-  "water",
-  "watt",
-  "watts",
-  "wave",
-  "waver",
-  "wax",
-  "way",
-  "ways",
-  "wear",
-  "weave",
-  "web",
-  "wed",
-  "wedge",
-  "week",
-  "weird",
-  "well",
-  "wells",
-  "welsh",
-  "west",
-  "wet",
-  "whack",
-  "whale",
-  "wharf",
-  "wheat",
-  "wheel",
-  "whey",
-  "whiff",
-  "while",
-  "whim",
-  "whip",
-  "whirl",
-  "whisk",
-  "white",
-  "who",
-  "whole",
-  "whore",
-  "why",
-  "wick",
-  "widow",
-  "width",
-  "wife",
-  "wig",
-  "wild",
-  "will",
-  "wilt",
-  "wimp",
-  "win",
-  "wince",
-  "winch",
-  "wind",
-  "wine",
-  "wing",
-  "wings",
-  "wink",
-  "wipe",
-  "wiper",
-  "wire",
-  "wise",
-  "wish",
-  "wit",
-  "witch",
-  "wits",
-  "woe",
-  "wolf",
-  "woman",
-  "womb",
-  "won",
-  "wood",
-  "woods",
-  "woof",
-  "wool",
-  "word",
-  "words",
-  "work",
-  "works",
-  "world",
-  "worm",
-  "worry",
-  "worse",
-  "worst",
-  "wort",
-  "worth",
-  "wound",
-  "wow",
-  "wrack",
-  "wrap",
-  "wrath",
-  "wreck",
-  "wring",
-  "wrist",
-  "wrong",
-  "yam",
-  "yard",
-  "yarn",
-  "yawn",
-  "yay",
-  "year",
-  "years",
-  "yeast",
-  "yell",
-  "yes",
-  "yeti",
-  "yield",
-  "yoga",
-  "yolk",
-  "young",
-  "youth",
-  "zap",
-  "zebra",
-  "zinc",
-  "zing",
-  "zip",
-  "zit",
-  "zone",
-  "zoo",
-  "zoom",
-  "zero",
-  "zany",
-  "whir",
-  "welt",
-  "whig",
-  "wand",
-  "twin",
-  "tribe",
-  "tilt",
-  "sword",
-  "spine",
-  "spear",
-  "site",
-  "shock",
-  "sent",
+  'ace',
+  'ache',
+  'acid',
+  'acme',
+  'acorn',
+  'acre',
+  'act',
+  'actor',
+  'add',
+  'adder',
+  'adept',
+  'advil',
+  'afro',
+  'agave',
+  'age',
+  'aged',
+  'agent',
+  'agony',
+  'ailey',
+  'aim',
+  'aioli',
+  'air',
+  'aisle',
+  'akron',
+  'alarm',
+  'album',
+  'ale',
+  'alert',
+  'algae',
+  'alias',
+  'alibi',
+  'alien',
+  'alley',
+  'alloy',
+  'ally',
+  'aloe',
+  'alpha',
+  'alps',
+  'altar',
+  'amber',
+  'amigo',
+  'amino',
+  'amish',
+  'ammo',
+  'amp',
+  'angel',
+  'anger',
+  'angle',
+  'angst',
+  'angus',
+  'anime',
+  'ankle',
+  'annex',
+  'anole',
+  'ant',
+  'ante',
+  'antic',
+  'anvil',
+  'ape',
+  'apex',
+  'aphid',
+  'apple',
+  'april',
+  'apron',
+  'aqua',
+  'arbor',
+  'arc',
+  'arch',
+  'area',
+  'arena',
+  'argon',
+  'argus',
+  'ark',
+  'arm',
+  'armor',
+  'arms',
+  'army',
+  'aroma',
+  'array',
+  'arrow',
+  'arson',
+  'art',
+  'ascot',
+  'aspen',
+  'asset',
+  'ate',
+  'atom',
+  'attic',
+  'audio',
+  'audit',
+  'auger',
+  'aunt',
+  'aunty',
+  'aura',
+  'auto',
+  'award',
+  'awe',
+  'awl',
+  'axe',
+  'axiom',
+  'axis',
+  'axle',
+  'azure',
+  'baby',
+  'back',
+  'bacon',
+  'bad',
+  'badge',
+  'bag',
+  'bagel',
+  'bail',
+  'bait',
+  'baker',
+  'bale',
+  'balk',
+  'ball',
+  'balm',
+  'ban',
+  'band',
+  'bane',
+  'banjo',
+  'bank',
+  'banks',
+  'bar',
+  'barb',
+  'bard',
+  'barge',
+  'bark',
+  'barn',
+  'baron',
+  'bars',
+  'base',
+  'bash',
+  'basic',
+  'basil',
+  'basin',
+  'basis',
+  'bass',
+  'bat',
+  'batch',
+  'bath',
+  'baton',
+  'bay',
+  'bayou',
+  'beach',
+  'bead',
+  'beads',
+  'beak',
+  'beam',
+  'bean',
+  'bear',
+  'beard',
+  'beast',
+  'beat',
+  'beats',
+  'bed',
+  'bee',
+  'beech',
+  'beef',
+  'beep',
+  'beer',
+  'beet',
+  'begin',
+  'beige',
+  'being',
+  'belch',
+  'bell',
+  'belly',
+  'belt',
+  'bench',
+  'bend',
+  'bends',
+  'bent',
+  'beret',
+  'berry',
+  'bet',
+  'beta',
+  'bevel',
+  'bevy',
+  'bias',
+  'bib',
+  'bible',
+  'bid',
+  'bidet',
+  'bike',
+  'biker',
+  'bill',
+  'bin',
+  'bind',
+  'bingo',
+  'biome',
+  'biped',
+  'birch',
+  'bird',
+  'birth',
+  'bison',
+  'bit',
+  'bite',
+  'biter',
+  'black',
+  'blade',
+  'blame',
+  'blank',
+  'blast',
+  'blaze',
+  'blend',
+  'blimp',
+  'blind',
+  'bling',
+  'blink',
+  'blip',
+  'bliss',
+  'blitz',
+  'bloat',
+  'blob',
+  'block',
+  'blog',
+  'bloke',
+  'blond',
+  'blood',
+  'bloom',
+  'blow',
+  'blue',
+  'blues',
+  'bluff',
+  'blur',
+  'blurb',
+  'blush',
+  'boa',
+  'boar',
+  'board',
+  'boast',
+  'boat',
+  'bod',
+  'body',
+  'bog',
+  'bogey',
+  'boil',
+  'bold',
+  'bolt',
+  'bomb',
+  'bond',
+  'bone',
+  'boner',
+  'bones',
+  'bong',
+  'bongo',
+  'bonus',
+  'boo',
+  'book',
+  'boom',
+  'boon',
+  'boost',
+  'boot',
+  'booth',
+  'booty',
+  'booze',
+  'bore',
+  'borer',
+  'born',
+  'boss',
+  'bot',
+  'botch',
+  'bound',
+  'bow',
+  'bowel',
+  'bowl',
+  'bowls',
+  'box',
+  'boxer',
+  'boy',
+  'bra',
+  'brace',
+  'brag',
+  'braid',
+  'brail',
+  'brain',
+  'brake',
+  'bran',
+  'brand',
+  'brass',
+  'brat',
+  'brave',
+  'bravo',
+  'brawl',
+  'brawn',
+  'bread',
+  'break',
+  'breed',
+  'brew',
+  'briar',
+  'bribe',
+  'brick',
+  'bride',
+  'brie',
+  'brief',
+  'brim',
+  'brine',
+  'brink',
+  'brit',
+  'brits',
+  'britt',
+  'broad',
+  'broil',
+  'brood',
+  'brook',
+  'broom',
+  'broth',
+  'brow',
+  'brown',
+  'brunt',
+  'brush',
+  'brute',
+  'buck',
+  'bud',
+  'buddy',
+  'budge',
+  'buff',
+  'bug',
+  'buggy',
+  'bugle',
+  'build',
+  'bulb',
+  'bulge',
+  'bulk',
+  'bull',
+  'bully',
+  'bum',
+  'bump',
+  'bun',
+  'bunch',
+  'bung',
+  'bunk',
+  'bunny',
+  'buns',
+  'bunt',
+  'buoy',
+  'bur',
+  'burn',
+  'burns',
+  'burp',
+  'burst',
+  'bus',
+  'bush',
+  'buss',
+  'bust',
+  'buy',
+  'buyer',
+  'buzz',
+  'bye',
+  'bylaw',
+  'byte',
+  'cab',
+  'cabin',
+  'cable',
+  'cabot',
+  'cache',
+  'caddy',
+  'cadet',
+  'cafe',
+  'cage',
+  'cager',
+  'cake',
+  'calf',
+  'call',
+  'calm',
+  'cam',
+  'camel',
+  'camp',
+  'can',
+  'canal',
+  'candy',
+  'cane',
+  'cap',
+  'cape',
+  'caper',
+  'car',
+  'carat',
+  'card',
+  'cards',
+  'care',
+  'caret',
+  'cargo',
+  'carp',
+  'carry',
+  'cart',
+  'case',
+  'cash',
+  'cask',
+  'cast',
+  'caste',
+  'cat',
+  'catch',
+  'caulk',
+  'cause',
+  'cave',
+  'cavil',
+  'caw',
+  'cease',
+  'cedar',
+  'cell',
+  'cello',
+  'cent',
+  'chaff',
+  'chain',
+  'chair',
+  'chalk',
+  'champ',
+  'chant',
+  'chaos',
+  'chap',
+  'chard',
+  'charm',
+  'chart',
+  'chase',
+  'chasm',
+  'chat',
+  'cheat',
+  'check',
+  'cheek',
+  'cheep',
+  'cheer',
+  'chef',
+  'chess',
+  'chest',
+  'chew',
+  'chic',
+  'chick',
+  'chief',
+  'child',
+  'chill',
+  'chime',
+  'chimp',
+  'chin',
+  'chip',
+  'chips',
+  'chirp',
+  'chit',
+  'chive',
+  'chock',
+  'choir',
+  'choke',
+  'choky',
+  'chomp',
+  'chop',
+  'chord',
+  'chore',
+  'chow',
+  'chuck',
+  'chug',
+  'chum',
+  'chump',
+  'chunk',
+  'churn',
+  'chute',
+  'cider',
+  'cigar',
+  'cinch',
+  'cite',
+  'city',
+  'clack',
+  'claim',
+  'clam',
+  'clamp',
+  'clams',
+  'clan',
+  'clang',
+  'clank',
+  'clap',
+  'clash',
+  'clasp',
+  'class',
+  'clay',
+  'clean',
+  'clear',
+  'cleat',
+  'cleft',
+  'clerk',
+  'click',
+  'cliff',
+  'climb',
+  'cling',
+  'clip',
+  'cloak',
+  'clock',
+  'clog',
+  'clone',
+  'close',
+  'clot',
+  'cloth',
+  'cloud',
+  'clout',
+  'clove',
+  'clown',
+  'club',
+  'cluck',
+  'clue',
+  'clump',
+  'clunk',
+  'coach',
+  'coal',
+  'coast',
+  'coat',
+  'cobra',
+  'cocoa',
+  'cod',
+  'code',
+  'cog',
+  'coil',
+  'coin',
+  'coke',
+  'cola',
+  'cold',
+  'colon',
+  'color',
+  'colt',
+  'coma',
+  'comb',
+  'combo',
+  'come',
+  'comet',
+  'comic',
+  'comma',
+  'conch',
+  'condo',
+  'cone',
+  'coney',
+  'conk',
+  'cook',
+  'cool',
+  'coot',
+  'cop',
+  'cope',
+  'copy',
+  'coral',
+  'cord',
+  'cords',
+  'core',
+  'cork',
+  'corn',
+  'corp',
+  'corps',
+  'cost',
+  'costs',
+  'cosy',
+  'cot',
+  'couch',
+  'cough',
+  'count',
+  'court',
+  'cove',
+  'coven',
+  'cover',
+  'cow',
+  'cowl',
+  'cows',
+  'cozy',
+  'crab',
+  'crabs',
+  'crack',
+  'craft',
+  'cramp',
+  'crane',
+  'crank',
+  'crash',
+  'crate',
+  'crawl',
+  'craze',
+  'crazy',
+  'creak',
+  'cream',
+  'cred',
+  'cree',
+  'creed',
+  'creek',
+  'creep',
+  'crepe',
+  'cress',
+  'crest',
+  'crew',
+  'crib',
+  'crime',
+  'crimp',
+  'crisp',
+  'croak',
+  'crock',
+  'crook',
+  'crop',
+  'cross',
+  'crow',
+  'crowd',
+  'crown',
+  'crud',
+  'crude',
+  'crumb',
+  'crush',
+  'crust',
+  'crux',
+  'cry',
+  'crypt',
+  'cub',
+  'cubby',
+  'cube',
+  'cubit',
+  'cue',
+  'cuff',
+  'cull',
+  'cult',
+  'cup',
+  'curb',
+  'curd',
+  'cure',
+  'curl',
+  'curry',
+  'curse',
+  'curve',
+  'cut',
+  'cyan',
+  'cycle',
+  'cynic',
+  'dab',
+  'daily',
+  'dairy',
+  'daisy',
+  'dame',
+  'damp',
+  'dance',
+  'dandy',
+  'dane',
+  'dare',
+  'dark',
+  'dart',
+  'darts',
+  'dash',
+  'data',
+  'date',
+  'dawn',
+  'day',
+  'days',
+  'daze',
+  'dead',
+  'deaf',
+  'deal',
+  'dean',
+  'dear',
+  'death',
+  'debit',
+  'debt',
+  'debut',
+  'decal',
+  'decay',
+  'deck',
+  'decor',
+  'decoy',
+  'deed',
+  'deeds',
+  'deep',
+  'deer',
+  'delay',
+  'deli',
+  'delta',
+  'demo',
+  'demon',
+  'denim',
+  'dent',
+  'depot',
+  'depth',
+  'derby',
+  'desk',
+  'detox',
+  'deuce',
+  'devil',
+  'dew',
+  'dial',
+  'diary',
+  'dibs',
+  'dice',
+  'diet',
+  'dig',
+  'digit',
+  'digs',
+  'dill',
+  'dime',
+  'diner',
+  'ding',
+  'dip',
+  'dirt',
+  'disc',
+  'disco',
+  'dish',
+  'disk',
+  'ditch',
+  'ditto',
+  'dive',
+  'diver',
+  'dock',
+  'dodge',
+  'dog',
+  'dogma',
+  'doll',
+  'dolly',
+  'dolt',
+  'dome',
+  'donor',
+  'donut',
+  'doom',
+  'door',
+  'dope',
+  'dork',
+  'dorm',
+  'dot',
+  'doubt',
+  'dough',
+  'dove',
+  'dowel',
+  'down',
+  'dozen',
+  'dozer',
+  'draft',
+  'drag',
+  'drain',
+  'drama',
+  'drape',
+  'draw',
+  'dread',
+  'dream',
+  'dress',
+  'drew',
+  'drier',
+  'drift',
+  'drill',
+  'drink',
+  'drip',
+  'drive',
+  'drone',
+  'drool',
+  'drop',
+  'drove',
+  'drug',
+  'druid',
+  'drum',
+  'dry',
+  'dryer',
+  'duck',
+  'duct',
+  'due',
+  'duel',
+  'duet',
+  'dug',
+  'dunce',
+  'dune',
+  'dunk',
+  'dusk',
+  'dust',
+  'duty',
+  'dye',
+  'dyer',
+  'dying',
+  'eager',
+  'eagle',
+  'ear',
+  'earth',
+  'ease',
+  'easel',
+  'east',
+  'eater',
+  'eats',
+  'echo',
+  'edge',
+  'eel',
+  'egg',
+  'eggs',
+  'ego',
+  'eight',
+  'elbow',
+  'elder',
+  'elect',
+  'elf',
+  'elite',
+  'elk',
+  'elm',
+  'elves',
+  'email',
+  'ember',
+  'empty',
+  'emu',
+  'end',
+  'enemy',
+  'entry',
+  'envy',
+  'epic',
+  'epoxy',
+  'equal',
+  'era',
+  'error',
+  'essay',
+  'eve',
+  'even',
+  'event',
+  'evil',
+  'exam',
+  'exile',
+  'exit',
+  'extra',
+  'eye',
+  'eyes',
+  'fable',
+  'face',
+  'facet',
+  'fact',
+  'fad',
+  'fade',
+  'faint',
+  'fair',
+  'fairy',
+  'faith',
+  'fake',
+  'fall',
+  'falls',
+  'fame',
+  'fan',
+  'fancy',
+  'fang',
+  'far',
+  'farce',
+  'fare',
+  'farm',
+  'fast',
+  'fat',
+  'fate',
+  'fault',
+  'favor',
+  'fawn',
+  'fax',
+  'fear',
+  'feast',
+  'feat',
+  'fed',
+  'fee',
+  'feed',
+  'feel',
+  'felt',
+  'femur',
+  'fence',
+  'fern',
+  'ferry',
+  'fetch',
+  'feud',
+  'fever',
+  'few',
+  'fib',
+  'fiber',
+  'field',
+  'fiend',
+  'fifth',
+  'fifty',
+  'fig',
+  'fight',
+  'file',
+  'filet',
+  'fill',
+  'film',
+  'filth',
+  'final',
+  'finch',
+  'find',
+  'fine',
+  'fire',
+  'firm',
+  'first',
+  'fish',
+  'fist',
+  'fit',
+  'five',
+  'fiver',
+  'fives',
+  'fix',
+  'fixer',
+  'fizz',
+  'flag',
+  'flair',
+  'flak',
+  'flake',
+  'flame',
+  'flank',
+  'flap',
+  'flaps',
+  'flare',
+  'flash',
+  'flask',
+  'flat',
+  'flats',
+  'flaw',
+  'flea',
+  'fleet',
+  'flesh',
+  'flex',
+  'flick',
+  'flier',
+  'flies',
+  'fling',
+  'flint',
+  'flip',
+  'flirt',
+  'float',
+  'flock',
+  'flood',
+  'floor',
+  'flop',
+  'floss',
+  'flour',
+  'flow',
+  'flu',
+  'flub',
+  'fluff',
+  'fluid',
+  'fluke',
+  'flume',
+  'flush',
+  'flute',
+  'flux',
+  'fly',
+  'flyer',
+  'foam',
+  'focus',
+  'fog',
+  'foil',
+  'fold',
+  'folk',
+  'folks',
+  'folly',
+  'font',
+  'food',
+  'fool',
+  'foot',
+  'force',
+  'forge',
+  'fork',
+  'form',
+  'fort',
+  'forth',
+  'forty',
+  'forum',
+  'foul',
+  'found',
+  'four',
+  'fowl',
+  'fox',
+  'foyer',
+  'frail',
+  'frame',
+  'frat',
+  'fraud',
+  'fray',
+  'freak',
+  'free',
+  'freon',
+  'fret',
+  'friar',
+  'fries',
+  'frill',
+  'frisk',
+  'frizz',
+  'frog',
+  'front',
+  'frost',
+  'froth',
+  'frown',
+  'fruit',
+  'fry',
+  'fryer',
+  'fudge',
+  'fuel',
+  'full',
+  'fume',
+  'fumes',
+  'fun',
+  'fund',
+  'funds',
+  'fungi',
+  'funk',
+  'funny',
+  'fur',
+  'fury',
+  'fuse',
+  'fuss',
+  'futon',
+  'fuze',
+  'fuzz',
+  'gag',
+  'gage',
+  'gain',
+  'game',
+  'gamma',
+  'gap',
+  'gape',
+  'gas',
+  'gash',
+  'gasp',
+  'gate',
+  'gates',
+  'gator',
+  'gauge',
+  'gavel',
+  'gawk',
+  'gaze',
+  'gear',
+  'gecko',
+  'geek',
+  'gel',
+  'gem',
+  'gene',
+  'genie',
+  'genoa',
+  'genre',
+  'gent',
+  'germ',
+  'ghost',
+  'ghoul',
+  'giant',
+  'gift',
+  'gild',
+  'gimp',
+  'gin',
+  'gipsy',
+  'girl',
+  'gist',
+  'give',
+  'given',
+  'giver',
+  'gizmo',
+  'glad',
+  'glade',
+  'gland',
+  'glans',
+  'glare',
+  'glass',
+  'glaze',
+  'gleam',
+  'glee',
+  'glide',
+  'glint',
+  'globe',
+  'gloom',
+  'glory',
+  'gloss',
+  'glove',
+  'glow',
+  'glue',
+  'gnat',
+  'gnome',
+  'goal',
+  'goat',
+  'going',
+  'gold',
+  'golem',
+  'golf',
+  'goner',
+  'goo',
+  'good',
+  'goof',
+  'goofy',
+  'goon',
+  'goose',
+  'goth',
+  'gouge',
+  'gown',
+  'grab',
+  'grace',
+  'grad',
+  'grade',
+  'graft',
+  'grail',
+  'grain',
+  'gram',
+  'grand',
+  'grant',
+  'grape',
+  'graph',
+  'grasp',
+  'grass',
+  'grate',
+  'grave',
+  'gravy',
+  'gray',
+  'graze',
+  'great',
+  'greed',
+  'green',
+  'grey',
+  'grid',
+  'grief',
+  'grill',
+  'grime',
+  'grin',
+  'grind',
+  'grip',
+  'gripe',
+  'grit',
+  'grits',
+  'groan',
+  'groom',
+  'gross',
+  'group',
+  'grove',
+  'growl',
+  'grub',
+  'gruel',
+  'grump',
+  'grunt',
+  'guard',
+  'guess',
+  'guest',
+  'guide',
+  'guild',
+  'guilt',
+  'gulch',
+  'gulf',
+  'gull',
+  'gulp',
+  'gum',
+  'gun',
+  'guppy',
+  'guru',
+  'gush',
+  'gust',
+  'gut',
+  'guts',
+  'guy',
+  'gym',
+  'habit',
+  'hack',
+  'hag',
+  'hail',
+  'hair',
+  'half',
+  'hall',
+  'halo',
+  'halt',
+  'ham',
+  'hand',
+  'hands',
+  'handy',
+  'hang',
+  'hare',
+  'harp',
+  'hash',
+  'haste',
+  'hat',
+  'hatch',
+  'hate',
+  'hater',
+  'haunt',
+  'have',
+  'haven',
+  'havoc',
+  'hawk',
+  'hay',
+  'haze',
+  'hazel',
+  'head',
+  'heap',
+  'heaps',
+  'heart',
+  'heat',
+  'heavy',
+  'hedge',
+  'heed',
+  'heel',
+  'heft',
+  'heir',
+  'helix',
+  'hell',
+  'hello',
+  'helm',
+  'help',
+  'hem',
+  'hemp',
+  'hen',
+  'herb',
+  'herd',
+  'here',
+  'hero',
+  'hex',
+  'hick',
+  'hide',
+  'high',
+  'hike',
+  'hiker',
+  'hill',
+  'hilt',
+  'hind',
+  'hinge',
+  'hint',
+  'hip',
+  'hippo',
+  'hippy',
+  'hire',
+  'hiss',
+  'hit',
+  'hitch',
+  'hive',
+  'hives',
+  'hoagy',
+  'hoard',
+  'hoax',
+  'hob',
+  'hobby',
+  'hobo',
+  'hog',
+  'hoist',
+  'hold',
+  'hole',
+  'home',
+  'honey',
+  'honk',
+  'honor',
+  'hoof',
+  'hook',
+  'hooks',
+  'hoop',
+  'hoops',
+  'hoot',
+  'hop',
+  'hope',
+  'hops',
+  'horde',
+  'horn',
+  'horse',
+  'hose',
+  'host',
+  'hotel',
+  'hound',
+  'hour',
+  'hours',
+  'house',
+  'howl',
+  'hub',
+  'hue',
+  'huff',
+  'hug',
+  'hula',
+  'hulk',
+  'hull',
+  'hum',
+  'human',
+  'humor',
+  'hump',
+  'humus',
+  'hunch',
+  'hunk',
+  'hunt',
+  'hurl',
+  'hurry',
+  'hurt',
+  'hush',
+  'husk',
+  'husky',
+  'hut',
+  'hydra',
+  'hyena',
+  'hymn',
+  'hype',
+  'ibis',
+  'ice',
+  'icing',
+  'icon',
+  'idea',
+  'ideal',
+  'idiom',
+  'idiot',
+  'idle',
+  'idler',
+  'idol',
+  'igloo',
+  'iglu',
+  'ill',
+  'image',
+  'imp',
+  'inch',
+  'index',
+  'info',
+  'ingot',
+  'ink',
+  'inlet',
+  'inn',
+  'input',
+  'intro',
+  'ion',
+  'iris',
+  'iron',
+  'irony',
+  'isle',
+  'issue',
+  'itch',
+  'ivory',
+  'ivy',
+  'jab',
+  'jack',
+  'jacks',
+  'jail',
+  'jam',
+  'jamb',
+  'jar',
+  'java',
+  'jaw',
+  'jay',
+  'jazz',
+  'jean',
+  'jeans',
+  'jeep',
+  'jeer',
+  'jello',
+  'jelly',
+  'jest',
+  'jet',
+  'jetty',
+  'jewel',
+  'jig',
+  'jive',
+  'job',
+  'jock',
+  'jog',
+  'join',
+  'joint',
+  'joist',
+  'joke',
+  'joker',
+  'jolly',
+  'jolt',
+  'joust',
+  'joy',
+  'judge',
+  'jug',
+  'juice',
+  'juke',
+  'jump',
+  'junk',
+  'junky',
+  'juror',
+  'jury',
+  'kale',
+  'kayak',
+  'kazoo',
+  'kebab',
+  'keen',
+  'keep',
+  'keg',
+  'kelp',
+  'key',
+  'kick',
+  'kid',
+  'kiddy',
+  'kiln',
+  'kilo',
+  'kilt',
+  'kin',
+  'kind',
+  'king',
+  'kiss',
+  'kit',
+  'kite',
+  'kitty',
+  'kiwi',
+  'knack',
+  'knee',
+  'kneel',
+  'knell',
+  'knife',
+  'knit',
+  'knob',
+  'knock',
+  'knot',
+  'know',
+  'koala',
+  'krill',
+  'lab',
+  'label',
+  'labor',
+  'lace',
+  'lack',
+  'lad',
+  'ladle',
+  'lady',
+  'lag',
+  'lair',
+  'lake',
+  'lamb',
+  'lame',
+  'lamp',
+  'lance',
+  'land',
+  'lane',
+  'lap',
+  'lapel',
+  'lapse',
+  'lard',
+  'large',
+  'larva',
+  'laser',
+  'lash',
+  'lass',
+  'lasso',
+  'last',
+  'lat',
+  'latch',
+  'latex',
+  'lathe',
+  'latte',
+  'laugh',
+  'lava',
+  'law',
+  'lawn',
+  'laws',
+  'lay',
+  'layer',
+  'layup',
+  'leach',
+  'lead',
+  'leaf',
+  'leak',
+  'lean',
+  'leap',
+  'lear',
+  'lease',
+  'leash',
+  'least',
+  'leave',
+  'ledge',
+  'leech',
+  'leeds',
+  'leek',
+  'leer',
+  'left',
+  'lefty',
+  'leg',
+  'lego',
+  'legs',
+  'lemon',
+  'lemur',
+  'lens',
+  'lent',
+  'let',
+  'level',
+  'lever',
+  'liar',
+  'libel',
+  'lick',
+  'lid',
+  'lie',
+  'lied',
+  'life',
+  'lift',
+  'light',
+  'like',
+  'lilac',
+  'limb',
+  'limbo',
+  'lime',
+  'limit',
+  'limp',
+  'line',
+  'linen',
+  'liner',
+  'link',
+  'links',
+  'lint',
+  'lion',
+  'lip',
+  'lisp',
+  'list',
+  'lit',
+  'liter',
+  'liver',
+  'llama',
+  'loach',
+  'load',
+  'loads',
+  'loaf',
+  'loan',
+  'lob',
+  'lobby',
+  'lobe',
+  'local',
+  'lock',
+  'lodge',
+  'loft',
+  'log',
+  'logic',
+  'logo',
+  'loner',
+  'look',
+  'loom',
+  'loon',
+  'loony',
+  'loop',
+  'loot',
+  'lord',
+  'loser',
+  'loss',
+  'lost',
+  'lot',
+  'lots',
+  'lotto',
+  'lotus',
+  'love',
+  'lover',
+  'low',
+  'lower',
+  'luck',
+  'lump',
+  'lunch',
+  'lung',
+  'lure',
+  'lush',
+  'lying',
+  'mace',
+  'macro',
+  'madam',
+  'mafia',
+  'magi',
+  'magic',
+  'magma',
+  'maid',
+  'mail',
+  'main',
+  'major',
+  'maker',
+  'male',
+  'malt',
+  'mam',
+  'mama',
+  'mamba',
+  'mambo',
+  'mamma',
+  'man',
+  'mane',
+  'mango',
+  'mania',
+  'manor',
+  'map',
+  'maple',
+  'march',
+  'mare',
+  'mark',
+  'marks',
+  'mars',
+  'marsh',
+  'mash',
+  'mask',
+  'mass',
+  'mast',
+  'mat',
+  'match',
+  'mate',
+  'mates',
+  'math',
+  'maths',
+  'max',
+  'maxim',
+  'may',
+  'mayo',
+  'mayor',
+  'maze',
+  'meal',
+  'mean',
+  'means',
+  'meat',
+  'medal',
+  'medic',
+  'meet',
+  'meld',
+  'melee',
+  'melon',
+  'melt',
+  'memo',
+  'men',
+  'mend',
+  'menu',
+  'meow',
+  'mercy',
+  'merit',
+  'mesh',
+  'mess',
+  'metal',
+  'meter',
+  'meth',
+  'metro',
+  'might',
+  'mile',
+  'milk',
+  'mill',
+  'mills',
+  'mimer',
+  'mimic',
+  'min',
+  'mince',
+  'mind',
+  'mine',
+  'miner',
+  'mini',
+  'mink',
+  'minor',
+  'mint',
+  'minus',
+  'miser',
+  'miss',
+  'mist',
+  'mite',
+  'miter',
+  'mitt',
+  'mix',
+  'mixer',
+  'moan',
+  'moat',
+  'mob',
+  'mocha',
+  'mock',
+  'mod',
+  'modal',
+  'mode',
+  'model',
+  'modem',
+  'mogul',
+  'mojo',
+  'molar',
+  'mold',
+  'mole',
+  'molt',
+  'mom',
+  'momma',
+  'mommy',
+  'money',
+  'monk',
+  'month',
+  'moo',
+  'mooch',
+  'mood',
+  'moody',
+  'moon',
+  'moose',
+  'mop',
+  'mope',
+  'moped',
+  'moral',
+  'morse',
+  'moss',
+  'motel',
+  'moth',
+  'motor',
+  'motto',
+  'mould',
+  'mound',
+  'mount',
+  'mouse',
+  'mouth',
+  'move',
+  'mover',
+  'movie',
+  'mow',
+  'mucus',
+  'mud',
+  'muff',
+  'mug',
+  'mulch',
+  'mule',
+  'mum',
+  'mummy',
+  'munch',
+  'mural',
+  'muse',
+  'mush',
+  'music',
+  'musk',
+  'must',
+  'mute',
+  'mutt',
+  'mylar',
+  'nacho',
+  'name',
+  'namer',
+  'names',
+  'nanna',
+  'nap',
+  'nasal',
+  'navy',
+  'neck',
+  'need',
+  'needy',
+  'neon',
+  'nepal',
+  'nerd',
+  'nerve',
+  'nest',
+  'net',
+  'news',
+  'newt',
+  'nick',
+  'niece',
+  'night',
+  'nine',
+  'niner',
+  'ninja',
+  'ninth',
+  'noble',
+  'nod',
+  'node',
+  'noise',
+  'nomad',
+  'none',
+  'nook',
+  'noon',
+  'noose',
+  'north',
+  'nose',
+  'notch',
+  'note',
+  'noun',
+  'nudge',
+  'nuke',
+  'nun',
+  'nurse',
+  'nut',
+  'nylon',
+  'oaf',
+  'oak',
+  'oar',
+  'oasis',
+  'oat',
+  'oates',
+  'oath',
+  'ocean',
+  'octet',
+  'odds',
+  'ode',
+  'odor',
+  'offer',
+  'ogre',
+  'oil',
+  'oiler',
+  'oink',
+  'okay',
+  'old',
+  'oldie',
+  'olive',
+  'omega',
+  'omen',
+  'one',
+  'onion',
+  'onset',
+  'ooze',
+  'open',
+  'optic',
+  'oral',
+  'orange',
+  'orb',
+  'orbit',
+  'orca',
+  'order',
+  'ore',
+  'oreo',
+  'organ',
+  'ounce',
+  'out',
+  'oval',
+  'oven',
+  'over',
+  'owl',
+  'owner',
+  'oxbow',
+  'oxen',
+  'ozone',
+  'pace',
+  'pacer',
+  'pack',
+  'pact',
+  'pad',
+  'page',
+  'pager',
+  'pail',
+  'pain',
+  'pains',
+  'paint',
+  'pair',
+  'pal',
+  'pale',
+  'palm',
+  'pan',
+  'panda',
+  'pane',
+  'panel',
+  'panic',
+  'pansy',
+  'pant',
+  'pants',
+  'papa',
+  'paper',
+  'par',
+  'park',
+  'parks',
+  'part',
+  'parts',
+  'party',
+  'pass',
+  'past',
+  'pasta',
+  'paste',
+  'pat',
+  'patch',
+  'path',
+  'patio',
+  'pause',
+  'pave',
+  'paw',
+  'pawn',
+  'pay',
+  'payer',
+  'peace',
+  'peach',
+  'peak',
+  'pear',
+  'pearl',
+  'pecan',
+  'pedal',
+  'peek',
+  'peel',
+  'peer',
+  'peg',
+  'pelt',
+  'pen',
+  'penny',
+  'perch',
+  'peril',
+  'perk',
+  'pesto',
+  'pet',
+  'petal',
+  'petty',
+  'phase',
+  'phone',
+  'photo',
+  'piano',
+  'pick',
+  'pie',
+  'piece',
+  'pier',
+  'pig',
+  'piggy',
+  'pigmy',
+  'pike',
+  'pile',
+  'piles',
+  'pill',
+  'pimp',
+  'pin',
+  'pinch',
+  'pine',
+  'ping',
+  'pink',
+  'pinky',
+  'pinot',
+  'pint',
+  'pipe',
+  'pit',
+  'pita',
+  'pitch',
+  'pitt',
+  'pity',
+  'pivot',
+  'pixel',
+  'pizza',
+  'place',
+  'plaid',
+  'plain',
+  'plan',
+  'plane',
+  'plank',
+  'plant',
+  'plate',
+  'play',
+  'plaza',
+  'plea',
+  'plier',
+  'plot',
+  'plow',
+  'ploy',
+  'pluck',
+  'plug',
+  'plum',
+  'plumb',
+  'plume',
+  'plump',
+  'plus',
+  'plush',
+  'plyer',
+  'pod',
+  'poem',
+  'poet',
+  'point',
+  'poke',
+  'poker',
+  'pole',
+  'poll',
+  'polls',
+  'pond',
+  'pong',
+  'pony',
+  'pooch',
+  'poof',
+  'pool',
+  'poor',
+  'pop',
+  'poppy',
+  'porch',
+  'pore',
+  'pork',
+  'port',
+  'pose',
+  'poser',
+  'post',
+  'pot',
+  'pouch',
+  'pound',
+  'power',
+  'prank',
+  'prawn',
+  'press',
+  'prey',
+  'price',
+  'pride',
+  'prime',
+  'prism',
+  'prize',
+  'pro',
+  'probe',
+  'prom',
+  'promo',
+  'proof',
+  'prop',
+  'props',
+  'prose',
+  'prowl',
+  'prune',
+  'pry',
+  'pub',
+  'puck',
+  'puff',
+  'pug',
+  'pull',
+  'pulp',
+  'pulse',
+  'puma',
+  'pump',
+  'pun',
+  'punch',
+  'punk',
+  'punks',
+  'punt',
+  'pup',
+  'pupil',
+  'puppy',
+  'purge',
+  'purse',
+  'push',
+  'put',
+  'putt',
+  'putty',
+  'quack',
+  'quad',
+  'quake',
+  'qualm',
+  'quart',
+  'queen',
+  'query',
+  'quest',
+  'quick',
+  'quid',
+  'quiet',
+  'quilt',
+  'quirk',
+  'quirt',
+  'quiz',
+  'quota',
+  'quote',
+  'race',
+  'racer',
+  'rad',
+  'radar',
+  'radio',
+  'raft',
+  'rafts',
+  'rag',
+  'rage',
+  'raid',
+  'rail',
+  'rails',
+  'rain',
+  'raise',
+  'rake',
+  'rally',
+  'ram',
+  'ramp',
+  'ranch',
+  'range',
+  'rank',
+  'rant',
+  'rap',
+  'rapid',
+  'rash',
+  'rat',
+  'rate',
+  'rates',
+  'ratio',
+  'raw',
+  'ray',
+  'razor',
+  'razz',
+  'reach',
+  'read',
+  'ready',
+  'real',
+  'realm',
+  'ream',
+  'rear',
+  'rebel',
+  'red',
+  'reed',
+  'reef',
+  'reek',
+  'reel',
+  'reign',
+  'relay',
+  'relic',
+  'rent',
+  'reply',
+  'reset',
+  'resin',
+  'rest',
+  'retro',
+  'revel',
+  'rhino',
+  'rhyme',
+  'rib',
+  'rice',
+  'ricer',
+  'rich',
+  'ride',
+  'rider',
+  'ridge',
+  'riff',
+  'rifle',
+  'rift',
+  'rig',
+  'right',
+  'rim',
+  'rind',
+  'ring',
+  'rings',
+  'rink',
+  'rinse',
+  'riot',
+  'rip',
+  'rise',
+  'riser',
+  'risk',
+  'rite',
+  'rival',
+  'river',
+  'roach',
+  'road',
+  'roads',
+  'roar',
+  'roast',
+  'robe',
+  'robin',
+  'robot',
+  'rock',
+  'rod',
+  'rodeo',
+  'rogue',
+  'role',
+  'roll',
+  'room',
+  'rooms',
+  'roost',
+  'root',
+  'roots',
+  'rope',
+  'rose',
+  'rot',
+  'rotor',
+  'rouge',
+  'rough',
+  'round',
+  'route',
+  'rover',
+  'row',
+  'rowdy',
+  'rower',
+  'royal',
+  'rub',
+  'rube',
+  'ruby',
+  'rug',
+  'rugby',
+  'ruin',
+  'rule',
+  'ruler',
+  'rum',
+  'rummy',
+  'rumor',
+  'run',
+  'rune',
+  'rung',
+  'runt',
+  'ruse',
+  'rush',
+  'rust',
+  'rut',
+  'saber',
+  'safe',
+  'sag',
+  'saga',
+  'sage',
+  'sail',
+  'saint',
+  'salad',
+  'sale',
+  'salem',
+  'sales',
+  'salon',
+  'salsa',
+  'salt',
+  'same',
+  'sand',
+  'sands',
+  'sang',
+  'sash',
+  'sass',
+  'sauce',
+  'sauna',
+  'save',
+  'saver',
+  'savor',
+  'saw',
+  'say',
+  'scale',
+  'scan',
+  'scar',
+  'scare',
+  'scarf',
+  'scene',
+  'scent',
+  'scold',
+  'scone',
+  'scoop',
+  'scope',
+  'score',
+  'scorn',
+  'scout',
+  'scrap',
+  'sea',
+  'seal',
+  'seam',
+  'seat',
+  'seats',
+  'sect',
+  'sedan',
+  'see',
+  'seed',
+  'seek',
+  'seer',
+  'self',
+  'sell',
+  'sense',
+  'serum',
+  'serve',
+  'servo',
+  'set',
+  'setup',
+  'seven',
+  'shack',
+  'shade',
+  'shake',
+  'sham',
+  'shame',
+  'shank',
+  'shape',
+  'shard',
+  'share',
+  'shark',
+  'sharp',
+  'shave',
+  'shawl',
+  'shed',
+  'sheep',
+  'sheet',
+  'shelf',
+  'shell',
+  'shift',
+  'shill',
+  'shim',
+  'shin',
+  'ship',
+  'shirt',
+  'shoe',
+  'shoes',
+  'shop',
+  'shore',
+  'shot',
+  'shove',
+  'show',
+  'shred',
+  'shrub',
+  'shrug',
+  'shy',
+  'sick',
+  'siege',
+  'sigh',
+  'sight',
+  'sign',
+  'silk',
+  'silks',
+  'silly',
+  'silo',
+  'sin',
+  'sink',
+  'sinus',
+  'sip',
+  'sir',
+  'siren',
+  'six',
+  'sixer',
+  'sixth',
+  'sixty',
+  'size',
+  'ski',
+  'skid',
+  'skier',
+  'skill',
+  'skim',
+  'skin',
+  'skip',
+  'skirt',
+  'skit',
+  'skull',
+  'skunk',
+  'sky',
+  'slab',
+  'slack',
+  'slag',
+  'slain',
+  'slam',
+  'slang',
+  'slant',
+  'slap',
+  'slash',
+  'slate',
+  'slave',
+  'slaw',
+  'sled',
+  'sleep',
+  'sleet',
+  'slew',
+  'slews',
+  'slice',
+  'slick',
+  'slide',
+  'slime',
+  'sling',
+  'slip',
+  'slit',
+  'slob',
+  'slope',
+  'slot',
+  'sloth',
+  'slug',
+  'slum',
+  'slump',
+  'slur',
+  'slush',
+  'smack',
+  'small',
+  'smart',
+  'smash',
+  'smear',
+  'smell',
+  'smelt',
+  'smile',
+  'smirk',
+  'smith',
+  'smock',
+  'smog',
+  'smoke',
+  'snack',
+  'snag',
+  'snail',
+  'snake',
+  'snap',
+  'snare',
+  'snarl',
+  'sneak',
+  'sniff',
+  'snipe',
+  'snore',
+  'snort',
+  'snot',
+  'snow',
+  'snug',
+  'soak',
+  'soap',
+  'soar',
+  'sob',
+  'sock',
+  'sofa',
+  'softy',
+  'soil',
+  'sole',
+  'solid',
+  'son',
+  'sonar',
+  'song',
+  'sonny',
+  'soot',
+  'sooth',
+  'sore',
+  'sort',
+  'soul',
+  'sound',
+  'soup',
+  'sour',
+  'south',
+  'spa',
+  'space',
+  'spade',
+  'spam',
+  'span',
+  'spar',
+  'spare',
+  'spark',
+  'spasm',
+  'spat',
+  'spawn',
+  'speed',
+  'spell',
+  'spelt',
+  'spice',
+  'spike',
+  'spill',
+  'spin',
+  'spit',
+  'spite',
+  'splat',
+  'split',
+  'spoil',
+  'spoke',
+  'spoof',
+  'spook',
+  'spool',
+  'spoon',
+  'spore',
+  'sport',
+  'spot',
+  'spots',
+  'spout',
+  'spray',
+  'spree',
+  'spud',
+  'spur',
+  'spurt',
+  'spy',
+  'squat',
+  'squid',
+  'stab',
+  'stack',
+  'staff',
+  'stag',
+  'stage',
+  'stain',
+  'stair',
+  'stake',
+  'stalk',
+  'stall',
+  'stamp',
+  'stand',
+  'star',
+  'stare',
+  'start',
+  'stash',
+  'state',
+  'stay',
+  'stays',
+  'steak',
+  'steal',
+  'steam',
+  'steed',
+  'steel',
+  'steer',
+  'stem',
+  'step',
+  'steps',
+  'stern',
+  'stew',
+  'stick',
+  'stiff',
+  'still',
+  'stilt',
+  'sting',
+  'stink',
+  'stint',
+  'stir',
+  'stock',
+  'stoic',
+  'stomp',
+  'stone',
+  'stool',
+  'stoop',
+  'stop',
+  'stops',
+  'store',
+  'stork',
+  'storm',
+  'story',
+  'stove',
+  'strap',
+  'straw',
+  'stray',
+  'strip',
+  'strum',
+  'strut',
+  'stub',
+  'stud',
+  'study',
+  'stuff',
+  'stump',
+  'stunt',
+  'style',
+  'sub',
+  'suds',
+  'sugar',
+  'suit',
+  'suite',
+  'sum',
+  'sumer',
+  'sun',
+  'sung',
+  'super',
+  'surf',
+  'surge',
+  'sushi',
+  'sutra',
+  'swab',
+  'swag',
+  'swamp',
+  'swan',
+  'swap',
+  'swarm',
+  'sway',
+  'sweat',
+  'sweep',
+  'sweet',
+  'swell',
+  'swift',
+  'swim',
+  'swine',
+  'swing',
+  'swipe',
+  'swirl',
+  'swish',
+  'syrup',
+  'table',
+  'tack',
+  'taco',
+  'tact',
+  'tad',
+  'taffy',
+  'tag',
+  'tail',
+  'tails',
+  'take',
+  'taker',
+  'tale',
+  'talk',
+  'talks',
+  'tall',
+  'tally',
+  'talon',
+  'tan',
+  'tank',
+  'tap',
+  'tape',
+  'taps',
+  'tar',
+  'tarp',
+  'tart',
+  'task',
+  'taste',
+  'taunt',
+  'tax',
+  'taxer',
+  'taxi',
+  'taxis',
+  'tea',
+  'teach',
+  'teal',
+  'team',
+  'tear',
+  'tears',
+  'tease',
+  'teen',
+  'teens',
+  'teeth',
+  'tell',
+  'temp',
+  'tempo',
+  'ten',
+  'tense',
+  'tent',
+  'tenth',
+  'term',
+  'terms',
+  'test',
+  'text',
+  'thaw',
+  'theft',
+  'theme',
+  'then',
+  'there',
+  'theta',
+  'thick',
+  'thief',
+  'thigh',
+  'thing',
+  'think',
+  'third',
+  'thorn',
+  'three',
+  'throw',
+  'thud',
+  'thug',
+  'thumb',
+  'tick',
+  'tide',
+  'tidy',
+  'tie',
+  'tier',
+  'tiger',
+  'tilde',
+  'tile',
+  'till',
+  'time',
+  'timer',
+  'times',
+  'timid',
+  'tin',
+  'tint',
+  'tip',
+  'tire',
+  'titan',
+  'title',
+  'toad',
+  'toady',
+  'toast',
+  'today',
+  'toe',
+  'toil',
+  'token',
+  'toll',
+  'tomb',
+  'tome',
+  'ton',
+  'tone',
+  'toner',
+  'tongs',
+  'tonic',
+  'tons',
+  'tool',
+  'toon',
+  'toot',
+  'tooth',
+  'top',
+  'topic',
+  'torch',
+  'torso',
+  'toss',
+  'total',
+  'tote',
+  'totem',
+  'touch',
+  'tough',
+  'tour',
+  'tours',
+  'tow',
+  'towel',
+  'tower',
+  'town',
+  'towny',
+  'toxin',
+  'toy',
+  'trace',
+  'track',
+  'trade',
+  'trail',
+  'train',
+  'trait',
+  'trap',
+  'trash',
+  'tray',
+  'tread',
+  'treat',
+  'tree',
+  'trek',
+  'trend',
+  'triad',
+  'trial',
+  'trick',
+  'trim',
+  'trio',
+  'trip',
+  'troll',
+  'troop',
+  'trot',
+  'trout',
+  'truce',
+  'truck',
+  'true',
+  'trump',
+  'trunk',
+  'trust',
+  'truth',
+  'try',
+  'tub',
+  'tuba',
+  'tube',
+  'tuck',
+  'tug',
+  'tulip',
+  'tummy',
+  'tumor',
+  'tuna',
+  'tune',
+  'tuner',
+  'tunic',
+  'turf',
+  'turn',
+  'tush',
+  'tusk',
+  'tutor',
+  'twine',
+  'twins',
+  'twirl',
+  'twist',
+  'two',
+  'tying',
+  'type',
+  'typo',
+  'udder',
+  'ulcer',
+  'uncle',
+  'union',
+  'unit',
+  'unity',
+  'upper',
+  'upset',
+  'urn',
+  'usage',
+  'use',
+  'user',
+  'usher',
+  'using',
+  'valet',
+  'valor',
+  'value',
+  'valve',
+  'van',
+  'vase',
+  'vat',
+  'vault',
+  'vegan',
+  'veil',
+  'vein',
+  'venom',
+  'vent',
+  'venue',
+  'verb',
+  'verge',
+  'vest',
+  'vet',
+  'vial',
+  'vibe',
+  'vibes',
+  'vice',
+  'video',
+  'view',
+  'vigil',
+  'vine',
+  'vinyl',
+  'viola',
+  'viper',
+  'virgo',
+  'virus',
+  'visit',
+  'visor',
+  'vista',
+  'vocal',
+  'vodka',
+  'vogue',
+  'voice',
+  'void',
+  'volt',
+  'vote',
+  'voter',
+  'vow',
+  'vowel',
+  'wacko',
+  'wad',
+  'wade',
+  'wader',
+  'wads',
+  'wafer',
+  'waft',
+  'wag',
+  'wage',
+  'wager',
+  'wages',
+  'wagon',
+  'wail',
+  'wain',
+  'waist',
+  'wait',
+  'wake',
+  'walk',
+  'wall',
+  'waltz',
+  'wane',
+  'want',
+  'war',
+  'ward',
+  'ware',
+  'warp',
+  'wart',
+  'wash',
+  'wasp',
+  'waste',
+  'watch',
+  'water',
+  'watt',
+  'watts',
+  'wave',
+  'waver',
+  'wax',
+  'way',
+  'ways',
+  'wear',
+  'weave',
+  'web',
+  'wed',
+  'wedge',
+  'week',
+  'weird',
+  'well',
+  'wells',
+  'welsh',
+  'west',
+  'wet',
+  'whack',
+  'whale',
+  'wharf',
+  'wheat',
+  'wheel',
+  'whey',
+  'whiff',
+  'while',
+  'whim',
+  'whip',
+  'whirl',
+  'whisk',
+  'white',
+  'who',
+  'whole',
+  'whore',
+  'why',
+  'wick',
+  'widow',
+  'width',
+  'wife',
+  'wig',
+  'wild',
+  'will',
+  'wilt',
+  'wimp',
+  'win',
+  'wince',
+  'winch',
+  'wind',
+  'wine',
+  'wing',
+  'wings',
+  'wink',
+  'wipe',
+  'wiper',
+  'wire',
+  'wise',
+  'wish',
+  'wit',
+  'witch',
+  'wits',
+  'woe',
+  'wolf',
+  'woman',
+  'womb',
+  'won',
+  'wood',
+  'woods',
+  'woof',
+  'wool',
+  'word',
+  'words',
+  'work',
+  'works',
+  'world',
+  'worm',
+  'worry',
+  'worse',
+  'worst',
+  'wort',
+  'worth',
+  'wound',
+  'wow',
+  'wrack',
+  'wrap',
+  'wrath',
+  'wreck',
+  'wring',
+  'wrist',
+  'wrong',
+  'yam',
+  'yard',
+  'yarn',
+  'yawn',
+  'yay',
+  'year',
+  'years',
+  'yeast',
+  'yell',
+  'yes',
+  'yeti',
+  'yield',
+  'yoga',
+  'yolk',
+  'young',
+  'youth',
+  'zap',
+  'zebra',
+  'zinc',
+  'zing',
+  'zip',
+  'zit',
+  'zone',
+  'zoo',
+  'zoom',
+  'zero',
+  'zany',
+  'whir',
+  'welt',
+  'whig',
+  'wand',
+  'twin',
+  'tribe',
+  'tilt',
+  'sword',
+  'spine',
+  'spear',
+  'site',
+  'shock',
+  'sent',
 ]
 
+
 def memorable_id():
-  return f"{random.choice(adjectives)}-{random.choice(adjectives)}-{random.choice(nouns)}"
+  return (
+    f'{random.choice(adjectives)}-{random.choice(adjectives)}-{random.choice(nouns)}'
+  )
diff --git a/yann/utils/profile.py b/yann/utils/profile.py
index d0e5ea3..cd7ce71 100644
--- a/yann/utils/profile.py
+++ b/yann/utils/profile.py
@@ -1,28 +1,35 @@
-from torch.autograd.profiler import profile
-from typing import Union, Tuple, Callable
+from typing import Callable, Tuple, Union
+
 import torch
+from torch.autograd.profiler import profile
+
 from .timer import Timer
 
 # TODO:
 # - https://github.com/pytorch/pytorch/issues/3749#issuecomment-374006211
 # - https://pytorch.org/docs/stable/bottleneck.html
 
+
 def param_count(model):
-  return sum(p.numel() for p in (model.parameters() if isinstance(model, torch.nn.Module) else model))
+  return sum(
+    p.numel()
+    for p in (model.parameters() if isinstance(model, torch.nn.Module) else model)
+  )
 
 
 def profile_module(
-    module,
-    input,
-    warmup=10,
-    iterations=20,
-    sync=True,
-    timer=None,
-    task_name='iteration',
-    jit=False
+  module,
+  input,
+  warmup=10,
+  iterations=20,
+  sync=True,
+  timer=None,
+  task_name='iteration',
+  jit=False,
 ):
   if jit:
     import torch.jit
+
     module = torch.jit.trace(module, input)
 
   for n in range(warmup):
diff --git a/yann/utils/tensor.py b/yann/utils/tensor.py
index 9182eba..b950751 100644
--- a/yann/utils/tensor.py
+++ b/yann/utils/tensor.py
@@ -1,5 +1,9 @@
+import dataclasses
+from typing import Any, Union
+
 import torch
 
+
 def weighted_sum(tensors, weights):
   if len(tensors) < 2:
     raise ValueError('must pass at least 2 tensors')
@@ -9,11 +13,22 @@ def weighted_sum(tensors, weights):
   return s
 
 
-def one_hot(targets: torch.Tensor, num_classes=None, device=None, dtype=None, normalize=False):
+def one_hot(
+  targets: torch.Tensor,
+  num_classes=None,
+  device=None,
+  dtype=None,
+  normalize=False,
+):
   if torch.is_tensor(targets):
     if len(targets.shape) == 1:
       num = targets.shape[0]
-      hot = torch.zeros(num, num_classes, device=device or targets.device, dtype=dtype)
+      hot = torch.zeros(
+        num,
+        num_classes,
+        device=device or targets.device,
+        dtype=dtype,
+      )
       hot.scatter_(1, targets.unsqueeze(1), 1.0)
       return hot
     elif len(targets.shape) == 2:
@@ -22,35 +37,208 @@ def one_hot(targets: torch.Tensor, num_classes=None, device=None, dtype=None, no
     raise ValueError('only dim 1 tensors supported')
 
 
-
 def show_hist(hist):
-  chars = ' ▁▂▃▄▅▆▇█'
-  top = max(hist)
-  step = (top / float(len(chars) - 1)) or 1
-  return ''.join(chars[int(round(count / step))] for count in hist)
+  """Generates a simple block character histogram string."""
+  chars = '  ▂▃▄▅▆▇█'
+  # Handle empty histogram case
+  if not hist:
+    return '(empty)'
+  top = max(hist) if hist else 0
+  # Prevent division by zero if top is 0
+  step = (top / float(len(chars) - 1)) if top > 0 else 1
+  # Ensure step is at least 1 to avoid division by zero if counts are very small
+  step = max(step, 1)
+  return ''.join(chars[min(int(round(count / step)), len(chars) - 1)] for count in hist)
+
+
+def tensor_histogram_stats(
+  tensor: torch.Tensor,
+  bins: int = 10,
+  indent: str = '',
+  min_val_item: float = None,
+  max_val_item: float = None,
+) -> str:
+  """Generates a formatted string representation of a tensor's histogram.
+
+  Includes a block character overview and detailed bins with counts, percentages,
+  and inline bars. Handles integer tensors with smarter binning.
+
+  Args:
+      tensor: The tensor to analyze.
+      bins: Default number of bins for histogram.
+      indent: Indentation string for formatting.
+      min_val_item: Pre-calculated minimum value (as Python number). Optional.
+      max_val_item: Pre-calculated maximum value (as Python number). Optional.
 
+  Returns:
+      A formatted string representing the histogram.
+  """
+  # Recalculate min/max items if not provided (needed for standalone use or if failed before)
+  if min_val_item is None or max_val_item is None:
+    try:
+      min_val_item = tensor.min().item()
+      max_val_item = tensor.max().item()
+    except RuntimeError:
+      min_val_item = None
+      max_val_item = None
 
-def describe(tensor: torch.Tensor, bins=10) -> str:
   try:
-    stats = (
-      f"mean: {tensor.mean():.4f} std: {tensor.std():.4f} "
+    # Check applicability and edge cases first
+    is_float = tensor.is_floating_point()
+    is_complex = torch.is_complex(tensor)
+    is_signed_int = tensor.dtype in (
+      torch.int8,
+      torch.int16,
+      torch.int32,
+      torch.int64,
+    )
+    is_unsigned_int = tensor.dtype == torch.uint8
+
+    if not (is_float or is_complex or is_signed_int or is_unsigned_int):
+      return f'{indent}  hist: (not applicable for this dtype)'
+
+    if tensor.numel() == 0:
+      return f'{indent}  hist: (empty tensor)'
+    if torch.isnan(tensor.float()).any():  # Use float for isnan check
+      return f'{indent}  hist: (contains NaN)'
+    if (
+      min_val_item is not None
+      and max_val_item is not None
+      and min_val_item == max_val_item
+    ):
+      return f'{indent}  hist: (all values ≈ {min_val_item:.4f})'
+    if (
+      min_val_item is None or max_val_item is None
+    ):  # Check again after potential recalc
+      return f'{indent}  hist: (could not determine min/max)'
+
+    # Prepare tensor for histc
+    float_tensor = tensor.float() if not (is_float or is_complex) else tensor
+
+    # --- Smart Histogram Logic ---
+    num_bins = bins
+    histc_min = min_val_item
+    histc_max = max_val_item
+    label_format = 'range'
+
+    is_integer_tensor = is_signed_int or is_unsigned_int
+    if is_integer_tensor:
+      num_unique_values = torch.unique(tensor).numel()
+      value_range = int(max_val_item - min_val_item + 1)
+      if num_unique_values <= 25 and num_unique_values == value_range:
+        num_bins = num_unique_values
+        histc_min = min_val_item - 0.5
+        histc_max = max_val_item + 0.5
+        label_format = 'int'
+
+    h_counts = (
+      float_tensor.histc(bins=num_bins, min=histc_min, max=histc_max).int().tolist()
     )
-  except:
+
+    max_h = max(h_counts) if h_counts else 0
+    bar_max_width = 20
+    bar_char = '█'
+
+    block_hist = show_hist(h_counts)
+    bin_width = (histc_max - histc_min) / num_bins
+    total_count = float_tensor.numel()
+    hist_lines = [f'hist: {block_hist}']
+    for i, count in enumerate(h_counts):
+      percentage = (count / total_count) * 100 if total_count > 0 else 0
+      if label_format == 'int':
+        label = f'{int(min_val_item + i):<5d}'
+        label_width = 8
+      else:
+        bin_start = histc_min + i * bin_width
+        bin_end = bin_start + bin_width
+        prec = 3 if max(abs(bin_start), abs(bin_end)) < 10 else 2
+        label = f'{bin_start:>{prec + 4}.{prec}f} - {bin_end:>{prec + 4}.{prec}f}'
+        label_width = 2 * (prec + 4) + 3
+
+      line_text = f'{label:<{label_width}} : {count:<8} ({percentage:5.1f}%)'
+      bar_len = int(round((count / max_h) * bar_max_width)) if max_h > 0 else 0
+      bar_str = bar_char * bar_len
+      hist_lines.append(f'{line_text} | {bar_str}')
+    return '\n'.join(f'{indent}  {line}' for line in hist_lines)
+
+  except RuntimeError as e:
+    return f'{indent}  hist: (error: {e})'
+
+
+def describe_tensor(tensor: torch.Tensor, bins=10, indent: str = '') -> str:
+  """Describes a single tensor."""
+  try:
+    stats = f'mean: {tensor.mean():.4f} std: {tensor.std():.4f} '
+  except (
+    RuntimeError
+  ):  # Handle cases like boolean tensors where mean/std are not defined
     stats = ''
 
   try:
-    h = tensor.histc(bins=bins).int().tolist()
-    hist = (
+    min_val = tensor.min()
+    max_val = tensor.max()
+    sum_val = tensor.sum()
+    # Extract item() immediately after successful calculation
+    min_val_item = min_val.item()
+    max_val_item = max_val.item()
+    min_max_sum = (
+      f'min: {min_val_item:.4f}  max: {max_val_item:.4f}  sum: {sum_val:.4f}'
+    )
+  except RuntimeError:  # Handle non-numeric types
+    min_val_item = None  # Set items to None if calculation failed
+    max_val_item = None
+    min_max_sum = ''
+
+  # Calculate histogram stats using the new helper function
+  hist_str = tensor_histogram_stats(
+    tensor,
+    bins,
+    indent,
+    min_val_item,
+    max_val_item,
+  )
+
+  # Limit tensor display for large tensors
+  tensor_str = str(tensor)
+  if len(tensor_str) > 1000:
+    tensor_str = tensor_str[:500] + '\n... (tensor truncated) ...\n' + tensor_str[-500:]
+
+  return f"""{indent}shape: {tuple(tensor.shape)} dtype: {tensor.dtype} device: {tensor.device} grad: {tensor.requires_grad} size: {tensor.numel() * tensor.element_size() / (1e6):,.5f} MB
+{indent}{min_max_sum}  {stats}
+{indent}{hist_str}
+
+{indent}{tensor_str}
+"""
+
 
-      f"hist: {show_hist(h)}\n"
-      f"      {h}\n"
+def describe(data: Any, bins=10, indent: str = '') -> str:
+  """
+  Recursively describes tensors found in nested structures like lists, tuples, dicts, and dataclasses.
+  """
+  if torch.is_tensor(data):
+    return describe_tensor(data, bins, indent)
+  elif isinstance(data, dict):
+    items_desc = []
+    for k, v in data.items():
+      items_desc.append(
+        f"{indent}  '{k}':\n{describe(v, bins, indent + '    ')}",
+      )
+    return f'{indent}{{\n' + '\n'.join(items_desc) + f'\n{indent}}}'
+  elif isinstance(data, (list, tuple)):
+    is_list = isinstance(data, list)
+    items_desc = [describe(item, bins, indent + '  ') for item in data]
+    start, end = ('[', ']') if is_list else ('(', ')')
+    return f'{indent}{start}\n' + '\n'.join(items_desc) + f'\n{indent}{end}'
+  elif dataclasses.is_dataclass(data) and not isinstance(data, type):
+    items_desc = []
+    for field in dataclasses.fields(data):
+      value = getattr(data, field.name)
+      items_desc.append(
+        f'{indent}  {field.name}:\n{describe(value, bins, indent + "    ")}',
+      )
+    return (
+      f'{indent}{data.__class__.__name__}(\n' + '\n'.join(items_desc) + f'\n{indent})'
     )
-  except:
-    hist = ''
-  return f"""
-shape: {tuple(tensor.shape)} dtype: {tensor.dtype} device: {tensor.device} grad: {tensor.requires_grad} size: {tensor.numel() * tensor.element_size() / (1e6):,.5f} MB
-min: {tensor.min():.4f}  max: {tensor.max():.4f}  {stats}sum: {tensor.sum():.4f}
-{hist}
-
-{tensor}
-  """
\ No newline at end of file
+  else:
+    # For other types, just return their string representation
+    return f'{indent}{str(data)}'
diff --git a/yann/utils/timer.py b/yann/utils/timer.py
index 64024d3..6c73e49 100644
--- a/yann/utils/timer.py
+++ b/yann/utils/timer.py
@@ -1,18 +1,28 @@
 from collections import OrderedDict, defaultdict
-from contextlib import contextmanager, ContextDecorator
-
+from contextlib import ContextDecorator, contextmanager
 from datetime import datetime
+
 import torch.cuda
 
 from ..viz.plot import plot_timeline
 
+
 def time(name=None, sync=False):
   return Task(name=name, sync=sync, log=True)
 
+
 class Task(ContextDecorator):
   __slots__ = ('name', 'start_time', 'end_time', 'meta', 'sync', 'log')
 
-  def __init__(self, name=None, start=None, end=None, meta=None, sync=False, log=False):
+  def __init__(
+    self,
+    name=None,
+    start=None,
+    end=None,
+    meta=None,
+    sync=False,
+    log=False,
+  ):
     self.name = name
     self.start_time = start
     self.end_time = end
@@ -63,7 +73,10 @@ class Task(ContextDecorator):
     self.end()
 
   def __repr__(self):
-    return f"Task({self.name or id(self)}, seconds={self.seconds:.9g}, sync={self.sync})"
+    return (
+      f'Task({self.name or id(self)}, seconds={self.seconds:.9g}, sync={self.sync})'
+    )
+
 
 class Timer:
   def __init__(self, name=None, log=False):
@@ -75,10 +88,13 @@ class Timer:
 
   def start(self, name, sync=True, **meta):
     task = self.task(name, sync=sync, **meta)
-    if self.log: print('Started', name)
+    if self.log:
+      print('Started', name)
 
     if task in self.active_tasks:
-      raise ValueError(f'Nesting tasks is not allowed, "{name}" was already started and not finished')
+      raise ValueError(
+        f'Nesting tasks is not allowed, "{name}" was already started and not finished',
+      )
     self.active_tasks[name] = task
 
   def end(self, name, sync=True, **meta):
@@ -104,4 +120,3 @@ class Timer:
 
   def plot(self):
     plot_timeline(self.tasks)
-
diff --git a/yann/viz/__init__.py b/yann/viz/__init__.py
index fbfa416..0351250 100644
--- a/yann/viz/__init__.py
+++ b/yann/viz/__init__.py
@@ -1,13 +1,18 @@
-from .plot import plot_line, plot_pred_scores, plot_rocs, \
-  plot_confusion_matrix, plot_cooccurrences
 from .image import show_images
-
+from .plot import (
+  plot_confusion_matrix,
+  plot_cooccurrences,
+  plot_line,
+  plot_pred_scores,
+  plot_rocs,
+)
 
 
 class Plotter:
   def __call__(self, *args, **kwargs):
     pass
 
+
 class Shower:
   def __call__(self, x, format=None, **kwargs):
     if isinstance(x, (list, tuple)):
@@ -17,6 +22,7 @@ class Shower:
 
       try:
         from PIL import Image
+
         if isinstance(x[0], Image.Image):
           return self.images(x, **kwargs)
       except:
@@ -27,16 +33,17 @@ class Shower:
 
     try:
       from PIL import Image
+
       if isinstance(x, Image.Image):
         return self.images(x, **kwargs)
     except:
       pass
 
     import torch
+
     if isinstance(x, torch.Tensor):
       return self.tensor(x)
 
-
     return x
 
   def images(self, *args, **kwargs):
@@ -44,7 +51,9 @@ class Shower:
 
   def tensor(self, t, *args, **kwargs):
     from .html import tensor
+
     return tensor(t, *args, **kwargs).display()
 
+
 show = Shower()
-plot = Plotter()
\ No newline at end of file
+plot = Plotter()
diff --git a/yann/viz/activations.py b/yann/viz/activations.py
index 1cdae0f..a94a347 100644
--- a/yann/viz/activations.py
+++ b/yann/viz/activations.py
@@ -1,18 +1,21 @@
 from scipy.misc import imresize
 
 
-
-def draw_mask(img, mask, blend=.5, cmap=None, interp='cubic'):
+def draw_mask(img, mask, blend=0.5, cmap=None, interp='cubic'):
   if not cmap:
     import matplotlib.pylab as plt
+
     cmap = plt.get_cmap('jet')
   if isinstance(cmap, str):
     import matplotlib.pylab as plt
+
     cmap = plt.get_cmap(cmap)
 
   if mask.shape[:2] != img.shape[:2]:
     mask = imresize(mask, img.shape[:2], interp=interp)
-  return (cmap(mask)[:,:,:3] * 255 * blend + img * (1-blend)).round().astype('uint8')
+  return (
+    (cmap(mask)[:, :, :3] * 255 * blend + img * (1 - blend)).round().astype('uint8')
+  )
 
 
 def class_activation_maps(features, weights, classes=None, normalize=True):
@@ -25,8 +28,8 @@ def class_activation_maps(features, weights, classes=None, normalize=True):
     class_maps = {}
     for c in classes:
       blended_channels = (
-          weights[c] @ sample.reshape(num_channels, rows * cols)).reshape(
-        rows, cols)
+        weights[c] @ sample.reshape(num_channels, rows * cols)
+      ).reshape(rows, cols)
       if normalize:
         x = blended_channels - blended_channels.min()
         x = x / x.max() * 255
@@ -54,4 +57,4 @@ def class_activation_maps(features, weights, classes=None, normalize=True):
 #     pass
 #
 #   def show(self):
-#     pass
\ No newline at end of file
+#     pass
diff --git a/yann/viz/html.py b/yann/viz/html.py
index 457f8e9..6cf80e5 100644
--- a/yann/viz/html.py
+++ b/yann/viz/html.py
@@ -1,5 +1,6 @@
 from abc import ABCMeta
 
+
 class styles(dict):
   def __init__(self, *args, **props):
     super().__init__()
@@ -17,8 +18,7 @@ class styles(dict):
     self.update(props)
 
   def __str__(self):
-    return ' '.join(f"{k.replace('_', '-')}: {v};"
-                    for k, v in self.items())
+    return ' '.join(f'{k.replace("_", "-")}: {v};' for k, v in self.items())
 
 
 class Node:
@@ -45,13 +45,15 @@ class Node:
   def html(self):
     if self.CHILDREN:
       return f"""
-         <{self.name} {' '.join(
-        f'{k}="{v}"' for k, v in self.props.items())}  style="{self.style}"/>
+         <{self.name} {' '.join(f'{k}="{v}"' for k, v in self.props.items())}  style="{
+        self.style
+      }"/>
       """
 
     return f"""
-    <{self.name} {' '.join(
-      f'{k}="{v}"' for k, v in self.props.items())} style="{self.style}">
+    <{self.name} {' '.join(f'{k}="{v}"' for k, v in self.props.items())} style="{
+      self.style
+    }">
        {self.format_children()}
     </{self.name}>
     """
@@ -62,6 +64,7 @@ class Node:
 
   def render(self):
     from IPython.core.display import HTML
+
     return HTML(self.html())
 
   def format_children(self):
@@ -69,6 +72,7 @@ class Node:
 
   def display(self):
     from IPython.core.display import display
+
     self._display_handle = display(self.render(), display_id=True)
 
   def update(self):
@@ -111,6 +115,7 @@ class ReactiveNodeMeta(ABCMeta):
 
     return super().__new__(mcls, name, bases, namespace)
 
+
 class ReactiveMixin(metaclass=ReactiveNodeMeta):
   _props: set
   _default_props: dict
@@ -131,19 +136,40 @@ class EmptyNode(Node):
   CHILDREN = True
 
 
-class div(Node): pass
+class div(Node):
+  pass
+
+
+class span(Node):
+  pass
+
+
+class img(EmptyNode):
+  pass
+
+
+class p(Node):
+  pass
+
+
+class h1(Node):
+  pass
+
 
+class h2(Node):
+  pass
 
-class span(Node): pass
 
+class h3(Node):
+  pass
 
-class img(EmptyNode): pass
-class p(Node): pass
-class h1(Node): pass
-class h2(Node): pass
-class h3(Node): pass
-class h4(Node): pass
-class progress(EmptyNode): pass
+
+class h4(Node):
+  pass
+
+
+class progress(EmptyNode):
+  pass
 
 
 class matplotfig(img):
@@ -157,64 +183,79 @@ class matplotfig(img):
 
     if not self.live:
       from .plot import figure_to_base64
-      self.props['src'] = figure_to_base64(figure, data_encoded=True)
 
+      self.props['src'] = figure_to_base64(figure, data_encoded=True)
 
   def html(self):
     if self.live:
       from .plot import figure_to_base64
+
       self.props['src'] = figure_to_base64(self.figure, data_encoded=True)
     if 'src' not in self.props:
       from .plot import figure_to_base64
+
       self.props['src'] = figure_to_base64(self.figure, data_encoded=True)
 
     return super(matplotfig, self).html()
 
 
-
 def _cell(val, size=25):
   return div(
     style=(
       f'width: {size}px; height: {size}px;'
       f' display: inline-block;'
-      f' background-color: rgba({50 + val}, {20 + val * .8}, {80 + val * .6}, 1);'
+      f' background-color: rgba({50 + val}, {20 + val * 0.8}, {80 + val * 0.6}, 1);'
       f' margin: 0; border: 1px solid rgba(0,0,0, .05)'
-    ))
+    ),
+  )
 
 
 def _row(*args):
-  return div(*args, style='margin: 0; padding: 0; font-size:0; white-space: nowrap;')
-
-
-def tensor(t, cell_size=15, scaled=None, min=None, max=None, max_elements=50000):
+  return div(
+    *args,
+    style='margin: 0; padding: 0; font-size:0; white-space: nowrap;',
+  )
+
+
+def tensor(
+  t,
+  cell_size=15,
+  scaled=None,
+  min=None,
+  max=None,
+  max_elements=50000,
+):
   if t.numel() > max_elements:
     raise ValueError('tensor too large')
   if scaled is None:
     max = t.max() if max is None else max
     min = t.min() if min is None else min
-    scaled = (t.float() - min) / (max-min) * 255
+    scaled = (t.float() - min) / (max - min) * 255
 
   if t.ndim == 1:
     return div(
-      f"shape: {tuple(t.shape)}",
-      _row(*(_cell(v, size=cell_size) for v in scaled))
+      f'shape: {tuple(t.shape)}',
+      _row(*(_cell(v, size=cell_size) for v in scaled)),
     )
 
   if t.ndim == 2:
     return div(
-      f"shape: {tuple(t.shape)}",
-      *[
-        _row(*(_cell(v, size=cell_size) for v in row)) for row in scaled
-      ]
+      f'shape: {tuple(t.shape)}',
+      *[_row(*(_cell(v, size=cell_size) for v in row)) for row in scaled],
     )
 
   if t.ndim >= 3:
     return div(
-      f"shape: {tuple(t.shape)}",
+      f'shape: {tuple(t.shape)}',
       div(
-        *(div(style='border: 1px solid #CCC; padding: 5px; margin: 3px; display: inline-block; border-radius: 6px')(
-          f"index: {n}", tensor(x, scaled=x, cell_size=cell_size, min=min, max=max)
-        ) for n, x in enumerate(scaled)
-        )
-      )
-    )
\ No newline at end of file
+        *(
+          div(
+            style='border: 1px solid #CCC; padding: 5px; margin: 3px; display: inline-block; border-radius: 6px',
+          )(
+            f'index: {n}',
+            tensor(x, scaled=x, cell_size=cell_size, min=min, max=max),
+          )
+          for n, x in enumerate(scaled)
+        ),
+      ),
+    )
diff --git a/yann/viz/image.py b/yann/viz/image.py
index ef911ed..4c9d676 100644
--- a/yann/viz/image.py
+++ b/yann/viz/image.py
@@ -1,25 +1,27 @@
+import base64
 from io import BytesIO
-from PIL import Image
 from itertools import zip_longest
-import base64
+
+from PIL import Image
 
 
 def base64str(img):
   s = BytesIO()
   img.save(s, format='JPEG')
-  return (
-      "data:image/jpeg;base64,"
-      + base64.b64encode(s.getvalue()).decode("utf-8")
+  return 'data:image/jpeg;base64,' + base64.b64encode(s.getvalue()).decode(
+    'utf-8',
   )
 
 
 def show_images(paths, labels=None, urls=None, w=400, h=400):
-  from IPython.core.display import display, HTML
   from pathlib import Path
 
+  from IPython.core.display import HTML, display
+
   if isinstance(paths, (str, Path)):
     if '*' in paths:
       from glob import glob
+
       paths = glob(paths)
     else:
       paths = [paths]
@@ -32,10 +34,11 @@ def show_images(paths, labels=None, urls=None, w=400, h=400):
   else:
     items = zip_longest(paths, labels or [])
 
-
   tags = []
   for x, l in items:
-    if urls or (urls is not False and isinstance(x, (str, Path)) and str(x).startswith('http')):
+    if urls or (
+      urls is not False and isinstance(x, (str, Path)) and str(x).startswith('http')
+    ):
       src = str(x)
     else:
       img = Image.open(x) if isinstance(x, str) else x
@@ -44,23 +47,23 @@ def show_images(paths, labels=None, urls=None, w=400, h=400):
 
     if l:
       tags.append(
-        f'''
+        f"""
         <div style="display: inline-block; padding: 3px">
           <img 
             style="max-width: {w}px; max-height: {h}px; margin: 3px;"
              src={src} />
           <p>{l if isinstance(l, str) else ', '.join(l)}</p>
         </div>
-        '''
+        """,
       )
     else:
       tags.append(
-        f'''
+        f"""
         <div style="display: inline-block; padding: 3px">
           <img 
             style="max-width: {w}px; max-height: {h}px; margin: 3px;"
              src={src} />
         </div>
-        '''
+        """,
       )
-  return display(HTML(f'<div>{"".join(tags)}</div>'))
\ No newline at end of file
+  return display(HTML(f'<div>{"".join(tags)}</div>'))
diff --git a/yann/viz/plot.py b/yann/viz/plot.py
index fcd0f70..8fa0def 100644
--- a/yann/viz/plot.py
+++ b/yann/viz/plot.py
@@ -1,42 +1,39 @@
-from matplotlib import pylab as plt
-import numpy as np
+import base64
 import datetime
+import io
+import itertools
 import pathlib
-from sklearn.metrics import roc_curve, auc, confusion_matrix
-import matplotlib.dates as mdates
-from matplotlib.collections import PolyCollection
 from collections import OrderedDict
-
-import io
-import base64
 from urllib.parse import quote
 
-import itertools
+import numpy as np
 
 from .. import to_numpy
 from ..metrics import moving_average
 
 
 def plot_line(
-    y,
-    x=None,
-    figsize=None,
-    window=1,
-    xlim=None,
-    ylim=None,
-    line_width=2,
-    stroke='-',
-    title=None,
-    xlabel=None,
-    ylabel=None,
-    xscale=None,
-    yscale=None,
-    show=True,
-    save=False,
-    legend=True,
-    name=None,
-    grid=True
+  y,
+  x=None,
+  figsize=None,
+  window=1,
+  xlim=None,
+  ylim=None,
+  line_width=2,
+  stroke='-',
+  title=None,
+  xlabel=None,
+  ylabel=None,
+  xscale=None,
+  yscale=None,
+  show=True,
+  save=False,
+  legend=True,
+  name=None,
+  grid=True,
 ):
+  from matplotlib import pylab as plt
+
   fig = figsize and plt.figure(figsize=figsize)
 
   if xlim:
@@ -78,8 +75,9 @@ def plot_line(
     plt.show()
   if save:
     plt.gcf().savefig(
-      save if isinstance(save, (str, pathlib.Path)) else
-      f"{title or ylabel or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg"
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f'{title or ylabel or datetime.datetime.utcnow().strftime("%y-%m-%dT%H%M%S")}.jpg',
     )
     plt.gcf().clear()
     if fig:
@@ -87,47 +85,81 @@ def plot_line(
 
   return plt.gcf()
 
+
 def plot_pred_scores(
-    preds,
-    targets,
-    classes=None,
-    logscale=True,
-    figsize=(12, 6),
-    show=True,
-    save=False,
-    title=None):
+  preds,
+  targets,
+  classes=None,
+  logscale=True,
+  figsize=(12, 6),
+  show=True,
+  save=False,
+  title=None,
+):
   import seaborn as sns
+  from matplotlib import pylab as plt
 
   preds, targets = to_numpy(preds), to_numpy(targets)
 
   if title:
     plt.title(title)
 
-
   if classes:
-    classes = classes.items() if isinstance(classes, dict) else \
-      ((c, n) for n, c in enumerate(classes))
+    classes = (
+      classes.items()
+      if isinstance(classes, dict)
+      else ((c, n) for n, c in enumerate(classes))
+    )
   else:
     classes = ((n, n) for n in range(preds.shape[1]))
 
-
   for cls, idx in classes:
     f, ax = plt.subplots(figsize=figsize)
     if logscale:
-      ax.set(yscale="log")
+      ax.set(yscale='log')
 
     if len(targets.shape) == 1:
-      sns.distplot(preds[targets != idx, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0, 1]}, ax=ax, color='red',
-                   label='Negative')
-      sns.distplot(preds[targets == idx, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0, 1]}, ax=ax, color='blue',
-                   label='Positive')
+      sns.distplot(
+        preds[targets != idx, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={'range': [0, 1]},
+        ax=ax,
+        color='red',
+        label='Negative',
+      )
+      sns.distplot(
+        preds[targets == idx, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={'range': [0, 1]},
+        ax=ax,
+        color='blue',
+        label='Positive',
+      )
     else:
-      sns.distplot(preds[targets[:, idx] == 0, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0,1]}, ax=ax, color='red', label='Negative')
-      sns.distplot(preds[targets[:, idx] == 1, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0,1]}, ax=ax, color='blue', label='Positive')
+      sns.distplot(
+        preds[targets[:, idx] == 0, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={'range': [0, 1]},
+        ax=ax,
+        color='red',
+        label='Negative',
+      )
+      sns.distplot(
+        preds[targets[:, idx] == 1, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={'range': [0, 1]},
+        ax=ax,
+        color='blue',
+        label='Positive',
+      )
     ax.set_title(cls)
     plt.xlabel('Score')
     plt.ylabel('Sample Count')
@@ -136,19 +168,25 @@ def plot_pred_scores(
   if show:
     plt.show()
   if save:
-    plt.savefig(save if isinstance(save, (str, pathlib.Path)) else
-                  f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg")
+    plt.savefig(
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f'{title or datetime.datetime.utcnow().strftime("%y-%m-%dT%H%M%S")}.jpg',
+    )
     plt.gcf().clear()
 
 
 def plot_rocs(
-    preds,
-    targets,
-    classes=None,
-    figsize=(12,12),
-    show=True,
-    save=False,
-    title=None):
+  preds,
+  targets,
+  classes=None,
+  figsize=(12, 12),
+  show=True,
+  save=False,
+  title=None,
+):
+  from matplotlib import pylab as plt
+  from sklearn.metrics import auc, roc_curve
 
   preds, targets = to_numpy(preds), to_numpy(targets)
 
@@ -157,8 +195,11 @@ def plot_rocs(
     plt.title(title)
 
   if classes:
-    classes = classes.items() if isinstance(classes, dict) else \
-      ((c, n) for n, c in enumerate(classes))
+    classes = (
+      classes.items()
+      if isinstance(classes, dict)
+      else ((c, n) for n, c in enumerate(classes))
+    )
   else:
     classes = ((n, n) for n in range(preds.shape[1]))
 
@@ -168,7 +209,6 @@ def plot_rocs(
   plt.ylabel('True Positive Rate')
   plt.plot([0, 1], [0, 1], 'k--', lw=2)
 
-
   vals = {}
   for cls, idx in classes:
     if len(targets.shape) == 1:
@@ -181,36 +221,37 @@ def plot_rocs(
     vals[cls] = (area, fpr, tpr, thresholds)
   plt.legend(loc='best')
 
-
   if show:
     plt.show()
   if save:
-    plt.savefig(save if isinstance(save, (str, pathlib.Path)) else
-                f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg")
+    plt.savefig(
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f'{title or datetime.datetime.utcnow().strftime("%y-%m-%dT%H%M%S")}.jpg',
+    )
     plt.gcf().clear()
 
   return vals
 
 
 def plot_cooccurrences(counts, classes, figsize=(14, 11)):
-  import seaborn as sns
   import pandas as pd
+  import seaborn as sns
+  from matplotlib import pylab as plt
 
   mask = np.ones_like(counts)
   mask[np.tril_indices_from(mask)] = False
 
-  df_cm = pd.DataFrame(
-    counts,
-    index=list(classes),
-    columns=list(classes))
+  df_cm = pd.DataFrame(counts, index=list(classes), columns=list(classes))
   plt.figure(figsize=figsize)
   plot = sns.heatmap(
     df_cm,
     robust=True,
     annot=True,
-    fmt="d",
-    cmap="YlGnBu",
-    mask=mask)
+    fmt='d',
+    cmap='YlGnBu',
+    mask=mask,
+  )
   plot.set_title('Class Co-occurrence')
 
 
@@ -219,7 +260,7 @@ def sorted_indices(matrix, desc=False):
   return (np.fliplr(inds) if desc else inds)[0]
 
 
-def truncate_confusion_matrix(matrix, thresh=.2, top=None, symmetric=True):
+def truncate_confusion_matrix(matrix, thresh=0.2, top=None, symmetric=True):
   inds = sorted_indices(matrix, desc=True)
   inds = [(r, c) for r, c in inds if r != c]
 
@@ -240,8 +281,18 @@ def truncate_confusion_matrix(matrix, thresh=.2, top=None, symmetric=True):
 
 
 def plot_confusion_matrix(
-    preds, targets, classes, figsize=(16, 16),
-    thresh=None, top=None, normalize=False, symmetric=True):
+  preds,
+  targets,
+  classes,
+  figsize=(16, 16),
+  thresh=None,
+  top=None,
+  normalize=False,
+  symmetric=True,
+):
+  from matplotlib import pylab as plt
+  from sklearn.metrics import confusion_matrix
+
   preds, targets = to_numpy(preds), to_numpy(targets)
 
   cm = confusion_matrix(targets, preds)
@@ -251,7 +302,11 @@ def plot_confusion_matrix(
 
   if thresh or top:
     cm, rows, cols = truncate_confusion_matrix(
-      cm, thresh=thresh, top=top, symmetric=symmetric)
+      cm,
+      thresh=thresh,
+      top=top,
+      symmetric=symmetric,
+    )
   else:
     rows, cols = list(range(len(classes))), list(range(len(classes)))
 
@@ -266,11 +321,15 @@ def plot_confusion_matrix(
   plt.yticks(np.arange(len(rows)), [classes[i] for i in rows])
 
   fmt = '.2f' if normalize else 'd'
-  th = cm.max() / 2.
+  th = cm.max() / 2.0
   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
-    plt.text(j, i, format(cm[i, j], fmt),
-             horizontalalignment="center",
-             color="white" if cm[i, j] > th else "black")
+    plt.text(
+      j,
+      i,
+      format(cm[i, j], fmt),
+      horizontalalignment='center',
+      color='white' if cm[i, j] > th else 'black',
+    )
 
   plt.tight_layout()
   plt.ylabel('True label')
@@ -281,17 +340,23 @@ def plot_confusion_matrix(
 
 
 def plot_timeline(tasks, figsize=None):
+  import matplotlib.dates as mdates
+  from matplotlib import pylab as plt
+  from matplotlib.collections import PolyCollection
+
   inds = OrderedDict((c, n) for n, c in enumerate(set(t.name for t in tasks)))
-  color_map = {c: f"C{n}" for c, n in inds.items()}
+  color_map = {c: f'C{n}' for c, n in inds.items()}
 
   verts = []
   colors = []
   for task in tasks:
-    v = [(mdates.date2num(task.start_time), inds[task.name] - .4),
-         (mdates.date2num(task.start_time), inds[task.name] + .4),
-         (mdates.date2num(task.end_time), inds[task.name] + .4),
-         (mdates.date2num(task.end_time), inds[task.name] - .4),
-         (mdates.date2num(task.start_time), inds[task.name] - .4)]
+    v = [
+      (mdates.date2num(task.start_time), inds[task.name] - 0.4),
+      (mdates.date2num(task.start_time), inds[task.name] + 0.4),
+      (mdates.date2num(task.end_time), inds[task.name] + 0.4),
+      (mdates.date2num(task.end_time), inds[task.name] - 0.4),
+      (mdates.date2num(task.start_time), inds[task.name] - 0.4),
+    ]
     verts.append(v)
     colors.append(color_map[task.name])
 
@@ -312,8 +377,9 @@ def plot_timeline(tasks, figsize=None):
   plt.show()
 
 
-
 def figure_to_base64(fig, format='png', data_encoded=True, close=False):
+  from matplotlib import pylab as plt
+
   buf = io.BytesIO()
   fig.savefig(buf, format=format)
   buf.seek(0)
@@ -323,6 +389,6 @@ def figure_to_base64(fig, format='png', data_encoded=True, close=False):
     plt.close(fig)
 
   if data_encoded:
-    return f"data:image/{format};base64,{quote(string)}"
+    return f'data:image/{format};base64,{quote(string)}'
 
   return string
diff --git a/yann/viz/viewer.py b/yann/viz/viewer.py
index 9fc57dd..fd0e500 100644
--- a/yann/viz/viewer.py
+++ b/yann/viz/viewer.py
@@ -15,4 +15,4 @@ class PandasViewer(Viewer):
 
 
 class DatasetViewer(Viewer):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/viz/widgets.py b/yann/viz/widgets.py
index 948e68a..1c532cf 100644
--- a/yann/viz/widgets.py
+++ b/yann/viz/widgets.py
@@ -1,4 +1,4 @@
-from .html import div, prop, ReactiveMixin, Node
+from .html import Node, ReactiveMixin, div, prop
 
 
 class ProgressBar(ReactiveMixin, Node):
@@ -17,11 +17,9 @@ class ProgressBar(ReactiveMixin, Node):
         background-color: {self.background}; 
         overflow: hidden; 
         padding: 5px 10px;
-        """
+        """,
       )(
-        div(style='position:relative; z-index: 1;')(
-          *self.children
-        ),
+        div(style='position:relative; z-index: 1;')(*self.children),
         div(
           style=f"""
           background-color: {self.color};
@@ -31,6 +29,7 @@ class ProgressBar(ReactiveMixin, Node):
           left: 0;
           right: {100 - self.value / self.max * 100}%;
           z-index: 0;
-        """)
-        )
-    ).html()
\ No newline at end of file
+        """,
+        ),
+      )
+    ).html()