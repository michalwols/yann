diff --git a/TODO.md b/TODO.md
index e69de29..d2a04dd 100644
--- a/TODO.md
+++ b/TODO.md
@@ -0,0 +1,80 @@
+- [ ] CapPa
+- [ ] Classification Done Right for Vision-Language
+Pre-Training
+
+
+- [ ] CLIP
+- [ ] SigLip
+- [ ] MAE
+- [ ] DINOv2
+- [ ] BERT
+- [ ] ColBert
+- [ ] Mamba
+- [ ] SWIN
+- [ ] DeiT
+- [ ] CoAtNet
+- [ ] AIM v2
+
+- [ ] CV Modules
+  - [ ] convnext
+  - [ ] NFNet
+  - [ ] FastViT
+  - [ ] mobileone
+  - [ ] SE Net
+
+https://github.com/LeapLabTHU/MLLA/blob/master/models/mlla.py
+
+- [ ] Diff transformer
+- [ ] Sigmoid Attention
+
+- [ ] duckdb dataset
+- [ ] hugging face datasets support (streaming)
+
+
+
+
+- [ ] Attention Zoo
+  - [ ] Sigmoid Attentio
+  - [ ] Diff Attention
+  - [ ] TokenFormer
+  - [ ] GKA
+
+- [ ] SSM Zoo / Hybrids
+
+
+- [ ] VLM Zoo
+  - [ ] fast VLM
+  - LLAVA
+
+- [ ] ViT Zoo
+  - dense former
+  - sparse former
+  - ViTamin
+  - mixture of depth
+  - MOE
+  - PEFT
+    - https://github.com/GraphPKU/PiSSA
+- [ ] schedule free 
+- - [ ] shampoo https://x.com/cloneofsimo/status/1836003682141577418
+  - [ ] soap
+  - [ ] muon
+  - [ ] adeamix
+  - [ ] https://github.com/ClashLuke/HeavyBall
+  - https://github.com/iShohei220/adopt/tree/main
+- [ ] bits and bytes
+
+- [ ] liger kernels
+- [ ] triton
+- [ ] torchao
+- [ ] distributed training
+
+https://arxiv.org/abs/2404.0p5196
+
+
+- [ ] optuna support
+
+
+
+
+- [ ] uv
+- [ ] upgrade docs
\ No newline at end of file
diff --git a/examples/benchmark.py b/examples/benchmark.py
index b749a31..75285de 100644
--- a/examples/benchmark.py
+++ b/examples/benchmark.py
@@ -1,30 +1,31 @@
+from itertools import repeat
+
 import timm
 import torch
-from itertools import repeat
-from torch.cuda.amp import autocast, GradScaler
+from torch.cuda.amp import GradScaler, autocast
+
 import yann
-from yann.callbacks import ProgressBar
 import yann.transforms
+from yann.callbacks import ProgressBar
 
 
 class Params(yann.params.HyperParams):
-  dataset = 'CIFAR10'
+  dataset = "CIFAR10"
 
   size = 64
-  model = 'resnet18'
-  loss = 'cross_entropy'
+  model = "resnet18"
+  loss = "cross_entropy"
 
-  optimizer = 'SGD'
+  optimizer = "SGD"
 
   batch_size = 32
   num_workers = 8
   pin_memory = False
   prefetch_factor = 2
 
-
   device = yann.default.device
-  memory_format: str = 'contiguous_format'
-  dtype = 'float32'
+  memory_format: str = "contiguous_format"
+  dtype = "float32"
   non_blocking = True
 
   amp = False
@@ -36,21 +37,20 @@ class Params(yann.params.HyperParams):
 
 
 def simple_train_loop(
-    model: torch.nn.Module,
-    loader,
-    loss,
-    optimizer,
-    device=None,
-    memory_format=None,
-    dtype=None,
-    non_blocking=False,
-    amp=False,
-    progress: ProgressBar = None
+  model: torch.nn.Module,
+  loader,
+  loss,
+  optimizer,
+  device=None,
+  memory_format=None,
+  dtype=None,
+  non_blocking=False,
+  amp=False,
+  progress: ProgressBar = None,
 ):
   model.train()
   progress.on_epoch_start()
   for i, (inputs, targets) in enumerate(loader):
-
     # print(i)
     if device or memory_format:
       inputs, targets = (
@@ -58,12 +58,9 @@ def simple_train_loop(
           device,
           memory_format=memory_format,
           dtype=dtype,
-          non_blocking=non_blocking
+          non_blocking=non_blocking,
         ),
-        targets.to(
-          device=device,
-          non_blocking=non_blocking
-        )
+        targets.to(device=device, non_blocking=non_blocking),
       )
 
     with autocast(enabled=amp):
@@ -78,7 +75,6 @@ def simple_train_loop(
   progress.on_epoch_end()
 
 
-
 def benchmark_train(params: Params):
   print(params)
 
@@ -90,9 +86,7 @@ def benchmark_train(params: Params):
 
   model = timm.create_model(params.model)
   transform = yann.transforms.ImageTransformer(
-    resize=params.size,
-    crop=params.size,
-    color_space='RGB'
+    resize=params.size, crop=params.size, color_space="RGB"
   )
 
   dataset = yann.resolve.dataset(params.dataset, download=True)
@@ -109,7 +103,7 @@ def benchmark_train(params: Params):
     try:
       from functorch.compile import memory_efficient_fusion
     except ImportError:
-      raise ValueError('functorch must be installed for aot_autograd support')
+      raise ValueError("functorch must be installed for aot_autograd support")
     model = memory_efficient_fusion(model)
 
   loader = yann.loader(
@@ -118,7 +112,7 @@ def benchmark_train(params: Params):
     batch_size=params.batch_size,
     num_workers=params.num_workers,
     pin_memory=params.pin_memory,
-    prefetch_factor=params.prefetch_factor
+    prefetch_factor=params.prefetch_factor,
   )
 
   if params.skip_loader:
@@ -141,7 +135,7 @@ def benchmark_train(params: Params):
       memory_format=memory_format,
       non_blocking=params.non_blocking,
       amp=params.amp,
-      progress=ProgressBar(length=len(dataset))
+      progress=ProgressBar(length=len(dataset)),
     )
   else:
     train = yann.train.Trainer(
@@ -153,12 +147,11 @@ def benchmark_train(params: Params):
       # dtype=dtype,
       # memory_format=memory_format,
       amp=params.amp,
-      callbacks=[ProgressBar(length=len(dataset))]
+      callbacks=[ProgressBar(length=len(dataset))],
     )
     train(1)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
   params = Params.from_command()
   benchmark_train(params)
-
diff --git a/examples/explore_losses.ipynb b/examples/explore_losses.ipynb
index ee61bcd..7a18370 100644
--- a/examples/explore_losses.ipynb
+++ b/examples/explore_losses.ipynb
@@ -19,43 +19,42 @@
    "source": [
     "%matplotlib inline\n",
     "import torch\n",
+    "from matplotlib import pyplot as plt\n",
     "from torch.nn import functional as F\n",
-    "from yann.viz import plot_line\n",
+    "\n",
     "from yann.modules.loss import binary_focal_loss\n",
-    "from matplotlib import pyplot as plt\n",
+    "from yann.viz import plot_line\n",
     "\n",
     "logits = [x / 100 for x in range(-500, 500, 1)]\n",
     "\n",
     "\n",
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[1.0]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]), torch.Tensor([[1.0]]), reduction=\"none\"\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f\"binary cross entropy\",\n",
+    "  show=False,\n",
+    "  title=\"Focal Loss\",\n",
+    "  xlabel=\"probability of ground truth class\",\n",
+    "  ylabel=\"loss\",\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[1.0]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]), targets=torch.Tensor([[1.0]]), gamma=g\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f\"gamma = {g}\",\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -79,34 +78,32 @@
    ],
    "source": [
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[0]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]), torch.Tensor([[0]]), reduction=\"none\"\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f\"binary cross entropy\",\n",
+    "  show=False,\n",
+    "  title=\"Focal Loss\",\n",
+    "  xlabel=\"probability of ground truth class\",\n",
+    "  ylabel=\"loss\",\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[0]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]), targets=torch.Tensor([[0]]), gamma=g\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f\"gamma = {g}\",\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -130,34 +127,32 @@
    ],
    "source": [
     "plot_line(\n",
-    "        y=[F.binary_cross_entropy_with_logits(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[.8]]),\n",
-    "            reduction='none'\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'binary cross entropy',\n",
-    "        show=False,\n",
-    "    title='Focal Loss with smooth target (.8)',\n",
-    "    xlabel=\"probability of ground truth class\",\n",
-    "    ylabel=\"loss\"\n",
-    "    )\n",
+    "  y=[\n",
+    "    F.binary_cross_entropy_with_logits(\n",
+    "      torch.Tensor([[logit]]), torch.Tensor([[0.8]]), reduction=\"none\"\n",
+    "    ).item()\n",
+    "    for logit in logits\n",
+    "  ],\n",
+    "  x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "  name=f\"binary cross entropy\",\n",
+    "  show=False,\n",
+    "  title=\"Focal Loss with smooth target (.8)\",\n",
+    "  xlabel=\"probability of ground truth class\",\n",
+    "  ylabel=\"loss\",\n",
+    ")\n",
     "\n",
-    "for g in [.5, 1, 2, 5]:\n",
-    "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            logits=torch.Tensor([[logit]]),\n",
-    "            targets=torch.Tensor([[.8]]),\n",
-    "            gamma=g\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma = {g}',\n",
-    "        show=False,\n",
-    "    )\n",
+    "for g in [0.5, 1, 2, 5]:\n",
+    "  plot_line(\n",
+    "    y=[\n",
+    "      binary_focal_loss(\n",
+    "        logits=torch.Tensor([[logit]]), targets=torch.Tensor([[0.8]]), gamma=g\n",
+    "      ).item()\n",
+    "      for logit in logits\n",
+    "    ],\n",
+    "    x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "    name=f\"gamma = {g}\",\n",
+    "    show=False,\n",
+    "  )\n",
     "\n",
     "plt.grid()\n",
     "plt.show()"
@@ -233,33 +228,29 @@
    ],
    "source": [
     "for g in [2]:\n",
-    " for alpha in [None, .1, .3, .5, .8, 1]:\n",
+    "  for alpha in [None, 0.1, 0.3, 0.5, 0.8, 1]:\n",
     "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[1]]),\n",
-    "            gamma=g,\n",
-    "            alpha=alpha\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma={g}, alpha={alpha}, target=1',\n",
-    "        show=False,\n",
+    "      y=[\n",
+    "        binary_focal_loss(\n",
+    "          torch.Tensor([[logit]]), torch.Tensor([[1]]), gamma=g, alpha=alpha\n",
+    "        ).item()\n",
+    "        for logit in logits\n",
+    "      ],\n",
+    "      x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "      name=f\"gamma={g}, alpha={alpha}, target=1\",\n",
+    "      show=False,\n",
     "    )\n",
-    "    \n",
+    "\n",
     "    plot_line(\n",
-    "        y=[binary_focal_loss(\n",
-    "            torch.Tensor([[logit]]),\n",
-    "            torch.Tensor([[0]]),\n",
-    "            gamma=g,\n",
-    "            alpha=alpha\n",
-    "        ).item() \n",
-    "            for logit in logits],\n",
-    "        x=[torch.sigmoid(torch.Tensor([x])).item()\n",
-    "           for x in logits],\n",
-    "        name=f'gamma={g}, alpha={alpha}, target=0',\n",
-    "        show=False,\n",
+    "      y=[\n",
+    "        binary_focal_loss(\n",
+    "          torch.Tensor([[logit]]), torch.Tensor([[0]]), gamma=g, alpha=alpha\n",
+    "        ).item()\n",
+    "        for logit in logits\n",
+    "      ],\n",
+    "      x=[torch.sigmoid(torch.Tensor([x])).item() for x in logits],\n",
+    "      name=f\"gamma={g}, alpha={alpha}, target=0\",\n",
+    "      show=False,\n",
     "    )\n",
     "\n",
     "    plt.grid()\n",
diff --git a/examples/train_mnist.py b/examples/train_mnist.py
index 26f6d94..97a1a52 100644
--- a/examples/train_mnist.py
+++ b/examples/train_mnist.py
@@ -1,78 +1,180 @@
 import torch
 from torch import nn
 from torchvision import transforms
+from torchvision.datasets import MNIST
 
 import yann
-from yann.modules import Stack, Flatten, Infer
-from yann.params import HyperParams, Choice, Range
+from yann.modules import Flatten, Infer, Stack
+from yann.params import Choice, HyperParams, Range
 from yann.train import Trainer
+from yann.callbacks import History
+from yann.callbacks.rich_progress import RichProgress
 
 
 class Params(HyperParams):
-  dataset = 'MNIST'
-  batch_size = 32
+  dataset = "MNIST"
+  batch_size = 3
   epochs = 10
-  optimizer: Choice(('SGD', 'Adam')) = 'SGD'
-  learning_rate: Range(.01, .0001) = .01
+  optimizer: Choice(("SGD", "Adam")) = "SGD"
+  learning_rate: Range(0.01, 0.0001) = 0.01
   momentum = 0
 
   seed = 1
 
-# parse command line arguments
-params = Params.from_command()
-params.validate()
 
-print(params)
+#
+#
+# class Vit(nn.Module):
+#   def __init__(self):
+#     self.conv = nn.Sequential(
+#       nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
+#     )
+#
+#     self.transformers = nn.Sequential(
+#       nn.TransformerEncoder()
+#
+#     )
+#
+
+
+class BoundedLeakyReLU(nn.Module):
+  def __init__(self, negative_slope=0.01, peak=1):
+    super(BoundedLeakyReLU, self).__init__()
+    self.negative_slope = negative_slope
+    self.peak = peak
+
+  def forward(self, x):
+    # Create the mask for the different regions
+    positive_mask = (x > self.peak).float()  # x > 1 -> apply -x
+    relu_mask = (x > 0).float() * (
+      x <= self.peak
+    ).float()  # 0 <= x <= 1 -> apply ReLU
+    leaky_relu_mask = (x <= 0).float()  # x <= 0 -> apply Leaky ReLU
+
+    # Compute the outputs for each region
+    negative_part = leaky_relu_mask * self.negative_slope * x
+    relu_part = relu_mask * x
+    inverted_part = positive_mask * (-x)
+
+    # Combine the outputs
+    return negative_part + relu_part + inverted_part
+
+
+if __name__ == "__main__":
+  # parse command line arguments
+  params = Params.from_command()
+  params.validate()
+
+  print(params)
+
+  # set random, numpy and pytorch seeds in one call
+  # yann.seed(params.seed)
+
+  Activation = BoundedLeakyReLU
+
+  lenet = Stack(
+    Infer(nn.Conv2d, 10, kernel_size=5),
+    nn.MaxPool2d(2),
+    Activation(),
+    Infer(nn.Conv2d, 20, kernel_size=5),
+    nn.MaxPool2d(2),
+    Activation(),
+    Flatten(),
+    Infer(nn.Linear, 50),
+    Activation(),
+    Infer(nn.Linear, 10),
+    activation=nn.LogSoftmax(dim=1),
+  )
+
+  # run a forward pass to infer input shapes using `Infer` modules
+  lenet(torch.rand(1, 1, 28, 28))
+
+  # use the registry to resolve optimizer name to an optimizer class
+  optimizer = yann.resolve.optimizer(
+    params.optimizer,
+    yann.trainable(lenet.parameters()),
+    momentum=params.momentum,
+    lr=params.learning_rate,
+  )
+
+  train = Trainer(
+    model=lenet,
+    optimizer=optimizer,
+    dataset=params.dataset,
+    val_dataset=(params.dataset, {"train": False}),
+    batch_size=params.batch_size,
+    transform=transforms.Compose(
+      [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
+    ),
+    loss="nll_loss",
+    metrics=("accuracy",),
+    callbacks=[
+        History(metrics=("accuracy",)),
+        RichProgress()
+    ],
+    amp=False,
+  )
+
+  train(params.epochs)
+
+  # save checkpoint
+  train.checkpoint()
+
+  # plot the loss curve
+  # train.history.plot()
+
+
+from typing import Optional
 
-# set random, numpy and pytorch seeds in one call
-yann.seed(params.seed)
+import torch
+from torch.nn import functional as F
+
+
+def linear(*args):
+  return args[0]
 
 
-lenet = Stack(
-  Infer(nn.Conv2d, 10, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
+def text_encoder(input_ids: torch.Tensor, w: TextModel):
+  return F.embedding(input_ids, w.wte)
 
-  Infer(nn.Conv2d, 20, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
 
-  Flatten(),
+def attn_mask(pos, seq_len):
+  mask = torch.ones(seq_len, pos + seq_len, dtype=torch.bool)
+  mask[:, pos:] = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
+  mask = mask.unsqueeze(0).unsqueeze(0)
+  return mask
 
-  Infer(nn.Linear, 50),
-  nn.ReLU(inplace=True),
-  Infer(nn.Linear, 10),
-  activation=nn.LogSoftmax(dim=1)
-)
 
-# run a forward pass to infer input shapes using `Infer` modules
-lenet(torch.rand(1, 1, 28, 28))
+def attn(
+  x: torch.Tensor,
+  w: AttentionWeights,
+  freqs_cis: torch.Tensor,
+  layer_kv_cache: Optional[torch.Tensorl],
+  i: int,
+):
+  bsz, q_len, d_model = x.shape
+  pos = 0 if layer_kv_cache is None else layer_kv_cache.shape[3]
 
-# use the registry to resolve optimizer name to an optimizer class
-optimizer = yann.resolve.optimizer(
-  params.optimizer,
-  yann.trainable(lenet.parameters()),
-  momentum=params.momentum,
-  lr=params.learning_rate
-)
+  n_heads, head_dim = w.n_heads, d_model // w.n_heads
+  q, k, v = linear(x, w.qkv).chunk(3, dim=-1)
 
-train = Trainer(
-  model=lenet,
-  optimizer=optimizer,
-  dataset=params.dataset,
-  batch_size=params.batch_size,
-  transform=transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.1307,), (0.3081,))
-  ]),
-  loss='nll_loss',
-  metrics=('accuracy',)
-)
+  q = q.view(bsz, q_len, n_heads, head_dim).transpose(1, 2)
+  k = k.view(bsz, q_len, n_heads, head_dim).transpose(1, 2)
+  v = v.view(bsz, q_len, n_heads, head_dim).transpose(1, 2)
 
-train(params.epochs)
+  q_rot, q_pass = q.chunk(2, dim=-1)
+  k_rot, k_pass = k.chunk(2, dim=-1)
+  q_rot, k_rot = apply_rotary_emb(
+    q_rot, k_rot, freqs_cis[pos : pos + q_len], torch.float32
+  )
+  q = torch.cat([q_rot, q_pass], dim=-1).to(torch.float16)
+  k = torch.cat([k_rot, k_pass], dim=-1).to(torch.float16)
 
-# save checkpoint
-train.checkpoint()
+  if layer_kv_cache is not None:
+    k = torch.cat([layer_kv_cache[0], k], dim=2)
+    v = torch.cat([layer_kv_cache[1], v], dim=2)
 
-# plot the loss curve
-train.history.plot()
\ No newline at end of file
+  out = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask(pos, q_len))
+  out = out.transpose(1, 2).reshape(bsz, q_len, d_model)
+  out = linear(out, w.proj)
+  return out, torch.stack([k, v])
diff --git a/pyproject.toml b/pyproject.toml
index 64ef87e..1fe4f58 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -1,7 +1,3 @@
-[build-system]
-requires = ["setuptools>=61.0", "wheel"]
-build-backend = "setuptools.build_meta"
-
 [project]
 name = "yann"
 version = "0.0.40"
@@ -13,13 +9,9 @@ authors = [
 ]
 requires-python = ">=3.7"
 dependencies = [
-    "numpy",
-    "scipy",
-    "scikit-learn",
-    "torch>=1.0.0",
-    "matplotlib",
+    "rich>=13.8.1",
+    "torch",
     "torchvision",
-    "tqdm"
 ]
 
 [project.optional-dependencies]
@@ -28,3 +20,22 @@ pretrainedmodels = ["pretrainedmodels"]
 
 [project.scripts]
 yann = "yann.cli:main"
+
+[build-system]
+requires = ["setuptools>=61.0", "wheel"]
+build-backend = "setuptools.build_meta"
+
+[tool.setuptools.packages.find]
+where = ["."]  # Specifies to look for packages in the current directory
+include = ["yann*"]  # Include the main 'yann' package and any potential 'yann_extras' etc.
+exclude = ["tests*"]  # Example: Exclude tests directory if you have one
+
+[dependency-groups]
+dev = [
+    "ipython>=7.34.0",
+]
+
+
+[tool.ruff]
+line-length = 80
+indent-width = 2
diff --git a/requirements-dev.txt b/requirements-dev.txt
deleted file mode 100644
index df0b7de..0000000
--- a/requirements-dev.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-pytest==3.9.3
-pytest-asyncio==0.9.0
-pytest-cov==2.7.1
-pytest-forked==0.2
-pytest-mocha==0.3.0
-pytest-testmon==0.9.18
-pytest-watch==4.2.0
-pytest-xdist==1.24.0
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 56d239d..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,8 +0,0 @@
-click
-numpy
-scipy
-matplotlib
-requests
-scikit-learn
-torch==1.2.0
-torchvision==0.4.0
diff --git a/tests/contrib/test_pretrained.py b/tests/contrib/test_pretrained.py
index d5c8042..969231d 100644
--- a/tests/contrib/test_pretrained.py
+++ b/tests/contrib/test_pretrained.py
@@ -1,5 +1,5 @@
-import torch
 import pytest
+import torch
 from torch import nn
 
 import yann
@@ -14,21 +14,21 @@ except ImportError:
 
 
 @pytest.mark.skipif(
-  not PRETRAINED_INSTALLED,
-  reason='pretrainedmodels not instaled')
+  not PRETRAINED_INSTALLED, reason="pretrainedmodels not instaled"
+)
 def test_pretrained():
   timer = Timer(log=True)
 
-  with timer.task('initialize model'):
+  with timer.task("initialize model"):
     model = squeezenet1_1(1000, pretrained=None)
 
   wrapped = PretrainedModelWrapper(model, activation=nn.Softmax())
 
   inputs = torch.rand(1, 3, 255, 255)
 
-  with yann.eval_mode(wrapped), timer.task('model predict'):
+  with yann.eval_mode(wrapped), timer.task("model predict"):
     outputs = wrapped.predict(inputs)
-  with yann.eval_mode(model), timer.task('model predict'):
+  with yann.eval_mode(model), timer.task("model predict"):
     model_outputs = model(inputs)
 
   assert torch.all(outputs.logits == model_outputs)
diff --git a/tests/data/io/test_download.py b/tests/data/io/test_download.py
index 2564443..66aa92f 100644
--- a/tests/data/io/test_download.py
+++ b/tests/data/io/test_download.py
@@ -54,4 +54,4 @@
 #
 #
 #   dataset_downloader.get_all(['1', '2', '3'])
-#   dataset_downloader.stream(['1', '2', '3'])
\ No newline at end of file
+#   dataset_downloader.stream(['1', '2', '3'])
diff --git a/tests/data/io/test_io.py b/tests/data/io/test_io.py
index 7524b0d..a706b38 100644
--- a/tests/data/io/test_io.py
+++ b/tests/data/io/test_io.py
@@ -1,20 +1,19 @@
-import yann
 import torch
 
+import yann
+
 
 def test_io(tmpdir):
-  x = {
-    'a': 2
-  }
+  x = {"a": 2}
 
-  yann.save(x, tmpdir / 'x.json')
-  y = yann.load(tmpdir / 'x.json')
+  yann.save(x, tmpdir / "x.json")
+  y = yann.load(tmpdir / "x.json")
   assert x == y
 
-  yann.save(x, tmpdir / 'x.th')
-  y = yann.load(tmpdir / 'x.th')
+  yann.save(x, tmpdir / "x.th")
+  y = yann.load(tmpdir / "x.th")
   assert x == y
 
-  yann.save(x, tmpdir / 'x.pickle')
-  y = yann.load(tmpdir / 'x.pickle')
-  assert x == y
\ No newline at end of file
+  yann.save(x, tmpdir / "x.pickle")
+  y = yann.load(tmpdir / "x.pickle")
+  assert x == y
diff --git a/tests/data/test_classes.py b/tests/data/test_classes.py
index 0c6dda0..926c754 100644
--- a/tests/data/test_classes.py
+++ b/tests/data/test_classes.py
@@ -1,7 +1,6 @@
 from yann.data.classes import Classes, get_class_weights
 
 
-
 def test_classes():
   classes = Classes.ordered(10)
 
@@ -9,22 +8,16 @@ def test_classes():
   assert 1 in classes
   assert classes[1] == 1
 
-  classes = Classes(['a', 'b', 'c'])
-
-  assert 'a' in classes
-  assert classes.indices['a'] == 0
-  assert classes.index_encode('a') == 0
-  assert classes.one_hot_encode('a')[0] == 1
+  classes = Classes(["a", "b", "c"])
 
+  assert "a" in classes
+  assert classes.indices["a"] == 0
+  assert classes.index_encode("a") == 0
+  assert classes.one_hot_encode("a")[0] == 1
 
-  classes = Classes(counts={'a': 10, 'b': 20})
-  assert classes.weights() == [30/10, 30/20]
+  classes = Classes(counts={"a": 10, "b": 20})
+  assert classes.weights() == [30 / 10, 30 / 20]
 
 
 def test_class_weights():
-  counts = {
-    0: 4,
-    1: 5,
-    2: 10
-  }
-
+  counts = {0: 4, 1: 5, 2: 10}
diff --git a/tests/data/test_containers.py b/tests/data/test_containers.py
index bf59d5d..3be642c 100644
--- a/tests/data/test_containers.py
+++ b/tests/data/test_containers.py
@@ -10,13 +10,13 @@ class TestContainer:
     assert c[:2] == [1, 2]
 
     assert c.args == (1, 2, 3)
-    assert c.kwargs == {'x': 5, 'y': 6}
+    assert c.kwargs == {"x": 5, "y": 6}
 
     c[3] = 9
     assert c.x == 9
 
-    input = Inputs(image='sfsd', mask=[0, 1, 2, 3])
+    input = Inputs(image="sfsd", mask=[0, 1, 2, 3])
 
     image, *rest = input
 
-    assert image == input.image == 'sfsd'
+    assert image == input.image == "sfsd"
diff --git a/tests/data/test_place.py b/tests/data/test_place.py
index a9aa870..b6ed8bc 100644
--- a/tests/data/test_place.py
+++ b/tests/data/test_place.py
@@ -1,18 +1,18 @@
-import torch
 import pytest
+import torch
 
 from yann.data.place import Place
 
+devices = ["cpu", "cuda"] if torch.cuda.is_available() else ["cpu"]
 
-devices = ['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']
 
-
-@pytest.mark.parametrize('device', devices)
+@pytest.mark.parametrize("device", devices)
 def test_place(device):
   import torch
-  tuple_batch = (torch.rand(3,3), torch.rand(3,1), 'foo')
 
-  place = Place(('cpu', device))
+  tuple_batch = (torch.rand(3, 3), torch.rand(3, 1), "foo")
+
+  place = Place(("cpu", device))
   b = place(tuple_batch)
   # assert b[0].device == 'cpu'
-  assert b[2] == 'foo'
\ No newline at end of file
+  assert b[2] == "foo"
diff --git a/tests/modules/conv/test_mixconv.py b/tests/modules/conv/test_mixconv.py
index dd14d97..210f418 100644
--- a/tests/modules/conv/test_mixconv.py
+++ b/tests/modules/conv/test_mixconv.py
@@ -1,11 +1,11 @@
-from yann.modules.conv.mixconv import MixConv
+import pytest
 import torch
 
-import pytest
+from yann.modules.conv.mixconv import MixConv
 
 
 def test_mixconv():
-  c = MixConv(10, 20, (3,5,7))
+  c = MixConv(10, 20, (3, 5, 7))
 
   assert c.input_channel_counts == [4, 3, 3]
   assert c.output_channel_counts == [8, 6, 6]
@@ -16,6 +16,7 @@ def test_mixconv():
 
   assert x.shape == torch.Size([8, 20, 32, 32])
 
+
 def test_not_depthwise():
   c = MixConv(10, 20, (3, 5), depthwise=False)
   t = torch.rand((8, 10, 32, 32))
@@ -34,10 +35,12 @@ def test_1_group():
   t = torch.rand((10, 4, 32, 32))
   c(t)
 
+
 def test_auto_groups():
-  c = MixConv((2,2,2), (2,2,2))
+  c = MixConv((2, 2, 2), (2, 2, 2))
+
+  assert c.kernel_sizes == [3, 5, 7]
 
-  assert c.kernel_sizes == [3,5,7]
 
 def test_variable_input_channel_counts():
   c = MixConv((16, 8, 4, 4), (16, 8, 4, 4), (3, 5, 7, 9))
@@ -56,4 +59,4 @@ def test_variable_input_channel_counts():
   c(t)
 
   c = MixConv((16, 8, 4, 4), (16, 16, 4, 4), (3, 5, 7))
-  c(t)
\ No newline at end of file
+  c(t)
diff --git a/tests/modules/conv/test_utils.py b/tests/modules/conv/test_utils.py
index 747c579..cf2ebe1 100644
--- a/tests/modules/conv/test_utils.py
+++ b/tests/modules/conv/test_utils.py
@@ -5,4 +5,3 @@ def test_get_same_padding():
   assert get_same_padding(3) == 1
   assert get_same_padding(5) == 2
   assert get_same_padding(7) == 3
-
diff --git a/tests/modules/test_loss.py b/tests/modules/test_loss.py
index dbb4ab1..50fa7ae 100644
--- a/tests/modules/test_loss.py
+++ b/tests/modules/test_loss.py
@@ -1,107 +1,77 @@
-from yann.modules.loss import binary_focal_loss
 import torch
 from torch.nn.functional import binary_cross_entropy_with_logits
 
+from yann.modules.loss import binary_focal_loss
 from yann.testing import check_tensor
 
 T = torch.tensor
 
 
 def test_focal_loss():
-  logits = torch.Tensor([
-      [-10, 2.3],
-      [-10, 5.5]
-    ])
+  logits = torch.Tensor([[-10, 2.3], [-10, 5.5]])
 
-  targets = torch.Tensor([
-      [1.0, 0.0],
-      [.0, 1.0]
-    ])
+  targets = torch.Tensor([[1.0, 0.0], [0.0, 1.0]])
 
   assert torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='none', gamma=0, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='none')
-  ), "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+    binary_focal_loss(logits, targets, reduction="none", gamma=0, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction="none"),
+  ), (
+    "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+  )
 
   assert torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='mean', gamma=0, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='mean')
-  ), "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+    binary_focal_loss(logits, targets, reduction="mean", gamma=0, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction="mean"),
+  ), (
+    "focal loss with gamma == 0 and no alpha should be same as binary cross entropy loss"
+  )
 
   assert not torch.allclose(
-    binary_focal_loss(
-      logits, targets, reduction='none', gamma=.4, alpha=None),
-    binary_cross_entropy_with_logits(
-      logits, targets, reduction='none')
+    binary_focal_loss(logits, targets, reduction="none", gamma=0.4, alpha=None),
+    binary_cross_entropy_with_logits(logits, targets, reduction="none"),
   ), "focal loss with gamma > 0 should not be same as binary cross entropy loss"
 
-
   assert binary_focal_loss(
-    T([[100.0]]),
-    T([[1.0]]),
-    gamma=4
-  ) == binary_focal_loss(
-    T([[-100.0]]),
-    T([[0.0]]),
-    gamma=4
-  ), "loss should be symmetrical"
+    T([[100.0]]), T([[1.0]]), gamma=4
+  ) == binary_focal_loss(T([[-100.0]]), T([[0.0]]), gamma=4), (
+    "loss should be symmetrical"
+  )
 
-  assert binary_focal_loss(
-    T([[100.0]]),
-    T([[0.0]])
-  ) == binary_focal_loss(
-    T([[-100.0]]),
-    T([[1.0]])
+  assert binary_focal_loss(T([[100.0]]), T([[0.0]])) == binary_focal_loss(
+    T([[-100.0]]), T([[1.0]])
   )
 
-  assert binary_focal_loss(
-    T([[2.0]]),
-    T([[1.0]]),
-    gamma=1
-  ) > binary_focal_loss(
-    T([[2.0]]),
-    T([[1.0]]),
-    gamma=2
+  assert binary_focal_loss(T([[2.0]]), T([[1.0]]), gamma=1) > binary_focal_loss(
+    T([[2.0]]), T([[1.0]]), gamma=2
   ), "larger gamma should reduce loss for well classified examples"
 
   assert binary_focal_loss(
-    T([[2.0]]),
-    T([[1.0]]),
-    gamma=1
+    T([[2.0]]), T([[1.0]]), gamma=1
   ) < binary_cross_entropy_with_logits(
     T([[2.0]]),
     T([[1.0]]),
-  ), "focal loss should penalize well classified examples less than binary cross entropy"
+  ), (
+    "focal loss should penalize well classified examples less than binary cross entropy"
+  )
 
   for g in range(0, 10):
     for logit in range(-1_000, 1_000, 100):
       check_tensor(
-        binary_focal_loss(
-          T([[float(logit)]]),
-          T([[1.0]]),
-          gamma=g
-        ),
+        binary_focal_loss(T([[float(logit)]]), T([[1.0]]), gamma=g),
         anomalies=True,
         gte=0,
       )
   check_tensor(
-    binary_focal_loss(
-      T([[-.3]]),
-      T([[.2]]),
-      gamma=2
-    ),
+    binary_focal_loss(T([[-0.3]]), T([[0.2]]), gamma=2),
     anomalies=True,
     gte=0,
   )
 
+
 def test_focal_loss_gradients():
   logits = T([[0.0], [0.0]], requires_grad=True)
   targets = T([[0.0], [0.0]])
 
   loss = binary_focal_loss(logits, targets)
   loss.backward()
-  check_tensor(logits.grad, anomalies=True)
\ No newline at end of file
+  check_tensor(logits.grad, anomalies=True)
diff --git a/tests/test_callbacks.py b/tests/test_callbacks.py
index 41e5b9d..4bd80f8 100644
--- a/tests/test_callbacks.py
+++ b/tests/test_callbacks.py
@@ -1,12 +1,9 @@
 from yann import callbacks
 
 
-
 def test():
   cbs = callbacks.get_callbacks(
-    checkpoint={'freq': 5},
-    plot=True,
-    progress=False
+    checkpoint={"freq": 5}, plot=True, progress=False
   )
 
   assert not any(isinstance(x, callbacks.ProgressBar) for x in cbs)
@@ -15,4 +12,4 @@ def test():
 
   for cb in cbs:
     if isinstance(cb, callbacks.Checkpoint):
-      assert cb.freq == 5
\ No newline at end of file
+      assert cb.freq == 5
diff --git a/tests/test_export.py b/tests/test_export.py
index ddcd89a..beebcd7 100644
--- a/tests/test_export.py
+++ b/tests/test_export.py
@@ -35,10 +35,9 @@ def test_export_traced(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))]
+  )
 
   path = tmpdir
 
@@ -52,16 +51,15 @@ def test_export_traced(tmpdir):
     model=model,
     trace=torch.rand(1, 1, 32, 32),
     preprocess=preprocess,
-    classes=classes
-
+    classes=classes,
   )
 
   expected_files = [
-    'model.traced.th',
-    'preprocess.pkl',
-    'classes.json',
-    'requirements.txt',
-    'env.yml'
+    "model.traced.th",
+    "preprocess.pkl",
+    "classes.json",
+    "requirements.txt",
+    "env.yml",
   ]
 
   for name in expected_files:
@@ -90,10 +88,9 @@ def test_export_state_dict(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))]
+  )
 
   path = tmpdir
 
@@ -105,13 +102,13 @@ def test_export_state_dict(tmpdir):
     model=model,
     state_dict=True,
     preprocess=preprocess,
-    classes=[str(n) for n in range(NUM_CLASSES)]
+    classes=[str(n) for n in range(NUM_CLASSES)],
   )
 
   expected_files = [
-    'model.state_dict.th',
-    'preprocess.pkl',
-    'classes.json',
+    "model.state_dict.th",
+    "preprocess.pkl",
+    "classes.json",
   ]
 
   for name in expected_files:
@@ -124,10 +121,9 @@ def test_export_pickled(tmpdir):
   NUM_CLASSES = 10
   model = Net(NUM_CLASSES)
 
-  preprocess = transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.4,), (0.4,))
-  ])
+  preprocess = transforms.Compose(
+    [transforms.ToTensor(), transforms.Normalize((0.4,), (0.4,))]
+  )
 
   path = tmpdir
 
@@ -139,14 +135,13 @@ def test_export_pickled(tmpdir):
     model=model,
     state_dict=False,
     preprocess=preprocess,
-    classes=[str(n) for n in range(NUM_CLASSES)]
-
+    classes=[str(n) for n in range(NUM_CLASSES)],
   )
 
   expected_files = [
-    'model.th',
-    'preprocess.pkl',
-    'classes.json',
+    "model.th",
+    "preprocess.pkl",
+    "classes.json",
   ]
 
   for name in expected_files:
diff --git a/tests/test_params.py b/tests/test_params.py
index 1b93df4..96b6be3 100644
--- a/tests/test_params.py
+++ b/tests/test_params.py
@@ -4,47 +4,45 @@ from yann import params
 def test():
   class Params(params.HyperParams):
     a = 4
-    b = 'b'
-
+    b = "b"
 
   p = Params()
   assert p.a == 4
-  assert p.b == 'b'
+  assert p.b == "b"
 
   assert len(p) == 2
 
   p = Params(a=3)
   assert p.a == 3
-  assert p.b == 'b'
+  assert p.b == "b"
 
-  assert p['a', 'b'] == (3, 'b')
+  assert p["a", "b"] == (3, "b")
 
   for k in p:
-    assert k in ('a', 'b')
+    assert k in ("a", "b")
 
-  assert 'a' in p
-  assert 'x' not in p
+  assert "a" in p
+  assert "x" not in p
 
 
 def test_serialization(tmpdir):
-
   class Params(params.HyperParams):
     a = 4
-    b = 'b'
+    b = "b"
 
   p = Params()
 
-  p.save(tmpdir / 'params.json')
-  p2 = p.load(tmpdir / 'params.json')
-  assert (tmpdir / 'params.json').exists()
+  p.save(tmpdir / "params.json")
+  p2 = p.load(tmpdir / "params.json")
+  assert (tmpdir / "params.json").exists()
   assert p == p2
 
-  p.save(tmpdir / 'params.yaml')
-  p2 = p.load(tmpdir / 'params.yaml')
-  assert (tmpdir / 'params.yaml').exists()
+  p.save(tmpdir / "params.yaml")
+  p2 = p.load(tmpdir / "params.yaml")
+  assert (tmpdir / "params.yaml").exists()
   assert p == p2
 
-  p.save(tmpdir / 'params.pkl')
-  p2 = p.load(tmpdir / 'params.pkl')
-  assert (tmpdir / 'params.pkl').exists()
-  assert p == p2
\ No newline at end of file
+  p.save(tmpdir / "params.pkl")
+  p2 = p.load(tmpdir / "params.pkl")
+  assert (tmpdir / "params.pkl").exists()
+  assert p == p2
diff --git a/tests/test_train.py b/tests/test_train.py
index ed21cd9..29f6112 100644
--- a/tests/test_train.py
+++ b/tests/test_train.py
@@ -4,19 +4,22 @@ from torch import nn
 from torch.optim import SGD
 
 from yann.callbacks import (
-  History, HistoryPlotter, HistoryWriter, Logger, Checkpoint
+  Checkpoint,
+  History,
+  HistoryPlotter,
+  HistoryWriter,
+  Logger,
 )
 from yann.datasets import TinyDigits
 from yann.datasets.wrappers import Slice
 from yann.modules import Flatten
 from yann.train import Trainer
 
-
-devices = ['cpu', 'cuda'] if torch.cuda.is_available() else ['cpu']
+devices = ["cpu", "cuda"] if torch.cuda.is_available() else ["cpu"]
 
 
 @pytest.mark.slow
-@pytest.mark.parametrize('device', devices)
+@pytest.mark.parametrize("device", devices)
 def test_train(tmpdir, device):
   """Sanity check train run"""
 
@@ -26,7 +29,7 @@ def test_train(tmpdir, device):
     nn.Conv2d(20, 20, 3),
     nn.ReLU(inplace=True),
     Flatten(),
-    nn.Linear(320, 10)
+    nn.Linear(320, 10),
   )
 
   train = Trainer(
@@ -34,15 +37,17 @@ def test_train(tmpdir, device):
     model=model,
     dataset=Slice(TinyDigits(), 0, 256),
     device=device,
-    optimizer=SGD(model.parameters(), lr=.01, momentum=0.9, weight_decay=.001),
+    optimizer=SGD(
+      model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001
+    ),
     loss=nn.CrossEntropyLoss(),
     callbacks=[
       History(),
       HistoryPlotter(save=True),
       HistoryWriter(),
       Logger(batch_freq=20),
-      Checkpoint()
-    ]
+      Checkpoint(),
+    ],
   )
 
   train(2)
@@ -56,7 +61,6 @@ def test_train(tmpdir, device):
   assert export_path.is_dir()
 
 
-
 def test_interface():
   train = Trainer()
 
@@ -65,7 +69,6 @@ def test_interface():
   train.paths
 
 
-
 # @pytest.mark.slow
 # @pytest.mark.parametrize('device', devices)
 # def test_train_resolved(tmpdir, device):
@@ -90,4 +93,4 @@ def test_interface():
 #       'mask': 'foo',
 #       'label': 'foo'
 #     }
-#   )
\ No newline at end of file
+#   )
diff --git a/tests/utils/test_registry.py b/tests/utils/test_registry.py
index 1374bdb..fd6bdfc 100644
--- a/tests/utils/test_registry.py
+++ b/tests/utils/test_registry.py
@@ -11,11 +11,11 @@ def test_registry():
   r = Registry()
 
   assert len(r) == 0, "Registry didn't start out empty"
-  r.register(MNIST, name='mnist')
+  r.register(MNIST, name="mnist")
   assert len(r) == 1
 
-  assert 'mnist' in r._records
-  assert r._records['mnist'].x is MNIST
+  assert "mnist" in r._records
+  assert r._records["mnist"].x is MNIST
 
   r2 = Registry(types=(Dataset,))
 
@@ -44,9 +44,9 @@ def test():
   yann.register = registry
   yann.resolve = registry.resolve
 
-  yann.register(MNIST, 'mnist')
+  yann.register(MNIST, "mnist")
 
-  assert yann.registry.resolve('mnist', instance=False) is MNIST
+  assert yann.registry.resolve("mnist", instance=False) is MNIST
 
   yann.register.dataset(MNIST)
 
@@ -54,7 +54,9 @@ def test():
 
   yann.register.optimizers(
     SGD,
-    init=lambda SGD, *args, parameters=None, **kwargs: SGD(parameters, )
+    init=lambda SGD, *args, parameters=None, **kwargs: SGD(
+      parameters,
+    ),
   )
 
 
@@ -66,12 +68,12 @@ def test_indexing():
   yann.resolve = registry.resolve
 
   yann.registry.dataset.index(datasets, types=(Dataset,))
-  assert yann.resolve.dataset('MNIST', instance=False) is datasets.MNIST
+  assert yann.resolve.dataset("MNIST", instance=False) is datasets.MNIST
 
   assert yann.registry.has(datasets.MNIST)
-  assert 'MNIST' in yann.registry
-  assert 'MNIST' in yann.registry.dataset
-  assert 'MNIST' not in yann.registry.loss
+  assert "MNIST" in yann.registry
+  assert "MNIST" in yann.registry.dataset
+  assert "MNIST" not in yann.registry.loss
 
   class CustomDataset(Dataset):
     def __init__(self, required_arg):
@@ -79,22 +81,22 @@ def test_indexing():
 
   yann.register.dataset(CustomDataset)
 
-  dset = CustomDataset('value')
+  dset = CustomDataset("value")
   assert yann.resolve.dataset(dset, instance=True) is dset
 
   @yann.register.dataset
   class Foobar(Dataset):
     pass
 
-  assert yann.resolve.dataset('Foobar', instance=False) is Foobar
+  assert yann.resolve.dataset("Foobar", instance=False) is Foobar
 
-  @yann.register('ReLU')
+  @yann.register("ReLU")
   def relu(x):
     return max(0, x)
 
-  assert yann.resolve('ReLU') is relu
-  assert yann.resolve('ReLU')(2) == 2
-  assert yann.resolve('ReLU')(-1) == 0
+  assert yann.resolve("ReLU") is relu
+  assert yann.resolve("ReLU")(2) == 2
+  assert yann.resolve("ReLU")(-1) == 0
 
   assert yann.resolve(relu)(-1) == 0
 
@@ -106,8 +108,7 @@ def test_yann_registry():
   assert len(yann.registry.loss)
   assert len(yann.registry.optimizer)
 
-  yann.resolve('MNIST', required=True)
-
+  yann.resolve("MNIST", required=True)
 
 
 def test_tuple_arg():
@@ -118,5 +119,5 @@ def test_tuple_arg():
     def __init__(self, **kwargs):
       self.kwargs = kwargs
 
-  foo = yann.resolve(('Foo', {'test': True}))
-  assert foo.kwargs['test'] == True
\ No newline at end of file
+  foo = yann.resolve(("Foo", {"test": True}))
+  assert foo.kwargs["test"] == True
diff --git a/yann/__init__.py b/yann/__init__.py
index 62c1604..f311e1c 100644
--- a/yann/__init__.py
+++ b/yann/__init__.py
@@ -1,34 +1,28 @@
 import typing
 from contextlib import contextmanager
 
-__version__ = '0.0.40'
+__version__ = "0.0.40"
 
 from typing import Union
 
+import numpy as np
 import torch
 from torch import nn
-import numpy as np
-
-from .config.defaults import default
-from .config.setup import registry
-
-register = registry
-resolve = registry.resolve
 
 from pathlib import Path
 
-from yann.utils import to_numpy, repeat, counter, is_notebook, timeout
-from yann.data import batches, shuffle, chunk
+from yann.data import batches, chunk, shuffle
 from yann.data.io import load, save
 from yann.data.io.download import download
+from yann.data.loaders import loader
 from yann.data.utils import pad, pad_to_largest
-# from yann.data import datasets
-from yann.viz import show, plot
+from yann.testing import Checker
+from yann.utils import counter, is_notebook, repeat, timeout, to_numpy
+from yann.utils.profile import param_count, profile
 from yann.utils.timer import time
-from yann.utils.profile import profile, param_count
 
-from yann.testing import Checker
-from yann.data.loaders import loader
+# from yann.data import datasets
+from yann.viz import plot, show
 
 # T = torch.Tensor
 #
@@ -80,27 +74,32 @@ context = object()
 memory_formats = dict(
   contiguous_format=torch.contiguous_format,
   channels_last=torch.channels_last,
-  preserve_format=torch.preserve_format
+  preserve_format=torch.preserve_format,
 )
 
+
 def to_tensor(
-    x: Union[list, tuple, np.ndarray, torch.Tensor, 'PIL.Image.Image']
+  x: Union[list, tuple, np.ndarray, torch.Tensor, "PIL.Image.Image"],
 ) -> torch.Tensor:
   if torch.is_tensor(x):
     return x
   if isinstance(x, np.ndarray):
     return torch.from_numpy(x)
   import PIL.Image
+
   if isinstance(x, PIL.Image.Image):
     from torchvision.transforms import functional as F
+
     return F.to_tensor(x)
   return torch.Tensor(x)
 
 
 def seed(val=1, deterministic=False):
+  import random
+
   import numpy as np
   import torch
-  import random
+
   random.seed(val)
   np.random.seed(val)
   torch.manual_seed(val)
@@ -127,11 +126,13 @@ def get_item(x: Union[torch.Tensor, np.ndarray]):
 
 def benchmark():
   from torch.backends import cudnn
+
   cudnn.benchmark = True
 
 
 def detect_anomalies(val=True):
   import torch.autograd
+
   torch.autograd.set_detect_anomaly(val)
 
 
@@ -151,7 +152,7 @@ def evaluate(model, batches, device=None, transform=None):
     yield x, y, pred
 
 
-def predict_multicrop(model, inputs, reduce='mean'):
+def predict_multicrop(model, inputs, reduce="mean"):
   batch_size, num_crops, *sample_shape = inputs.shape
   flat_preds = model(inputs.view(-1, *sample_shape))
   outputs = flat_preds.view(batch_size, num_crops, -1)
@@ -170,7 +171,7 @@ class Multicrop(torch.nn.Module):
 
 
 def set_param(x, param, val):
-  if hasattr(x, 'param_groups'):
+  if hasattr(x, "param_groups"):
     for group in x.param_groups:
       group[param] = val
   else:
@@ -178,7 +179,7 @@ def set_param(x, param, val):
 
 
 def scale_param(x, param, mult):
-  if hasattr(x, 'param_groups'):
+  if hasattr(x, "param_groups"):
     for group in x.param_groups:
       group[param] *= mult
   else:
@@ -191,15 +192,16 @@ def group_params(model, get_key):
     splits[get_key(name, param)] = param
   return splits
 
+
 from torch.nn.modules.batchnorm import _BatchNorm
 
 
 def split_regularization_params(
-    module: nn.Module,
-    excluded_modules=(_BatchNorm,),
-    excluded_names=('bias',),
-    param_groups=True,
-    weight_decay=1e-4
+  module: nn.Module,
+  excluded_modules=(_BatchNorm,),
+  excluded_names=("bias",),
+  param_groups=True,
+  weight_decay=1e-4,
 ):
   """
   filter out parameters which should not be regularized
@@ -217,7 +219,10 @@ def split_regularization_params(
           else:
             reg.append(param)
   if param_groups:
-    return [dict(params=reg, weight_decay=weight_decay), dict(params=no_reg, weight_decay=0)]
+    return [
+      dict(params=reg, weight_decay=weight_decay),
+      dict(params=no_reg, weight_decay=0),
+    ]
   else:
     return reg, no_reg
 
@@ -241,7 +246,8 @@ def freeze(x, exclude=None):
     raise ValueError(
       "can't exclude modules if parameters are passed, "
       "pass an instance of nn.Module if you need to "
-      "exclude certain modules")
+      "exclude certain modules"
+    )
   else:
     for p in x:
       p.requires_grad = False
@@ -280,17 +286,16 @@ def replace_linear(model, num_outputs, layer_name=None):
     if len(linear_layers) == 1:
       layer_name = linear_layers[0][0]
     elif len(linear_layers) == 0:
-      raise ValueError('No linear layers found in model')
+      raise ValueError("No linear layers found in model")
     else:
       raise ValueError(
-        f'Multiple linear layers found and layer name was not provided, '
-        f'provide a valid layer_name, '
-        f'(valid names: {", ".join([n for n, m in linear_layers])})'
+        f"Multiple linear layers found and layer name was not provided, "
+        f"provide a valid layer_name, "
+        f"(valid names: {', '.join([n for n, m in linear_layers])})"
       )
 
-
-  if '.' in layer_name:
-    *path, layer_name = list(layer_name.split('.'))
+  if "." in layer_name:
+    *path, layer_name = list(layer_name.split("."))
     for p in path:
       model = getattr(model, p)
 
@@ -298,11 +303,7 @@ def replace_linear(model, num_outputs, layer_name=None):
   new_linear = nn.Linear(old_linear.in_features, num_outputs)
   new_linear.to(old_linear.weight.device)
 
-  setattr(
-    model,
-    layer_name,
-    new_linear
-  )
+  setattr(model, layer_name, new_linear)
 
   return layer_name
 
@@ -331,7 +332,8 @@ def eval_mode(*modules, grad=False):
   if grad:
     training = (m.training for m in modules)
     try:
-      for m in modules: m.eval()
+      for m in modules:
+        m.eval()
       yield
     finally:
       for m, train in zip(modules, training):
@@ -341,7 +343,8 @@ def eval_mode(*modules, grad=False):
     with torch.no_grad():
       training = (m.training for m in modules)
       try:
-        for m in modules: m.eval()
+        for m in modules:
+          m.eval()
         yield
       finally:
         for m, train in zip(modules, training):
@@ -378,7 +381,7 @@ def optim_step(optimizer, zero_grad=True):
 
 def to(items, **kwargs):
   """call `.to()` on all items that have a `to()` method, skips ones that don't"""
-  if hasattr(items, 'to'):
+  if hasattr(items, "to"):
     return items.to(**kwargs)
   elif isinstance(items, dict):
     return {k: to(v, **kwargs) for k, v in items.items()}
@@ -399,23 +402,24 @@ def get_device(module):
 
 def get_trainer(params=None, **kwargs):
   from yann.train import Trainer
+
   return Trainer(params=params, **kwargs)
 
 
 def get_model_name(model):
-  if isinstance(model, (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel)):
+  if isinstance(
+    model, (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel)
+  ):
     model = model.module
 
-  if hasattr(model, 'name'):
+  if hasattr(model, "name"):
     return model.name
 
   return model.__class__.__name__
 
 
 def load_state_dict(
-    x,
-    state_dict: Union[str, 'pathlib.Path', dict],
-    strict: bool = True
+  x, state_dict: Union[str, "pathlib.Path", dict], strict: bool = True
 ):
   if not isinstance(state_dict, dict):
     state_dict = yann.load(state_dict)
@@ -423,7 +427,6 @@ def load_state_dict(
   return x.load_state_dict(state_dict, strict=strict)
 
 
-
 def grad_norm(parameters, norm_type: float = 2.0):
   """
 
@@ -444,7 +447,7 @@ def grad_norm(parameters, norm_type: float = 2.0):
 
   norm_type = float(norm_type)
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   device = parameters[0].grad.device
   try:
     from torch import inf
@@ -456,10 +459,10 @@ def grad_norm(parameters, norm_type: float = 2.0):
     norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
   else:
     norm = torch.norm(
-      torch.stack([
-        torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters
-      ]),
-      norm_type
+      torch.stack(
+        [torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]
+      ),
+      norm_type,
     )
   return norm
 
@@ -473,7 +476,7 @@ def param_norm(parameters, norm_type: float = 2.0):
   parameters = list(parameters)
   norm_type = float(norm_type)
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   device = parameters[0].device
   try:
     from torch import inf
@@ -484,16 +487,16 @@ def param_norm(parameters, norm_type: float = 2.0):
     norm = norms[0] if len(norms) == 1 else torch.max(torch.stack(norms))
   else:
     norm = torch.norm(
-      torch.stack([
-        torch.norm(p.detach(), norm_type).to(device) for p in parameters
-      ]),
-      norm_type
+      torch.stack(
+        [torch.norm(p.detach(), norm_type).to(device) for p in parameters]
+      ),
+      norm_type,
     )
   return norm
 
 
 def nested_lookup(obj, key):
-  keys = key.split('.')
+  keys = key.split(".")
 
   for k in keys:
     if isinstance(obj, typing.Mapping):
@@ -503,12 +506,21 @@ def nested_lookup(obj, key):
     elif hasattr(obj, k):
       obj = getattr(obj, k)
     else:
-      raise KeyError(f'{key} not found')
+      raise KeyError(f"{key} not found")
   return obj
 
 
-import yann.params
+
+from yann.config.defaults import default
+from yann.config.setup import registry
+
+register = registry
+resolve = registry.resolve
+
+import yann.callbacks
 import yann.metrics
+import yann.optim
+import yann.params
 import yann.train
-import yann.callbacks
-import yann.optim
\ No newline at end of file
+
+
diff --git a/yann/callbacks/__init__.py b/yann/callbacks/__init__.py
index 0aa232c..5bde8a5 100644
--- a/yann/callbacks/__init__.py
+++ b/yann/callbacks/__init__.py
@@ -1,19 +1,20 @@
-from .base import FunctionCallback, Callback
+from yann.utils import is_notebook
+
+from .base import Callback, FunctionCallback
+from .callbacks import Callbacks
 from .checkpoint import Checkpoint
 from .eval import MulticlassEval
 from .history import History, HistoryPlotter, HistoryWriter
 from .logging import Logger
-from .stop import StopOnNaN
-from .timing import Timing
-from .progbar import ProgressBar
-from .wandb import Wandb
+
 # from .ema import EMA
 # from .swa import SWA
 from .lr import LRRangeTest
+from .progbar import ProgressBar
+from .stop import StopOnNaN
+from .timing import Timing
+from .wandb import Wandb
 
-from .callbacks import Callbacks
-
-from yann.utils import is_notebook
 
 def _maybe_init(value, cls, **kwargs):
   if value is None or value is False:
@@ -24,26 +25,27 @@ def _maybe_init(value, cls, **kwargs):
 
 
 def get_callbacks(
-    interactive=None,
-    plot=True,
-    write=True,
-    log=True,
-    checkpoint=True,
-    time=False,
-    progress=True,
-    tensorboard=True,
+  interactive=None,
+  plot=True,
+  write=True,
+  log=True,
+  checkpoint=True,
+  time=False,
+  progress=True,
+  tensorboard=True,
 ):
-
   if interactive is None:
     interactive = is_notebook()
 
   if tensorboard:
     from .tensorboard import Tensorboard
+
     tb = _maybe_init(tensorboard, Tensorboard)
   else:
     tb = None
   return [
-    x for x in (
+    x
+    for x in (
       # History(),
       _maybe_init(progress, ProgressBar, notebook=interactive),
       _maybe_init(plot, HistoryPlotter, save=not interactive),
@@ -51,5 +53,7 @@ def get_callbacks(
       _maybe_init(checkpoint, Checkpoint),
       _maybe_init(log, Logger),
       _maybe_init(time, Timing),
-      tb
-    ) if x]
+      tb,
+    )
+    if x
+  ]
diff --git a/yann/callbacks/base.py b/yann/callbacks/base.py
index b9900a7..4f9e856 100644
--- a/yann/callbacks/base.py
+++ b/yann/callbacks/base.py
@@ -21,8 +21,15 @@ class Callback:
   def on_step_start(self, index=None, inputs=None, targets=None, trainer=None):
     pass
 
-  def on_step_end(self, index=None, inputs=None, targets=None, outputs=None, loss=None,
-                   trainer=None):
+  def on_step_end(
+    self,
+    index=None,
+    inputs=None,
+    targets=None,
+    outputs=None,
+    loss=None,
+    trainer=None,
+  ):
     pass
 
   def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
@@ -31,11 +38,14 @@ class Callback:
   def on_validation_start(self, trainer=None):
     pass
 
-  def on_validation_batch(self, inputs=None, targets=None, outputs=None, trainer=None):
+  def on_validation_batch(
+    self, inputs=None, targets=None, outputs=None, trainer=None
+  ):
     pass
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     pass
 
   def on_train_end(self, trainer=None):
@@ -50,17 +60,17 @@ class Callback:
 
 class FunctionCallback(Callback):
   _valid_names = {
-    'init',
-    'train_start',
-    'epoch_start',
-    'step_start',
-    'step_end',
-    'epoch_end',
-    'validation_start',
-    'validation_batch',
-    'validation_end',
-    'train_end',
-    'error',
+    "init",
+    "train_start",
+    "epoch_start",
+    "step_start",
+    "step_end",
+    "epoch_end",
+    "validation_start",
+    "validation_batch",
+    "validation_end",
+    "train_end",
+    "error",
   }
 
   def __init__(self, callbacks=None):
@@ -69,53 +79,54 @@ class FunctionCallback(Callback):
   def on(self, event, callback):
     if event not in self._valid_names:
       raise ValueError(
-        f'{event} is not a valid option, '
-        f'must be one of {self._valid_names}')
+        f"{event} is not a valid option, must be one of {self._valid_names}"
+      )
     callbacks = self.callbacks[event]
     if callback not in callbacks:
       callbacks.append(callback)
 
   def on_init(self, *args, **kwargs):
-    for f in self.callbacks['init']:
+    for f in self.callbacks["init"]:
       f(*args, **kwargs)
+
   def on_train_start(self, *args, **kwargs):
-    for f in self.callbacks['train_start']:
+    for f in self.callbacks["train_start"]:
       f(*args, **kwargs)
 
   def on_epoch_start(self, *args, **kwargs):
-    for f in self.callbacks['epoch_start']:
+    for f in self.callbacks["epoch_start"]:
       f(*args, **kwargs)
 
   def on_step_start(self, *args, **kwargs):
-    for f in self.callbacks['step_start']:
+    for f in self.callbacks["step_start"]:
       f(*args, **kwargs)
 
   def on_step_end(self, *args, **kwargs):
-    for f in self.callbacks['step_end']:
+    for f in self.callbacks["step_end"]:
       f(*args, **kwargs)
 
   def on_epoch_end(self, *args, **kwargs):
-    for f in self.callbacks['epoch_end']:
+    for f in self.callbacks["epoch_end"]:
       f(*args, **kwargs)
 
   def on_validation_start(self, *args, **kwargs):
-    for f in self.callbacks['validation_start']:
+    for f in self.callbacks["validation_start"]:
       f(*args, **kwargs)
 
   def on_validation_batch(self, *args, **kwargs):
-    for f in self.callbacks['validation_batch']:
+    for f in self.callbacks["validation_batch"]:
       f(*args, **kwargs)
 
   def on_validation_end(self, *args, **kwargs):
-    for f in self.callbacks['validation_end']:
+    for f in self.callbacks["validation_end"]:
       f(*args, **kwargs)
 
   def on_train_end(self, *args, **kwargs):
-    for f in self.callbacks['train_end']:
+    for f in self.callbacks["train_end"]:
       f(*args, **kwargs)
 
   def on_error(self, *args, **kwargs):
-    for f in self.callbacks['error']:
+    for f in self.callbacks["error"]:
       f(*args, **kwargs)
 
 
@@ -134,4 +145,4 @@ class TempCallback(Callback):
 
   def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
     if self.epochs and self.epochs < epoch:
-      self.unregister()
\ No newline at end of file
+      self.unregister()
diff --git a/yann/callbacks/callbacks.py b/yann/callbacks/callbacks.py
index 4b718a0..9cd4890 100644
--- a/yann/callbacks/callbacks.py
+++ b/yann/callbacks/callbacks.py
@@ -1,27 +1,27 @@
 from collections import OrderedDict
 
 import yann.callbacks
-from yann.utils import camel_to_snake
 from yann.callbacks.base import Callback
+from yann.utils import camel_to_snake
 
 
 class Events:
-  init = 'init'
-  train_start = 'train_start'
-  train_end = 'train_end'
-  epoch_start = 'epoch_start'
-  epoch_end = 'epoch_end'
-  step_start = 'step_start'
-  step_end = 'step_end'
-  step_error = 'step_error'
-  error = 'error'
+  init = "init"
+  train_start = "train_start"
+  train_end = "train_end"
+  epoch_start = "epoch_start"
+  epoch_end = "epoch_end"
+  step_start = "step_start"
+  step_end = "step_end"
+  step_error = "step_error"
+  error = "error"
 
 
 def callback(method):
   def wrapped_method(self, *args, **kwargs):
     ret = method(self, *args, **kwargs)
     for callback_ in self:
-      if hasattr(callback_, 'enabled') and not callback_.enabled:
+      if hasattr(callback_, "enabled") and not callback_.enabled:
         continue
       if hasattr(callback_, method.__name__):
         getattr(callback_, method.__name__)(*args, **kwargs)
@@ -80,7 +80,9 @@ class Callbacks(Callback):
     return len(self._callbacks) > 0
 
   def __str__(self):
-    return f"Callbacks({', '.join(f'{k}={v}' for k,v in self._callbacks.items())}"
+    return (
+      f"Callbacks({', '.join(f'{k}={v}' for k, v in self._callbacks.items())}"
+    )
 
   def append(self, callback):
     name = self._get_name(callback)
@@ -90,13 +92,14 @@ class Callbacks(Callback):
     return camel_to_snake(x.__class__.__name__)
 
   def on(self, event, callback=None):
-    if 'function_callback' not in self._callbacks:
+    if "function_callback" not in self._callbacks:
       self.function_callback = yann.callbacks.FunctionCallback()
 
     if callback:
       self.function_callback.on(event, callback)
       return self
     else:
+
       def decorated(func):
         self.function_callback.on(event, func)
         return func
@@ -120,7 +123,9 @@ class Callbacks(Callback):
     pass
 
   @callback
-  def on_step_end(self, index: int, inputs, targets, outputs, loss, trainer=None):
+  def on_step_end(
+    self, index: int, inputs, targets, outputs, loss, trainer=None
+  ):
     pass
 
   @callback
@@ -144,10 +149,11 @@ class Callbacks(Callback):
     pass
 
   @callback
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     pass
 
   @callback
   def on_train_end(self, trainer=None):
     pass
-
diff --git a/yann/callbacks/checkpoint.py b/yann/callbacks/checkpoint.py
index 0852b68..e4ad017 100644
--- a/yann/callbacks/checkpoint.py
+++ b/yann/callbacks/checkpoint.py
@@ -1,4 +1,5 @@
 from collections import OrderedDict
+
 from .base import Callback
 
 
@@ -12,9 +13,13 @@ class Checkpoint(Callback):
     self.save_on_end = save_on_end
 
   def on_epoch_end(self, epoch, loss=None, metrics=None, trainer=None):
-    if epoch % self.freq == 0 :
+    if epoch % self.freq == 0:
       self.paths[trainer.num_steps] = trainer.checkpoint()
 
   def on_train_end(self, trainer=None):
-    if self.save_on_end and trainer.num_steps > 10 and trainer.num_steps not in self.paths:
+    if (
+      self.save_on_end
+      and trainer.num_steps > 10
+      and trainer.num_steps not in self.paths
+    ):
       self.paths[trainer.num_steps] = trainer.checkpoint()
diff --git a/yann/callbacks/eval.py b/yann/callbacks/eval.py
index 1cd768b..e84cd95 100644
--- a/yann/callbacks/eval.py
+++ b/yann/callbacks/eval.py
@@ -1,8 +1,7 @@
 import sys
-from sklearn.metrics import classification_report, accuracy_score
-from .base import Callback
 
 from ..metrics import get_preds
+from .base import Callback
 
 
 class MulticlassEval(Callback):
@@ -10,22 +9,27 @@ class MulticlassEval(Callback):
     self.dest = dest
 
   def __call__(self, targets=None, outputs=None, trainer=None, **kwargs):
-    if trainer and trainer.dataset and hasattr(trainer.dataset, 'classes'):
+    if trainer and trainer.dataset and hasattr(trainer.dataset, "classes"):
       classes = trainer.dataset.classes
     else:
       classes = None
 
     preds = get_preds(outputs)
 
-    preds, targets = preds.to('cpu').numpy(), targets.to('cpu').numpy()
+    preds, targets = preds.to("cpu").numpy(), targets.to("cpu").numpy()
+
+    from sklearn.metrics import accuracy_score, classification_report
 
-    print(classification_report(targets, preds, target_names=classes),
-          file=self.dest)
+    print(
+      classification_report(targets, preds, target_names=classes),
+      file=self.dest,
+    )
 
-    print('Accuracy: ', accuracy_score(targets, preds))
+    print("Accuracy: ", accuracy_score(targets, preds))
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None, **kwargs):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None, **kwargs
+  ):
     self(targets=targets, outputs=outputs, trainer=trainer)
 
 
diff --git a/yann/callbacks/history.py b/yann/callbacks/history.py
index f39e27c..92aa863 100644
--- a/yann/callbacks/history.py
+++ b/yann/callbacks/history.py
@@ -1,13 +1,14 @@
-from time import time as get_time
 from pathlib import Path
+from time import time as get_time
+
 import torch
 
-from ..utils.decorators import lazy
+from .. import get_item, resolve
 from ..callbacks.base import Callback
-from ..viz import plot_line
-from .. import resolve, get_item
-from ..data.metrics import MetricStore, EventStore, Event
+from ..data.metrics import Event, EventStore, MetricStore
 from ..evaluation import evaluate_metrics
+from ..utils.decorators import lazy
+from ..viz import plot_line
 
 
 class History(Callback):
@@ -26,18 +27,14 @@ class History(Callback):
     for m in metrics:
       if isinstance(m, str):
         self.metric_funcs[m] = resolve.metric(m)
-      elif hasattr(m, '__name__'):
+      elif hasattr(m, "__name__"):
         self.metric_funcs[m.__name__] = m
       else:
-        raise ValueError(f'Unknown metric {m}')
+        raise ValueError(f"Unknown metric {m}")
 
-    self.metrics = MetricStore(
-      self.metric_funcs.keys(),
-      cast_value=get_item
-    )
+    self.metrics = MetricStore(self.metric_funcs.keys(), cast_value=get_item)
     self.val_metrics = MetricStore(
-      self.metric_funcs.keys(),
-      cast_value=get_item
+      self.metric_funcs.keys(), cast_value=get_item
     )
     self.events = EventStore()
 
@@ -47,14 +44,20 @@ class History(Callback):
     self.metrics.update(
       step=trainer.num_steps,
       loss=loss,
-      **evaluate_metrics(targets=targets, outputs=outputs, metrics=self.metric_funcs)
+      **evaluate_metrics(
+        targets=targets, outputs=outputs, metrics=self.metric_funcs
+      ),
     )
 
-  def on_validation_end(self, loss=None, outputs=None, targets=None, trainer=None):
+  def on_validation_end(
+    self, loss=None, outputs=None, targets=None, trainer=None
+  ):
     self.val_metrics.update(
       step=trainer.num_epochs,
       loss=loss,
-      **evaluate_metrics(targets=targets, outputs=outputs, metrics=self.metric_funcs)
+      **evaluate_metrics(
+        targets=targets, outputs=outputs, metrics=self.metric_funcs
+      ),
     )
 
   @lazy
@@ -62,10 +65,16 @@ class History(Callback):
     return HistoryPlotter(history=self)
 
 
-
 class HistoryPlotter(Callback):
-  def __init__(self, freq=500, window=50, metrics=None,
-               clear=False, save=False, history: History = None):
+  def __init__(
+    self,
+    freq=500,
+    window=50,
+    metrics=None,
+    clear=False,
+    save=False,
+    history: History = None,
+  ):
     super().__init__()
     self.history: History = history
     self.freq = freq
@@ -85,11 +94,11 @@ class HistoryPlotter(Callback):
     if self.clear:
       try:
         from IPython.display import clear_output
+
         clear_output(wait=True)
       except:
         pass
 
-
     if validation:
       metrics = self.history.val_metrics
     else:
@@ -104,15 +113,15 @@ class HistoryPlotter(Callback):
       plot_line(
         metrics[name],
         x=metrics.times if time else range(len(metrics)),
-        xlabel='time' if time else 'step',
-        ylabel=f'validation {name}' if validation else name,
-        name=f'validation {name}' if validation else name,
+        xlabel="time" if time else "step",
+        ylabel=f"validation {name}" if validation else name,
+        name=f"validation {name}" if validation else name,
         window=1 if validation else self.window,
-        save=self.save and
-             self.root / (f'validation {name}' if validation else name),
+        save=self.save
+        and self.root / (f"validation {name}" if validation else name),
         show=not self.save,
         figsize=self.figsize,
-        **kwargs
+        **kwargs,
       )
 
   def on_train_start(self, trainer=None):
@@ -122,20 +131,25 @@ class HistoryPlotter(Callback):
 
   def on_step_end(self, *args, trainer=None, **kwargs):
     if trainer.num_steps % self.freq == 0:
-      self.plot(
-        title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}'
-      )
+      self.plot(title=f"Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}")
 
   def on_validation_end(self, *args, trainer=None, **kwargs):
     self.plot(
       validation=True,
-      title=f'Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}'
+      title=f"Epoch: {trainer.num_epochs} Steps: {trainer.num_steps}",
     )
 
 
 class HistoryWriter(Callback):
-  def __init__(self, root=None, train=True, val=True, mode='a+',
-               write_freq=1, flush_freq=500):
+  def __init__(
+    self,
+    root=None,
+    train=True,
+    val=True,
+    mode="a+",
+    write_freq=1,
+    flush_freq=500,
+  ):
     self.root = root
     self.mode = mode
     self.train = train
@@ -170,8 +184,8 @@ class HistoryWriter(Callback):
     if self.val_file:
       self.val_file.close()
 
-    self.train_file = open(self.root / 'history-train.tsv', self.mode)
-    self.val_file = open(self.root / 'history-val.tsv', self.mode)
+    self.train_file = open(self.root / "history-train.tsv", self.mode)
+    self.val_file = open(self.root / "history-val.tsv", self.mode)
 
   def on_train_start(self, trainer=None):
     self.prep_files(trainer.root if trainer else None)
@@ -181,35 +195,50 @@ class HistoryWriter(Callback):
       return
 
     if self.header is None:
-      self.header = ['timestamp', 'epoch', 'step', *trainer.history.metrics.keys()]
-      self.train_file.write('\t'.join(self.header) + '\n')
+      self.header = [
+        "timestamp",
+        "epoch",
+        "step",
+        *trainer.history.metrics.keys(),
+      ]
+      self.train_file.write("\t".join(self.header) + "\n")
 
     self.train_file.write(
       f"{trainer.history.metrics.times[-1]}\t"
       f"{trainer.num_epochs}\t"
       f"{len(trainer.history.metrics.times)}\t"
-      + '\t'.join((f"{trainer.history.metrics[m][-1]:.4f}"
-                   for m in self.header[3:]))
-      + '\n'
+      + "\t".join(
+        (f"{trainer.history.metrics[m][-1]:.4f}" for m in self.header[3:])
+      )
+      + "\n"
     )
 
     if index % self.flush_freq == 0:
       self.train_file.flush()
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     if self.val_header is None:
-      self.val_header = ['timestamp', 'epoch', 'step',
-                         *trainer.history.val_metrics.keys()]
-      self.val_file.write('\t'.join(self.header) + '\n')
+      self.val_header = [
+        "timestamp",
+        "epoch",
+        "step",
+        *trainer.history.val_metrics.keys(),
+      ]
+      self.val_file.write("\t".join(self.header) + "\n")
 
     self.val_file.write(
       f"{trainer.history.val_metrics.times[-1]}\t"
       f"{trainer.num_epochs}\t"
       f"{len(trainer.history.val_metrics.times)}\t"
-      + '\t'.join((f"{trainer.history.val_metrics[m][-1]:.4f}"
-                   for m in self.val_header[3:]))
-      + '\n'
+      + "\t".join(
+        (
+          f"{trainer.history.val_metrics[m][-1]:.4f}"
+          for m in self.val_header[3:]
+        )
+      )
+      + "\n"
     )
 
     self.val_file.flush()
diff --git a/yann/callbacks/logging.py b/yann/callbacks/logging.py
index 67ad419..697e061 100644
--- a/yann/callbacks/logging.py
+++ b/yann/callbacks/logging.py
@@ -1,9 +1,11 @@
-import sys
 import logging
+import sys
 
-from .base import Callback
 from yann.utils.tensor import describe
 
+from .base import Callback
+
+
 class Logger(Callback):
   dist_placement = 0
 
@@ -16,12 +18,11 @@ class Logger(Callback):
     self.batch_string = None
     self.logged_batch_shapes = False
 
-  def log(self, *args, sep='\t', **kwargs):
+  def log(self, *args, sep="\t", **kwargs):
     if kwargs:
       print(
-        *args,
-        sep.join(f'{k}: {v}' for k, v in kwargs.items()),
-        file=self.dest)
+        *args, sep.join(f"{k}: {v}" for k, v in kwargs.items()), file=self.dest
+      )
     else:
       print(*args, file=self.dest)
 
@@ -29,32 +30,42 @@ class Logger(Callback):
     self.log(*args, **kwargs)
 
   def on_train_start(self, trainer=None):
-    self.log('Starting training\n', trainer)
+    self.log("Starting training\n", trainer)
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
     if index % self.batch_freq == 0:
       if not self.logged_batch_shapes:
         try:
-          self.log("\ninputs:", describe(inputs),
-                   "\ntargets:", describe(targets),
-                   "\noutputs:", describe(outputs), '\n')
+          self.log(
+            "\ninputs:",
+            describe(inputs),
+            "\ntargets:",
+            describe(targets),
+            "\noutputs:",
+            describe(outputs),
+            "\n",
+          )
         except Exception as e:
           raise e
         self.logged_batch_shapes = True
 
       if self.batch_string:
-        self.log(self.batch_string.format(
-          batch=index,
-          **({m: v[-1] for m, v in trainer.history.metrics.items()})))
+        self.log(
+          self.batch_string.format(
+            batch=index,
+            **({m: v[-1] for m, v in trainer.history.metrics.items()}),
+          )
+        )
       else:
         self.log(
-          batch=f'{index:>8}',
-          **({m: f'{v[-1]:.4f}' for m, v in trainer.history.metrics.items()}))
+          batch=f"{index:>8}",
+          **({m: f"{v[-1]:.4f}" for m, v in trainer.history.metrics.items()}),
+        )
 
   def on_epoch_start(self, epoch, trainer=None):
-    self.log('\nStarting epoch', epoch)
+    self.log("\nStarting epoch", epoch)
 
-    self.log(f'''
+    self.log(f"""
 OPTIMIZER
 =========
 
@@ -65,24 +76,28 @@ PROGRESS
 ========
 epochs: {trainer.num_epochs}
 steps: {trainer.num_steps}
-samples: {trainer.num_samples}\n''')
+samples: {trainer.num_samples}\n""")
 
   def on_validation_start(self, trainer=None):
-    self.log('\nStarting Validation')
+    self.log("\nStarting Validation")
 
-  def on_validation_end(self, loss=None, outputs=None, targets=None,
-    trainer=None):
-    self.log('\nCompleted Validation')
+  def on_validation_end(
+    self, loss=None, outputs=None, targets=None, trainer=None
+  ):
+    self.log("\nCompleted Validation")
     self.log(
       epoch=trainer.num_epochs,
       steps=len(trainer.history.val_metrics),
-      **{m: f'{vals[-1]:.4f}' for m, vals in trainer.history.val_metrics.items()}
+      **{
+        m: f"{vals[-1]:.4f}" for m, vals in trainer.history.val_metrics.items()
+      },
     )
 
   def on_epoch_end(self, epoch, loss=None, metrics=None, trainer=None):
-    self.log('Completed epoch', epoch,)
+    self.log(
+      "Completed epoch",
+      epoch,
+    )
 
   def on_train_end(self, trainer=None):
-    self.log('Completed training run. \n\n')
-
-
+    self.log("Completed training run. \n\n")
diff --git a/yann/callbacks/lr.py b/yann/callbacks/lr.py
index 5f71f1c..81b7ab7 100644
--- a/yann/callbacks/lr.py
+++ b/yann/callbacks/lr.py
@@ -1,10 +1,11 @@
 import logging
 import os
 from math import cos, pi
+
 import numpy as np
 
-from ..callbacks.base import Callback
 from .. import set_param
+from ..callbacks.base import Callback
 from ..metrics import exp_moving_avg
 from ..viz import plot_line
 
@@ -15,15 +16,15 @@ def cosine_anneal(min_lr, max_lr, cur_step, num_steps):
 
 class SGDR(Callback):
   def __init__(
-      self,
-      optimizer=None,
-      max_lr=None,
-      min_lr=0,
-      cycle_len=10,
-      cycle_mult=1,
-      lr_mult=1,
-      verbose=True,
-      checkpoint=False
+    self,
+    optimizer=None,
+    max_lr=None,
+    min_lr=0,
+    cycle_len=10,
+    cycle_mult=1,
+    lr_mult=1,
+    verbose=True,
+    checkpoint=False,
   ):
     self.optimizer = optimizer
     self.max_lr = max_lr
@@ -52,10 +53,13 @@ class SGDR(Callback):
 
   def restart(self):
     if self.save_checkpoints:
-      self.checkpoints.append(self.trainer.checkpoint(
-        f'cycle-{self.completed_cycles}'
-        f'-epochs-{self.trainer.num_epochs}'
-        f'-steps-{self.trainer.num_steps}'))
+      self.checkpoints.append(
+        self.trainer.checkpoint(
+          f"cycle-{self.completed_cycles}"
+          f"-epochs-{self.trainer.num_epochs}"
+          f"-steps-{self.trainer.num_steps}"
+        )
+      )
 
     self.cur_cycle_len *= self.cycle_mult
     self.cur_max_lr *= self.lr_mult
@@ -66,32 +70,33 @@ class SGDR(Callback):
     self.update_lr(self.cur_lr)
 
     if self.verbose:
-      print('Restarting SGDR')
+      print("Restarting SGDR")
 
   def update_lr(self, lr):
-    set_param(self.optimizer, 'lr', lr)
+    set_param(self.optimizer, "lr", lr)
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
     if self.cur_step >= self.cur_cycle_len:
       self.restart()
     else:
-      new_lr = cosine_anneal(self.cur_min_lr, self.cur_max_lr, self.cur_step,
-                             self.cur_cycle_len)
+      new_lr = cosine_anneal(
+        self.cur_min_lr, self.cur_max_lr, self.cur_step, self.cur_cycle_len
+      )
       self.update_lr(new_lr)
       self.cur_step += 1
 
 
 class LRRangeTest(Callback):
   def __init__(
-      self,
-      start_lr=.00001,
-      end_lr=1,
-      steps=500,
-      step=None,
-      log_freq=10,
-      plot_freq=100,
-      divergence_multiplier=4,
-      plot_path=None,
+    self,
+    start_lr=0.00001,
+    end_lr=1,
+    steps=500,
+    step=None,
+    log_freq=10,
+    plot_freq=100,
+    divergence_multiplier=4,
+    plot_path=None,
   ):
     super(LRRangeTest, self).__init__()
     self.checkpoint_path = None
@@ -99,7 +104,7 @@ class LRRangeTest(Callback):
     self.end_lr = end_lr
 
     if not step and not steps:
-      raise ValueError('step or steps must be provided')
+      raise ValueError("step or steps must be provided")
 
     delta = end_lr - start_lr
     self.steps = steps or (delta / step)
@@ -115,7 +120,6 @@ class LRRangeTest(Callback):
     self.plot_freq = plot_freq
     self.log_freq = log_freq
 
-
     self.divergence_multiplier = divergence_multiplier
 
   def __repr__(self):
@@ -130,15 +134,16 @@ class LRRangeTest(Callback):
 
   def on_train_start(self, trainer=None):
     self.lrs = [self.start_lr]
-    set_param(trainer.optimizer, 'lr', self.lrs[-1])
+    set_param(trainer.optimizer, "lr", self.lrs[-1])
 
     if self.log_freq:
       print(self)
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
     self.losses.append(loss.item())
-    self.avg_loss = exp_moving_avg(self.losses[-1], self.avg_loss,
-                                   steps=len(self.losses))
+    self.avg_loss = exp_moving_avg(
+      self.losses[-1], self.avg_loss, steps=len(self.losses)
+    )
 
     if self.log_freq and len(self.lrs) % self.log_freq == 0:
       print(f"lr: {self.lrs[-1]:.5f}  loss: {self.avg_loss:.5f}")
@@ -148,9 +153,10 @@ class LRRangeTest(Callback):
 
     if self.min_loss is None:
       self.min_loss = self.avg_loss
-    elif (self.avg_loss > self.divergence_multiplier * self.min_loss) and \
-        len(self.lrs) > 50:
-      logging.info('Loss diverged, stopping LR Range Test')
+    elif (self.avg_loss > self.divergence_multiplier * self.min_loss) and len(
+      self.lrs
+    ) > 50:
+      logging.info("Loss diverged, stopping LR Range Test")
       trainer.stop()
       return
     elif self.avg_loss < self.min_loss:
@@ -161,18 +167,17 @@ class LRRangeTest(Callback):
       return
 
     self.lrs.append(self.lrs[-1] + self.step)
-    set_param(trainer.optimizer, 'lr', self.lrs[-1])
-
+    set_param(trainer.optimizer, "lr", self.lrs[-1])
 
   def plot(self, **kwargs):
     plot_line(
       x=self.lrs,
       y=self.losses,
-      xlabel='learning rate',
-      ylabel='loss',
+      xlabel="learning rate",
+      ylabel="loss",
       save=self.plot_path,
       show=not self.plot_path,
-      **kwargs
+      **kwargs,
     )
 
 
@@ -189,7 +194,7 @@ class CyclicalLR(Callback):
     self.cur_lr = self.start_lr
 
   def on_train_start(self, trainer=None):
-    set_param(trainer.optimizer, 'lr', self.cur_lr)
+    set_param(trainer.optimizer, "lr", self.cur_lr)
 
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
     if self.cur_step % self.cycle_len // self.steps:
@@ -198,7 +203,7 @@ class CyclicalLR(Callback):
       self.cur_lr += self.step
 
     if self.cur_step % 100 == 0:
-      print(f'lr: {self.cur_lr:.5f}')
+      print(f"lr: {self.cur_lr:.5f}")
 
-    set_param(trainer.optimizer, 'lr', self.cur_lr)
+    set_param(trainer.optimizer, "lr", self.cur_lr)
     self.cur_step += 1
diff --git a/yann/callbacks/profile.py b/yann/callbacks/profile.py
index c35c6ea..b816320 100644
--- a/yann/callbacks/profile.py
+++ b/yann/callbacks/profile.py
@@ -2,12 +2,7 @@ from . import Callback
 
 
 class Profile(Callback):
-  def __init__(
-      self,
-      start_step=16,
-      stop_step=80,
-      **kwargs
-  ):
+  def __init__(self, start_step=16, stop_step=80, **kwargs):
     self.profiler = None
 
     self.start_step = start_step
@@ -16,10 +11,10 @@ class Profile(Callback):
 
     self.profiler_args = kwargs
 
-
   def on_step_start(self, index=None, **kwargs):
     if index == self.start_step:
       from torch.profiler import profile
+
       self.profiler = profile(**kwargs)
       self.profiler.start()
 
@@ -31,27 +26,24 @@ class Profile(Callback):
       self.save(root=trainer.paths.profile)
       self.disable()
 
-
-  def save(self, root: 'pathlib.Path' = None):
-     self.profiler.export_chrome_trace(str(root / 'chrome_trace.json'))
-
-     try:
-      self.profiler.tensorboard_trace_handler(str(root / 'tensorboard'))
-     except:
-       pass
-
-     try:
-       self.profiler.export_stacks(
-         str(root / 'cpu.stacks'),
-         metric='self_cpu_time_total'
-       )
-     except:
-       pass
-
-     try:
-       self.profiler.export_stacks(
-         str(root / 'cuda.stacks'),
-         metric='self_cuda_time_total'
-       )
-     except:
-       pass
+  def save(self, root: "pathlib.Path" = None):
+    self.profiler.export_chrome_trace(str(root / "chrome_trace.json"))
+
+    try:
+      self.profiler.tensorboard_trace_handler(str(root / "tensorboard"))
+    except:
+      pass
+
+    try:
+      self.profiler.export_stacks(
+        str(root / "cpu.stacks"), metric="self_cpu_time_total"
+      )
+    except:
+      pass
+
+    try:
+      self.profiler.export_stacks(
+        str(root / "cuda.stacks"), metric="self_cuda_time_total"
+      )
+    except:
+      pass
diff --git a/yann/callbacks/progbar.py b/yann/callbacks/progbar.py
index a14ab24..96d585d 100644
--- a/yann/callbacks/progbar.py
+++ b/yann/callbacks/progbar.py
@@ -1,4 +1,5 @@
 import yann.utils
+
 from .base import Callback
 
 
@@ -10,7 +11,9 @@ class ProgressBar(Callback):
       samples: count samples, if False will track batches/steps
       notebook: use notebook progress bar
     """
-    self.notebook = notebook if notebook is not None else yann.utils.is_notebook()
+    self.notebook = (
+      notebook if notebook is not None else yann.utils.is_notebook()
+    )
 
     self.length = length
     self.samples = samples
@@ -24,10 +27,9 @@ class ProgressBar(Callback):
     else:
       from tqdm import tqdm
 
-
     if trainer:
       try:
-        total = (len(trainer.dataset) if self.samples else len(trainer.loader))
+        total = len(trainer.dataset) if self.samples else len(trainer.loader)
       except:
         total = None
     else:
@@ -36,12 +38,10 @@ class ProgressBar(Callback):
     self.bar = tqdm(
       desc=f"Epoch {epoch}",
       total=self.length or total,
-      unit='samples' if self.samples else 'batches'
+      unit="samples" if self.samples else "batches",
     )
 
-  def on_epoch_end(
-    self, epoch=None, loss=None, metrics=None, trainer=None
-  ):
+  def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
     self.bar.close()
 
   def on_step_end(
@@ -51,7 +51,7 @@ class ProgressBar(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
     if self.samples:
       self.bar.update(len(inputs))
diff --git a/yann/callbacks/stop.py b/yann/callbacks/stop.py
index 824ca55..b206719 100644
--- a/yann/callbacks/stop.py
+++ b/yann/callbacks/stop.py
@@ -1,7 +1,7 @@
-from .base import Callback
-
 import torch
 
+from .base import Callback
+
 
 class EarlyStopping(Callback):
   def __init__(self):
@@ -10,7 +10,6 @@ class EarlyStopping(Callback):
 
 class StopOnNaN(Callback):
   def on_step_end(self, index, inputs, targets, outputs, loss, trainer=None):
-    if torch.isnan(loss).any() \
-       or torch.isinf(loss).any():
-      print('NaN or Inf detected, stopping training')
-      trainer.stop()
\ No newline at end of file
+    if torch.isnan(loss).any() or torch.isinf(loss).any():
+      print("NaN or Inf detected, stopping training")
+      trainer.stop()
diff --git a/yann/callbacks/tensorboard.py b/yann/callbacks/tensorboard.py
index e485537..6d50127 100644
--- a/yann/callbacks/tensorboard.py
+++ b/yann/callbacks/tensorboard.py
@@ -1,15 +1,17 @@
-from pathlib import Path
 import logging
-from torch.utils.tensorboard import SummaryWriter
+from pathlib import Path
+
 from torch import nn
+from torch.utils.tensorboard import SummaryWriter
 
 from .base import Callback
 
-
 log = logging.getLogger(__name__)
 
+
 class Tensorboard(Callback):
   writer: SummaryWriter
+
   def __init__(self, root=None, trainer=None, writer=None):
     self.root = root
     self.trainer = trainer
@@ -26,21 +28,20 @@ class Tensorboard(Callback):
 
     if isinstance(val, nn.Module):
       self.writer.add_graph(
-        val,
-        kwargs.get('input'),
-        verbose=kwargs.get('verbose', False)
+        val, kwargs.get("input"), verbose=kwargs.get("verbose", False)
       )
 
   def show(self, root=None):
     from IPython import get_ipython
+
     ipython = get_ipython()
-    ipython.magic('load_ext tensorboard')
+    ipython.magic("load_ext tensorboard")
 
     if isinstance(self, (str, Path)):
       # static method (Tensorboard.show())
-      ipython.magic(f'tensorboard --logdir {self}')
+      ipython.magic(f"tensorboard --logdir {self}")
     else:
-      ipython.magic(f'tensorboard --logdir {root or self.root or "./"}')
+      ipython.magic(f"tensorboard --logdir {root or self.root or './'}")
 
   def close(self):
     if self.writer:
@@ -61,14 +62,15 @@ class Tensorboard(Callback):
     try:
       self.writer.add_hparams(dict(self.trainer.params), {})
     except Exception as e:
-      print(f'failed to write tensorboard hparams: {e}')
+      print(f"failed to write tensorboard hparams: {e}")
 
   def on_train_end(self, trainer=None):
-    if self.trainer and self.trainer.params and self.trainer.history.val_metrics:
+    if (
+      self.trainer and self.trainer.params and self.trainer.history.val_metrics
+    ):
       try:
         self.writer.add_hparams(
-          dict(self.trainer.params),
-          self.trainer.history.val_metrics.summary()
+          dict(self.trainer.params), self.trainer.history.val_metrics.summary()
         )
       except ValueError as e:
         log.error(e)
@@ -85,17 +87,23 @@ class Tensorboard(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
-    self.writer.add_scalar('train/loss', loss, global_step=trainer.num_steps)
+    self.writer.add_scalar("train/loss", loss, global_step=trainer.num_steps)
     for metric, values in trainer.history.metrics.items():
-      self.writer.add_scalar(f'train/{metric}', values[-1], global_step=len(values) - 1)
+      self.writer.add_scalar(
+        f"train/{metric}", values[-1], global_step=len(values) - 1
+      )
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     for metric, values in trainer.history.val_metrics.items():
-      self.writer.add_scalar(f'validation/{metric}', values[-1], global_step=len(values) - 1)
+      self.writer.add_scalar(
+        f"validation/{metric}", values[-1], global_step=len(values) - 1
+      )
 
   def sanitize_model(self, model):
     if isinstance(model, nn.DataParallel):
       return model.module
-    return model
\ No newline at end of file
+    return model
diff --git a/yann/callbacks/timing.py b/yann/callbacks/timing.py
index 25562de..8905ead 100644
--- a/yann/callbacks/timing.py
+++ b/yann/callbacks/timing.py
@@ -1,5 +1,5 @@
 import time
-from matplotlib import pylab as plt
+
 
 from .base import Callback
 
@@ -29,20 +29,25 @@ class Timing(Callback):
     return [s - e for (s, e) in zip(self.starts, [self.start_time, *self.ends])]
 
   def plot(self, start=0, end=None, scatter=False):
+    from matplotlib import pylab as plt
     end = end or len(self.starts)
 
     fig = plt.figure(figsize=(12, 4))
     plt.grid()
     if scatter:
-      plt.scatter(range(start, end), list(self.times)[start:end], label='step')
-      plt.scatter(range(start, end), list(self.waits)[start:end], label='prep')
+      plt.scatter(range(start, end), list(self.times)[start:end], label="step")
+      plt.scatter(range(start, end), list(self.waits)[start:end], label="prep")
     else:
-      plt.bar(range(start, end), list(self.waits)[start:end], label='prep')
-      plt.bar(range(start, end), list(self.times)[start:end],
-              bottom=list(self.waits)[start:end], label='step')
-
-    plt.xlabel('step')
-    plt.ylabel('seconds')
-    plt.title('Train Run Timings')
+      plt.bar(range(start, end), list(self.waits)[start:end], label="prep")
+      plt.bar(
+        range(start, end),
+        list(self.times)[start:end],
+        bottom=list(self.waits)[start:end],
+        label="step",
+      )
+
+    plt.xlabel("step")
+    plt.ylabel("seconds")
+    plt.title("Train Run Timings")
     plt.legend()
-    plt.show()
\ No newline at end of file
+    plt.show()
diff --git a/yann/callbacks/unfreeze.py b/yann/callbacks/unfreeze.py
index 5310dd2..7c008d1 100644
--- a/yann/callbacks/unfreeze.py
+++ b/yann/callbacks/unfreeze.py
@@ -1,14 +1,19 @@
 from typing import Dict
 
 import torch
-from yann.train import Trainer
+
 import yann
+from yann.train import Trainer
+
 from . import Callback
 
 
 class GradualUnfreezing(Callback):
   trainer: Trainer
-  def __init__(self, modules: Dict[int, torch.nn.Module], unfreeze=yann.unfreeze):
+
+  def __init__(
+    self, modules: Dict[int, torch.nn.Module], unfreeze=yann.unfreeze
+  ):
     self.modules = modules
     self.unfreeze = unfreeze
 
@@ -25,10 +30,10 @@ class GradualUnfreezing(Callback):
 
     # clone param group variables to avoid missing keys (used by things like lr_schedulers)
     param_group = {
-      k: v for (k, v)
-      in self.trainer.optimizer.param_groups[0].items()
-      if k != 'params'
+      k: v
+      for (k, v) in self.trainer.optimizer.param_groups[0].items()
+      if k != "params"
     }
-    param_group['params'] = parameters
+    param_group["params"] = parameters
 
-    self.trainer.optimizer.add_param_group(param_group)
\ No newline at end of file
+    self.trainer.optimizer.add_param_group(param_group)
diff --git a/yann/callbacks/wandb.py b/yann/callbacks/wandb.py
index a594f7b..eb93003 100644
--- a/yann/callbacks/wandb.py
+++ b/yann/callbacks/wandb.py
@@ -11,17 +11,17 @@ from yann.callbacks import Callback
 class Wandb(Callback):
   dist_placement = 0
 
-  run: Optional['wandb.wandb_sdk.wandb_run.Run']
+  run: Optional["wandb.wandb_sdk.wandb_run.Run"]
 
   def __init__(
-      self,
-      project=None,
-      entity=None,
-      name=None,
-      watch_freq=0,
-      log_code=True,
-      batch_log_freq=10,
-      trackers=None,
+    self,
+    project=None,
+    entity=None,
+    name=None,
+    watch_freq=0,
+    log_code=True,
+    batch_log_freq=10,
+    trackers=None,
   ):
     self.client = wandb
     self.run = None
@@ -39,18 +39,18 @@ class Wandb(Callback):
   def log(self, *args, **kwargs):
     self.run.log(*args, **kwargs, step=self.trainer.num_steps)
 
-  def on_train_start(self, trainer: 'yann.train.Trainer' = None):
+  def on_train_start(self, trainer: "yann.train.Trainer" = None):
     self.trainer = trainer
     if self.run is None:
       self.run = self.client.init(
         project=self.project,
         entity=self.entity,
         name=self.name or trainer.name,
-        config=dict(trainer.params) if trainer.params else {}
+        config=dict(trainer.params) if trainer.params else {},
       )
 
       if self.log_code is True:
-        self.run.log_code('.')
+        self.run.log_code(".")
       elif isinstance(self.log_code, dict):
         self.run.log_code(**self.log_code)
       elif isinstance(self.log_code, str):
@@ -59,9 +59,9 @@ class Wandb(Callback):
     if trainer.model and self.watch_freq:
       self.run.watch(
         models=trainer.model,
-        log='all',
+        log="all",
         log_graph=True,
-        log_freq=self.watch_freq
+        log_freq=self.watch_freq,
       )
 
     if self.trackers is None:
@@ -79,31 +79,30 @@ class Wandb(Callback):
     targets=None,
     outputs=None,
     loss=None,
-    trainer=None
+    trainer=None,
   ):
     if trainer.num_steps % self.batch_log_freq == 0:
-      self.run.log({'train/loss': loss}, step=trainer.num_steps)
+      self.run.log({"train/loss": loss}, step=trainer.num_steps)
       for metric, values in trainer.history.metrics.items():
-        self.run.log({f'train/{metric}': values[-1]}, step=len(values) - 1)
+        self.run.log({f"train/{metric}": values[-1]}, step=len(values) - 1)
 
       if self.trackers:
         for track in self.trackers:
           self.run.log(track(trainer), step=trainer.num_steps)
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None, trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     for metric, values in trainer.history.val_metrics.items():
-      self.run.log({f'validation/{metric}': values[-1]}, step=trainer.num_steps)
+      self.run.log({f"validation/{metric}": values[-1]}, step=trainer.num_steps)
 
   def on_epoch_end(self, epoch=None, loss=None, metrics=None, trainer=None):
     self.run.summary.update(trainer.summary)
-    self.run.log({'epoch': trainer.num_epochs}, step=trainer.num_steps)
-
+    self.run.log({"epoch": trainer.num_epochs}, step=trainer.num_steps)
 
   def get_default_trackers(self, trainer=None):
-    if not trainer: return None
+    if not trainer:
+      return None
     import yann.train.track
 
-    return [
-      yann.train.track.OptimizerState()
-    ]
-
+    return [yann.train.track.OptimizerState()]
diff --git a/yann/cli.py b/yann/cli.py
index 89308ee..513ea9c 100644
--- a/yann/cli.py
+++ b/yann/cli.py
@@ -9,29 +9,29 @@ def cli():
 
 
 @cli.command()
-@click.option('-n', '--name', default=None)
-@click.option('-d', '--dataset', default=None)
-@click.option('-opt', '--optimizer', default='SGD', show_default=True)
-@click.option('-m', '--model', default='resnet18', show_default=True)
-@click.option('-e', '--epochs', default=10, show_default=True)
-@click.option('-l', '--loss')
-@click.option('-cp', '--checkpoint')
-@click.option('-c', '--continue')
+@click.option("-n", "--name", default=None)
+@click.option("-d", "--dataset", default=None)
+@click.option("-opt", "--optimizer", default="SGD", show_default=True)
+@click.option("-m", "--model", default="resnet18", show_default=True)
+@click.option("-e", "--epochs", default=10, show_default=True)
+@click.option("-l", "--loss")
+@click.option("-cp", "--checkpoint")
+@click.option("-c", "--continue")
 def train(
-    name,
-    model,
-    dataset,
-    loss=None,
-    transform=None,
-    optimizer='SGD',
-    checkpoint=None,
-    lr=0.01,
-    momentum=.9,
-    epochs=10
+  name,
+  model,
+  dataset,
+  loss=None,
+  transform=None,
+  optimizer="SGD",
+  checkpoint=None,
+  lr=0.01,
+  momentum=0.9,
+  epochs=10,
 ):
   """Train model"""
-  from .train import Trainer
   from .callbacks import get_callbacks
+  from .train import Trainer
 
   t = Trainer(
     name=name,
@@ -40,7 +40,7 @@ def train(
     dataset=dataset,
     transform=transform,
     loss=loss,
-    callbacks=get_callbacks(interactive=False)
+    callbacks=get_callbacks(interactive=False),
   )
 
   if checkpoint:
@@ -66,7 +66,7 @@ def validate():
 
 
 @cli.command()
-@click.argument('names', nargs=-1)
+@click.argument("names", nargs=-1)
 def resolve(names):
   import yann
 
@@ -80,7 +80,7 @@ def resolve(names):
 
 
 @cli.command()
-@click.argument('names', nargs=-1)
+@click.argument("names", nargs=-1)
 def registry(names):
   """List contents of registry"""
   import yann
@@ -137,17 +137,17 @@ def scaffold():
   raise NotImplementedError()
 
 
-
-
 @cli.command()
-@click.argument('src')
-@click.argument('dst')
+@click.argument("src")
+@click.argument("dst")
 def convert(src: str, dst: str):
   import yann
+
   data = yann.load(src)
-  print(f'loaded {type(data)}')
+  print(f"loaded {type(data)}")
   yann.save(data, dst)
 
+
 def main():
   cli()
 
@@ -158,21 +158,23 @@ def dataset():
 
 
 @dataset.command()
-@click.argument('name')
+@click.argument("name")
 def preview(name: str):
   count = 10
 
   ds = yann.resolve.dataset(name)
 
   print(yann.utils.fully_qualified_name(ds))
-  print('length:', len(ds))
+  print("length:", len(ds))
   for i in range(count):
     x = ds[i]
     print(x)
 
+
 @dataset.command()
 def list():
   print(yann.registry.dataset.print_tree())
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
   cli()
diff --git a/yann/config/defaults.py b/yann/config/defaults.py
index 451103d..361b12a 100644
--- a/yann/config/defaults.py
+++ b/yann/config/defaults.py
@@ -1,20 +1,21 @@
-import torch
 from pathlib import Path
 
+import torch
+
 
 class default:
-  root = Path('~/.yann/').expanduser()
-  torch_root = Path('~/.torch').expanduser()
-  train_root = './runs/'
-  datasets_root = torch_root / 'datasets'
+  root = Path("~/.yann/").expanduser()
+  torch_root = Path("~/.torch").expanduser()
+  train_root = "./runs/"
+  datasets_root = torch_root / "datasets"
 
   device = None
   if torch.cuda.is_available():
-    device = torch.device('cuda')
-  elif hasattr(torch, 'backends') and torch.backends.mps.is_available():
-    device = torch.device('mps')
+    device = torch.device("cuda")
+  elif hasattr(torch, "backends") and torch.backends.mps.is_available():
+    device = torch.device("mps")
   else:
-    device = torch.device('cpu')
+    device = torch.device("cpu")
 
   batch_size = 32
   num_workers = None
@@ -22,12 +23,12 @@ class default:
 
   callbacks = None
 
-  checkpoint_name_format = ''
+  checkpoint_name_format = ""
 
   ddp_find_unused_parameters = True
 
   @classmethod
   def dataset_root(cls, dataset):
-    if hasattr(dataset, 'root'):
+    if hasattr(dataset, "root"):
       return dataset.root
-    return str(cls.datasets_root / dataset.__name__)
\ No newline at end of file
+    return str(cls.datasets_root / dataset.__name__)
diff --git a/yann/config/registry.py b/yann/config/registry.py
index ed5d778..1f0275b 100644
--- a/yann/config/registry.py
+++ b/yann/config/registry.py
@@ -1,9 +1,8 @@
 import typing
-from typing import Union, Tuple, Dict, Any
-from collections import defaultdict, OrderedDict
-
+from collections import OrderedDict, defaultdict
 from functools import partial
 from itertools import chain
+from typing import Any, Dict, Tuple, Union
 
 
 def dedupe(items):
@@ -19,14 +18,12 @@ def pass_args(x, *args, **kwargs):
 
 
 def is_public(x):
-  return (hasattr(x, '__name__') and not x.__name__.startswith('_'))
+  return hasattr(x, "__name__") and not x.__name__.startswith("_")
 
 
 def is_public_callable(x):
   return (
-      hasattr(x, '__name__')
-      and not x.__name__.startswith('_')
-      and callable(x)
+    hasattr(x, "__name__") and not x.__name__.startswith("_") and callable(x)
   )
 
 
@@ -35,7 +32,7 @@ class default:
 
   @staticmethod
   def get_names(x):
-    return x.__name__,
+    return (x.__name__,)
 
 
 class RegistryError(Exception):
@@ -47,7 +44,7 @@ class ResolutionError(RegistryError):
 
 
 class Record:
-  __slots__ = ('x', 'init')
+  __slots__ = ("x", "init")
 
   def __init__(self, x, init=None):
     """
@@ -66,21 +63,21 @@ class Record:
 
 
 class Resolver:
-  __slots__ = ('registry',)
+  __slots__ = ("registry",)
 
-  def __init__(self, registry: 'Registry'):
+  def __init__(self, registry: "Registry"):
     self.registry = registry
 
   def resolve(
-      self,
-      x: Union[Any, Tuple[Any, Dict]],
-      required=False,
-      validate=None,
-      instance=True,
-      types=None,
-      init=None,
-      args=None,
-      kwargs=None
+    self,
+    x: Union[Any, Tuple[Any, Dict]],
+    required=False,
+    validate=None,
+    instance=True,
+    types=None,
+    init=None,
+    args=None,
+    kwargs=None,
   ):
     """
 
@@ -138,9 +135,18 @@ class Resolver:
     return x
 
   def __call__(
-      self, x, *_args, required=False, validate=None,
-      instance=True, types=None, args=None, kwargs=None, init=None,
-      **_kwargs, ):
+    self,
+    x,
+    *_args,
+    required=False,
+    validate=None,
+    instance=True,
+    types=None,
+    args=None,
+    kwargs=None,
+    init=None,
+    **_kwargs,
+  ):
     return self.resolve(
       x,
       required=required,
@@ -149,7 +155,7 @@ class Resolver:
       types=types,
       args=args or _args,
       kwargs=kwargs or _kwargs,
-      init=init
+      init=init,
     )
 
   def __getattr__(self, name):
@@ -184,11 +190,14 @@ class Registry:
       return partial(self.register, name=x)
 
     if self.types and not (
-        issubclass(x, self.types) if isinstance(x, type)
-        else isinstance(x, self.types)):
+      issubclass(x, self.types)
+      if isinstance(x, type)
+      else isinstance(x, self.types)
+    ):
       raise RegistryError(
         f"Can't register an object of type {type(x)} in "
-        f"typed registry which expects one of {self.types}")
+        f"typed registry which expects one of {self.types}"
+      )
 
     r = Record(x, init=init)
 
@@ -224,11 +233,11 @@ class Registry:
   def __call__(self, x, name=None, init=None):
     return self.register(x, name, init)
 
-  def __getattr__(self, name: str) -> 'Registry':
+  def __getattr__(self, name: str) -> "Registry":
     if name in self.__dict__:
       return self.__dict__[name]
     # allow defining new registries on attribute lookup
-    if name not in self._subregistries and not name.startswith('_'):
+    if name not in self._subregistries and not name.startswith("_"):
       self._subregistries[name] = Registry(name=name)
     return self._subregistries[name]
 
@@ -256,18 +265,21 @@ class Registry:
 
     raise KeyError(
       f"Couldn't find key: '{item}', "
-      f"valid options include: {', '.join(self._records.keys())}")
+      f"valid options include: {', '.join(self._records.keys())}"
+    )
 
   def values(self):
-    return dedupe((
-      *(r.x for r in self._records.values()),
-      *chain(*(c.values() for c in self.public_subregistries()))
-    ))
+    return dedupe(
+      (
+        *(r.x for r in self._records.values()),
+        *chain(*(c.values() for c in self.public_subregistries())),
+      )
+    )
 
   def items(self):
     return (
       *((k, r.x) for k, r in self._records.items()),
-      *chain(*(c.items() for c in self.public_subregistries()))
+      *chain(*(c.items() for c in self.public_subregistries())),
     )
 
   def keys(self):
@@ -277,14 +289,14 @@ class Registry:
     return len(self.values())
 
   def index(
-      self,
-      modules,
-      types=None,
-      get_names=None,
-      include=None,
-      exclude=None,
-      init=None,
-      include_private=False
+    self,
+    modules,
+    types=None,
+    get_names=None,
+    include=None,
+    exclude=None,
+    init=None,
+    include_private=False,
   ):
     """
     Indexes a module. If types are specified will only include entries of
@@ -306,8 +318,10 @@ class Registry:
           continue
         if types:
           if not (
-              isinstance(item, types) or
-              (isinstance(item, type)) and issubclass(item, types)):
+            isinstance(item, types)
+            or (isinstance(item, type))
+            and issubclass(item, types)
+          ):
             continue
 
         if include and not include(item):
@@ -335,16 +349,18 @@ class Registry:
   def print_tree(self, contents=True, indent=0):
     if not indent:
       print(
-        f'registry{" (Private - not resolvable from higher scopes)" if self.is_private else ""}')
+        f"registry{' (Private - not resolvable from higher scopes)' if self.is_private else ''}"
+      )
       indent += 2
     for name, registry in self._subregistries.items():
       print(
-        f"{' ' * indent}.{name} {' (Private - not resolvable from higher scopes)' if registry.is_private else ''}")
+        f"{' ' * indent}.{name} {' (Private - not resolvable from higher scopes)' if registry.is_private else ''}"
+      )
       registry.print_tree(indent=indent + 2, contents=contents)
 
     if contents:
       for name, record in self._records.items():
-        if isinstance(record.x, partial) or not hasattr(record.x, '__module__'):
+        if isinstance(record.x, partial) or not hasattr(record.x, "__module__"):
           details = str(record.x)
         else:
           details = f"{record.x.__module__}.{record.x.__name__ if hasattr(record.x, '__name__') else record.x}"
diff --git a/yann/config/setup.py b/yann/config/setup.py
index 7318801..a297b46 100644
--- a/yann/config/setup.py
+++ b/yann/config/setup.py
@@ -1,10 +1,7 @@
 import torch
 
 from .defaults import default
-from .registry import Registry, pass_args, is_public_callable
-
-
-
+from .registry import Registry, is_public_callable, pass_args
 
 ## Configure Registry
 
@@ -12,16 +9,16 @@ registry = Registry()
 
 # Datasets
 import torchvision.datasets
-
 from torch.utils.data import Dataset
-from ..datasets import imagenette, voc, coco, food101
+
+from ..datasets import coco, food101, imagenette, voc
 
 registry.dataset.index(
   [torchvision.datasets, imagenette, voc, coco, food101],
   types=(Dataset,),
-  init=lambda D, root=None, download=True, **kwargs: \
-    D(root=str(root or default.dataset_root(D)),
-      download=download, **kwargs)
+  init=lambda D, root=None, download=True, **kwargs: D(
+    root=str(root or default.dataset_root(D)), download=download, **kwargs
+  ),
 )
 
 
@@ -29,60 +26,56 @@ registry.dataset.index(
 registry.loss.index(
   torch.nn.modules.loss,
   types=(torch.nn.modules.loss._Loss,),
-  get_names=lambda x:
-  (x.__name__, x.__name__[:-len('Loss')]) if x.__name__.endswith('Loss') else (
-    x.__name__,)
+  get_names=lambda x: (x.__name__, x.__name__[: -len("Loss")])
+  if x.__name__.endswith("Loss")
+  else (x.__name__,),
 )
 
 import torch.nn.functional as F
 
 registry.loss.index(
   F,
-  include=lambda x: hasattr(x, '__name__') and 'loss' in x.__name__.lower(),
-  get_names=lambda x: (x.__name__, x.__name__[:-len('_loss')]) if
-  x.__name__.endswith('_loss') else (x.__name__,)
+  include=lambda x: hasattr(x, "__name__") and "loss" in x.__name__.lower(),
+  get_names=lambda x: (x.__name__, x.__name__[: -len("_loss")])
+  if x.__name__.endswith("_loss")
+  else (x.__name__,),
 )
 
 from ..modules import loss
 
-registry.loss.update((
-  F.cross_entropy,
-  F.binary_cross_entropy,
-  F.binary_cross_entropy_with_logits,
-  loss.soft_target_cross_entropy,
-  loss.SoftTargetCrossEntropyLoss
-))
+registry.loss.update(
+  (
+    F.cross_entropy,
+    F.binary_cross_entropy,
+    F.binary_cross_entropy_with_logits,
+    loss.soft_target_cross_entropy,
+    loss.SoftTargetCrossEntropyLoss,
+  )
+)
 
 # Optimizers
 from torch.optim import optimizer
 
-registry.optimizer.index(
-  torch.optim,
-  types=(optimizer.Optimizer,)
-)
+registry.optimizer.index(torch.optim, types=(optimizer.Optimizer,))
 
-registry.optimizer['SGD'].init = \
-  lambda SGD, params, lr=.01, momentum=0, weight_decay=0: \
-    SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)
+registry.optimizer["SGD"].init = (
+  lambda SGD, params, lr=0.01, momentum=0, weight_decay=0: SGD(
+    params, lr=lr, momentum=momentum, weight_decay=weight_decay
+  )
+)
 
 # LR Schedulers
 from torch.optim import lr_scheduler
 
-registry.lr_scheduler.index(
-  lr_scheduler,
-  types=(lr_scheduler._LRScheduler,)
-)
+registry.lr_scheduler.index(lr_scheduler, types=(lr_scheduler._LRScheduler,))
 # ReduceLROnPlateau subclasses object
 registry.lr_scheduler.register(lr_scheduler.ReduceLROnPlateau)
 
 # Models
 from torchvision import models
 
-
 registry.model.torchvision.index(
-  models,
-  init=pass_args,
-  include=is_public_callable
+  models, init=pass_args, include=is_public_callable
 )
 
 # NOTE: moved to yann.contrib.pretrainedmodels
@@ -99,24 +92,18 @@ registry.model.torchvision.index(
 #   )
 
 from .. import metrics
-registry.metric.update((
-  metrics.accuracy,
-  metrics.average_precision,
-  metrics.average_precision_at_k,
-  metrics.coverage_error,
-))
-
-registry.metric.register(
-  metrics.top_3_accuracy,
-  name='top_3_accuracy'
-)
 
-registry.metric.register(
-  metrics.top_5_accuracy,
-  name='top_5_accuracy'
+registry.metric.update(
+  (
+    metrics.accuracy,
+    metrics.average_precision,
+    metrics.average_precision_at_k,
+    metrics.coverage_error,
+  )
 )
 
-registry.metric.register(
-  metrics.top_10_accuracy,
-  name='top_10_accuracy'
-)
\ No newline at end of file
+registry.metric.register(metrics.top_3_accuracy, name="top_3_accuracy")
+
+registry.metric.register(metrics.top_5_accuracy, name="top_5_accuracy")
+
+registry.metric.register(metrics.top_10_accuracy, name="top_10_accuracy")
diff --git a/yann/contrib/gcp.py b/yann/contrib/gcp.py
index fac206f..1dfe39a 100644
--- a/yann/contrib/gcp.py
+++ b/yann/contrib/gcp.py
@@ -4,48 +4,53 @@ from ..callbacks.base import Callback
 from ..utils.bash import run
 
 
-
 def gcloud(command):
-  return run(['gcloud', command])
+  return run(["gcloud", command])
 
 
 def gsutil(command):
-  return run(['gsutil', command])
+  return run(["gsutil", command])
 
 
 def args(*flags, hyphenate=True, **kwargs):
-  return ' \ \n'.join((
-    *(f'--{str(n).replace("_", "-") if hyphenate else n}' for n in flags),
-    *(x for x in (
-      f'--{str(k).replace("_", "-") if hyphenate else k}={v}'
-      if not (v is True or v is False)
-      else (f'--{str(k).replace("_", "-") if hyphenate else k}' if v else '')
-      for k, v in kwargs.items() if v is not None)
-      if x)
-  ))
-
-
-def start_instance(
-    name,
-    zone=None,
-    preemptible=True):
-  command = (
-    f"""gcloud compute instances create {name} \
+  return " \ \n".join(
+    (
+      *(f"--{str(n).replace('_', '-') if hyphenate else n}" for n in flags),
+      *(
+        x
+        for x in (
+          f"--{str(k).replace('_', '-') if hyphenate else k}={v}"
+          if not (v is True or v is False)
+          else (
+            f"--{str(k).replace('_', '-') if hyphenate else k}" if v else ""
+          )
+          for k, v in kwargs.items()
+          if v is not None
+        )
+        if x
+      ),
+    )
+  )
+
+
+def start_instance(name, zone=None, preemptible=True):
+  command = f"""gcloud compute instances create {name} \
           {
     args(
       zone=zone,
       preemptible=preemptible,
-      maintenance_policy='foo',
+      maintenance_policy="foo",
     )
-    }
+  }
 
     """
-  )
 
   return run(command)
 
 
-def start_dl_instance(name, ):
+def start_dl_instance(
+  name,
+):
   pass
 
 
@@ -62,29 +67,16 @@ def kill_instance():
 
 
 def shutdown():
-  return run('sudo shutdown -h now')
+  return run("sudo shutdown -h now")
+
 
 def gcp_sync(src, dst, exclude=None):
   if exclude:
-    return subprocess.call([
-      'gsutil',
-      '-m',
-      'rsync',
-      '-r',
-      '-x',
-      exclude,
-      src,
-      dst
-    ])
+    return subprocess.call(
+      ["gsutil", "-m", "rsync", "-r", "-x", exclude, src, dst]
+    )
   else:
-    return subprocess.call([
-      'gsutil',
-      '-m',
-      'rsync',
-      '-r',
-      src,
-      dst
-    ])
+    return subprocess.call(["gsutil", "-m", "rsync", "-r", src, dst])
 
 
 class SyncCallback(Callback):
diff --git a/yann/contrib/pretrained.py b/yann/contrib/pretrained.py
index 08a41fd..b493de0 100644
--- a/yann/contrib/pretrained.py
+++ b/yann/contrib/pretrained.py
@@ -1,12 +1,14 @@
 from torch.nn import AdaptiveAvgPool2d
-from ..data.containers import Outputs, Inputs
+
+from ..data.containers import Inputs, Outputs
 from ..models import Model
 
 
 def register_models():
   import logging
+
   from .. import registry
-  from ..config.registry import pass_args, is_public_callable
+  from ..config.registry import is_public_callable, pass_args
 
   try:
     import pretrainedmodels.models
@@ -17,11 +19,10 @@ def register_models():
     )
   else:
     registry.model.pretrainedmodels.index(
-      pretrainedmodels.models,
-      init=pass_args,
-      include=is_public_callable
+      pretrainedmodels.models, init=pass_args, include=is_public_callable
     )
 
+
 # auto register the models on import of this module
 # NOTE:
 #   prerainedmodels has some top level model instantiation
@@ -39,9 +40,7 @@ class PretrainedModel(Model):
     logits = self.logits(embeddings)
     activations = self.activation(logits) if self.activation else None
     return Outputs(
-      embeddings=embeddings,
-      logits=logits,
-      activations=activations
+      embeddings=embeddings, logits=logits, activations=activations
     )
 
 
@@ -55,7 +54,7 @@ class PretrainedModelWrapper(Model):
     if self.pool:
       self.model.avg_pool = pool
 
-    if hasattr(self.model, 'avgpool') and not hasattr(self.model, 'avg_pool'):
+    if hasattr(self.model, "avgpool") and not hasattr(self.model, "avg_pool"):
       # the pretrained model api is inconsistent and a few cases have avgpool
       # instead of avg_pool
       self.model.avg_pool = self.model.avgpool
@@ -78,9 +77,7 @@ class PretrainedModelWrapper(Model):
     activations = self.activation(logits) if self.activation else None
 
     return Outputs(
-      embeddings=embeddings,
-      logits=logits,
-      activations=activations
+      embeddings=embeddings, logits=logits, activations=activations
     )
 
 
diff --git a/yann/contrib/slack.py b/yann/contrib/slack.py
index 663d4c7..486d92b 100644
--- a/yann/contrib/slack.py
+++ b/yann/contrib/slack.py
@@ -1,28 +1,35 @@
+import json
 import urllib
 import urllib.request
-import json
 
 from ..callbacks.base import Callback
 
+
 def post(url, data):
-  data = json.dumps(data).encode('utf8')
+  data = json.dumps(data).encode("utf8")
   req = urllib.request.Request(
-    url, data=data, headers={'content-type': 'application/json'})
+    url, data=data, headers={"content-type": "application/json"}
+  )
   return urllib.request.urlopen(req)
 
 
-DEFAULT_CHANNEL = '#training'
+DEFAULT_CHANNEL = "#training"
 
-def send(text, attachments=None, channel=None, username=None, icon=None,
-         url=None):
+
+def send(
+  text, attachments=None, channel=None, username=None, icon=None, url=None
+):
   """https://api.slack.com/docs/message-attachments"""
-  return post(url, {
-    'text': text,
-    'channel': channel or DEFAULT_CHANNEL,
-    'username': username,
-    'icon_emoji': icon and (icon if icon.startswith(':') else f':{icon}:'),
-    'attachments': attachments,
-  })
+  return post(
+    url,
+    {
+      "text": text,
+      "channel": channel or DEFAULT_CHANNEL,
+      "username": username,
+      "icon_emoji": icon and (icon if icon.startswith(":") else f":{icon}:"),
+      "attachments": attachments,
+    },
+  )
 
 
 def atch(title=None, text=None, fields=None, color=None, **kwargs):
@@ -31,10 +38,10 @@ def atch(title=None, text=None, fields=None, color=None, **kwargs):
     text=text,
     fields=[
       *(fields or {}),
-      *({'title': k, 'value': v} for k, v in kwargs.items())],
-    color=color)
-
-
+      *({"title": k, "value": v} for k, v in kwargs.items()),
+    ],
+    color=color,
+  )
 
 
 class Slack(Callback):
@@ -52,34 +59,35 @@ class Slack(Callback):
       channel=self.channel,
       username=self.username,
       url=self.url,
-      **kwargs
+      **kwargs,
     )
 
   def on_train_start(self, trainer=None):
     self.send(
-      text='Starting train run',
+      text="Starting train run",
       attachments=[
         atch(experiment=trainer.name, text=trainer.description),
-        atch('Configuration', f"```{trainer}```", color='good'),
-      ]
+        atch("Configuration", f"```{trainer}```", color="good"),
+      ],
     )
 
-  def on_validation_end(self, targets=None, outputs=None, loss=None,
-                        trainer=None):
+  def on_validation_end(
+    self, targets=None, outputs=None, loss=None, trainer=None
+  ):
     if self.validation:
       self.send(
-        text=f'Completed epoch {trainer.num_epochs} with loss: {loss.item()}'
+        text=f"Completed epoch {trainer.num_epochs} with loss: {loss.item()}"
       )
 
   def on_error(self, error, trainer=None):
     self.send(
-      text='Training run failed due to an exception',
+      text="Training run failed due to an exception",
       attachments=[
         atch(experiment=trainer.name),
         atch(epoch=trainer.num_epochs + 1),
-        atch('Exception', f"```{str(error)}```", color='danger'),
-      ]
+        atch("Exception", f"```{str(error)}```", color="danger"),
+      ],
     )
 
   def on_train_end(self, trainer=None):
-    self.send(text='Train run completed')
\ No newline at end of file
+    self.send(text="Train run completed")
diff --git a/yann/cuda.py b/yann/cuda.py
index 5981060..328b5c4 100644
--- a/yann/cuda.py
+++ b/yann/cuda.py
@@ -1,15 +1,14 @@
 import torch
 
 
-
 def device_info(d=None):
   d = d or torch.cuda.current_device()
   return {
-    'name': torch.cuda.get_device_name(d),
-    'capability': torch.cuda.get_device_capability(d),
-    'allocated_memory': torch.cuda.memory_allocated(d),
-    'cached_memory': torch.cuda.memory_cached(d),
+    "name": torch.cuda.get_device_name(d),
+    "capability": torch.cuda.get_device_capability(d),
+    "allocated_memory": torch.cuda.memory_allocated(d),
+    "cached_memory": torch.cuda.memory_cached(d),
   }
 
 
-sync = torch.cuda.synchronize
\ No newline at end of file
+sync = torch.cuda.synchronize
diff --git a/yann/data/__init__.py b/yann/data/__init__.py
index 7ec6aa3..652157a 100644
--- a/yann/data/__init__.py
+++ b/yann/data/__init__.py
@@ -1,23 +1,25 @@
-import torch
 import types
 
+import torch
+
+from . import place
 from .classes import Classes
 from .loaders import TransformLoader
 from .transform import Transformer
-from . import place
+
 
 def get_name(x):
-  if hasattr(x, 'name'):
+  if hasattr(x, "name"):
     return x.name
 
   return x.__class__.__name__
 
 
 def get_dataset_name(x):
-  while hasattr(x, 'dataset'):
+  while hasattr(x, "dataset"):
     x = x.dataset
 
-  if hasattr(x, 'name'):
+  if hasattr(x, "name"):
     return x.name
 
   return x.__class__.__name__
@@ -27,6 +29,7 @@ def batches(*tensors, size=32, shuffle=False, order=None):
   if len(tensors) == 1 and isinstance(tensors[0], str):
     # assume a registered dataset name was passed (like batches('MNIST'))
     import yann
+
     tensors = (yann.resolve.dataset(tensors[0]),)
   if shuffle:
     order = torch.randperm(len(tensors[0]))
@@ -34,17 +37,17 @@ def batches(*tensors, size=32, shuffle=False, order=None):
   if len(tensors) == 1:
     for i in range(0, len(tensors[0]), size):
       if order is not None:
-        indices = order[i:i+size]
+        indices = order[i : i + size]
         yield tensors[0][indices]
       else:
-        yield tensors[0][i:i+size]
+        yield tensors[0][i : i + size]
   else:
     for i in range(0, len(tensors[0]), size):
       if order is not None:
-        indices = order[i:i+size]
+        indices = order[i : i + size]
         yield tuple(t[indices] for t in tensors)
       else:
-        yield tuple(t[i:i+size] for t in tensors)
+        yield tuple(t[i : i + size] for t in tensors)
 
 
 def unbatch(batches):
@@ -61,7 +64,7 @@ def chunk(sequence, size=32):
         batch = []
   else:
     for i in range(0, len(sequence), size):
-      yield sequence[i:i+size]
+      yield sequence[i : i + size]
 
 
 def loop(items):
@@ -72,11 +75,12 @@ def loop(items):
 def shuffle(*sequences):
   order = torch.randperm(len(sequences[0]))
   return (
-     [s[i] for i in order] if isinstance(s, (tuple, list)) else s[order]
-     for s in sequences
+    [s[i] for i in order] if isinstance(s, (tuple, list)) else s[order]
+    for s in sequences
   )
 
-def flatten(x, out=None, prefix='', sep='.'):
+
+def flatten(x, out=None, prefix="", sep="."):
   """
   Flatten nested dict
   """
@@ -84,7 +88,9 @@ def flatten(x, out=None, prefix='', sep='.'):
 
   if isinstance(x, dict):
     for k in x:
-      flatten(x[k], out=out, prefix=f"{prefix}{sep if prefix else ''}{k}", sep=sep)
+      flatten(
+        x[k], out=out, prefix=f"{prefix}{sep if prefix else ''}{k}", sep=sep
+      )
   elif isinstance(x, (list, tuple)):
     for k, v in enumerate(x):
       flatten(k, out=out, prefix=f"{prefix}{sep if prefix else ''}{k}", sep=sep)
@@ -96,8 +102,9 @@ def flatten(x, out=None, prefix='', sep='.'):
 
 def print_tree(root, indent=4):
   from pathlib import Path
+
   root = Path(root)
-  print(f'{root}')
-  for path in sorted(root.rglob('*')):
+  print(f"{root}")
+  for path in sorted(root.rglob("*")):
     depth = len(path.relative_to(root).parts)
-    print(f'{" " * (depth * indent)} {path.name}')
\ No newline at end of file
+    print(f"{' ' * (depth * indent)} {path.name}")
diff --git a/yann/data/batch.py b/yann/data/batch.py
index e8d482d..26c1aa6 100644
--- a/yann/data/batch.py
+++ b/yann/data/batch.py
@@ -16,7 +16,7 @@ class Batch:
   def size(self):
     if self.inputs is not None:
       return len(self.inputs)
-    raise ValueError('Could not determine size')
+    raise ValueError("Could not determine size")
 
   def __iter__(self):
     yield self.inputs
@@ -29,7 +29,7 @@ class Batch:
       self.outputs,
       self.losses,
       device=device,
-      **kwargs
+      **kwargs,
     )
     return self
 
@@ -38,7 +38,7 @@ batch = Batch()
 batch.size
 
 for batch in batches:
-  batch.to('cuda:0')
+  batch.to("cuda:0")
 
 
 class Batch:
diff --git a/yann/data/classes.py b/yann/data/classes.py
index f60ed3c..ff436f0 100644
--- a/yann/data/classes.py
+++ b/yann/data/classes.py
@@ -1,6 +1,7 @@
-import numpy as np
 from collections import Counter
 
+import numpy as np
+
 # from enum import Enum
 #
 # class Encoding(Enum):
@@ -8,6 +9,7 @@ from collections import Counter
 #   one_hot = 'one_hot'
 #   normalized_one_hot = 'normalized_one_hot'
 
+
 class TargetTransformer:
   def encode(self, x, many=True):
     pass
@@ -30,18 +32,14 @@ class TargetTransformer:
 
 
 class Classes(TargetTransformer):
-  valid_encodings = {
-    'index',
-    'one_hot',
-    'normalized_one_hot'
-  }
+  valid_encodings = {"index", "one_hot", "normalized_one_hot"}
 
   def __init__(
-      self,
-      names=None,
-      meta=None,
-      counts=None,
-      default_encoding='index',
+    self,
+    names=None,
+    meta=None,
+    counts=None,
+    default_encoding="index",
   ):
     if names:
       self.names = list(names)
@@ -50,19 +48,20 @@ class Classes(TargetTransformer):
     elif counts:
       self.names = sorted(counts.keys())
     else:
-      raise ValueError('At least one of names, counts or meta must be defined')
+      raise ValueError("At least one of names, counts or meta must be defined")
     self.indices = {c: i for i, c in enumerate(self.names)}
     self.meta = meta
-    
+
     self.counts = counts
 
-    self.dtype = 'float32'
+    self.dtype = "float32"
 
-    assert default_encoding in self.valid_encodings, \
-      f'default_encoding must be one of {self.valid_encodings}, got {default_encoding}'
+    assert default_encoding in self.valid_encodings, (
+      f"default_encoding must be one of {self.valid_encodings}, got {default_encoding}"
+    )
     self.default_encoding = default_encoding
 
-  def weights(self, list=True, mode='multiclass', normalize=True):
+  def weights(self, list=True, mode="multiclass", normalize=True):
     if self.counts:
       weights = get_class_weights(self.counts, mode=mode, normalize=normalize)
       if list:
@@ -71,7 +70,7 @@ class Classes(TargetTransformer):
         return weights
     else:
       raise NotImplementedError(
-        'Weights can not be determined unless `counts` are set'
+        "Weights can not be determined unless `counts` are set"
       )
 
   @classmethod
@@ -82,10 +81,7 @@ class Classes(TargetTransformer):
         counts[l] += 1
       else:
         counts.update(l)
-    return cls(
-      counts=counts,
-      **kwargs
-    )
+    return cls(counts=counts, **kwargs)
 
   @classmethod
   def ordered(cls, num, **kwargs):
@@ -95,8 +91,8 @@ class Classes(TargetTransformer):
     c = min(len(self.names) // 2, 3)
 
     return (
-      f"Classes(\n" 
-      f"  count={len(self)},\n" 
+      f"Classes(\n"
+      f"  count={len(self)},\n"
       f"  default_encoding={self.default_encoding}\n"
       f"  names=[{', '.join([str(x) for x in self.names[:c]])}, ..., {', '.join([str(x) for x in self.names[-c:]])}]\n"
       # f"  encoded={self.encode(self.names[:c])}, ..., {self.encode(self.names[-c:])}\n"
@@ -105,22 +101,22 @@ class Classes(TargetTransformer):
 
   def state_dict(self):
     return {
-      'names': self.names,
-      'meta': self.meta,
-      'default_encoding': self.default_encoding,
-      'counts': self.counts
+      "names": self.names,
+      "meta": self.meta,
+      "default_encoding": self.default_encoding,
+      "counts": self.counts,
     }
 
   def load_state_dict(self, data):
-    if 'classes' in data:
+    if "classes" in data:
       # fr backwards compatibility since classes was renamed to names
-      self.names = data['classes']
+      self.names = data["classes"]
     else:
-      self.names = data['names']
+      self.names = data["names"]
     self.indices = {c: i for i, c in enumerate(self.names)}
-    self.meta = data['meta']
-    self.default_encoding = data['default_encoding']
-    self.counts = data.get('counts')
+    self.meta = data["meta"]
+    self.default_encoding = data["default_encoding"]
+    self.counts = data.get("counts")
 
   def __getitem__(self, idx):
     return self.names[idx]
@@ -138,11 +134,12 @@ class Classes(TargetTransformer):
     return self.indices == other.indices
 
   def encode(self, seq, encoding=None):
-    return getattr(self, (encoding or self.default_encoding) + '_encode')(seq)
+    return getattr(self, (encoding or self.default_encoding) + "_encode")(seq)
 
   def decode(self, encoded, encoding=None):
-    return getattr(self, (encoding or self.default_encoding) + '_decode')(
-      encoded)
+    return getattr(self, (encoding or self.default_encoding) + "_decode")(
+      encoded
+    )
 
   def index_encode(self, classes):
     if isinstance(classes, (str, int)):
@@ -178,7 +175,8 @@ class Classes(TargetTransformer):
   #     raise NotImplementedError("truncate not supported without counts")
   #   pass
 
-def smooth(y, eps=.1, num_classes=None):
+
+def smooth(y, eps=0.1, num_classes=None):
   if not num_classes:
     if len(y.shape) == 1:
       num_classes = len(y)
@@ -187,7 +185,9 @@ def smooth(y, eps=.1, num_classes=None):
   return y * (1 - eps) + eps * (1.0 / num_classes)
 
 
-def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num_samples=None):
+def get_class_weights(
+  class_counts: dict, mode="multiclass", normalize=True, num_samples=None
+):
   """
   Args:
     class_counts: dict mapping from class to count
@@ -197,21 +197,20 @@ def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num
   Returns:
     weights (dict): mapping from class to weight value
   """
-  if mode == 'multiclass':
+  if mode == "multiclass":
     num_samples = num_samples or sum(class_counts.values())
-    weights = {
-      k: num_samples / count for k, count in class_counts.items()
-    }
+    weights = {k: num_samples / count for k, count in class_counts.items()}
     if normalize:
       scale = len(weights) / sum(weights.values())
       return {k: w * scale for k, w in weights.items()}
     else:
       return weights
-  elif mode in ('binary', 'multilabel'):
+  elif mode in ("binary", "multilabel"):
     # NOTE: a bit of a hack, assuming num pos labels == num_samples
     num_samples = num_samples or sum(class_counts.values())
     weights = {
-      k: (num_samples - pos_count) / pos_count for k, pos_count in class_counts.items()
+      k: (num_samples - pos_count) / pos_count
+      for k, pos_count in class_counts.items()
     }
     if normalize:
       scale = len(weights) / sum(weights.values())
@@ -221,5 +220,5 @@ def get_class_weights(class_counts: dict, mode='multiclass', normalize=True, num
   else:
     raise ValueError(
       f'''Unsupported mode, got "{mode}", expected one of '''
-      '''multiclass, multilabel, binary'''
-    )
\ No newline at end of file
+      """multiclass, multilabel, binary"""
+    )
diff --git a/yann/data/collate.py b/yann/data/collate.py
index d21de64..44780c9 100644
--- a/yann/data/collate.py
+++ b/yann/data/collate.py
@@ -25,9 +25,9 @@ class PadCollate:
 class FilterCollate:
   def __init__(
     self,
-      filter=None,
-      value=None,
-      collate=default_collate,
+    filter=None,
+    value=None,
+    collate=default_collate,
   ):
     self.filter = filter or (
       lambda items: [it for it in items if it is not value]
@@ -61,21 +61,16 @@ def image_collate(batch, memory_format=torch.contiguous_format):
 
   for i, img in enumerate(images):
     nump_array = np.asarray(img, dtype=np.uint8)
-    if (nump_array.ndim < 3):
+    if nump_array.ndim < 3:
       nump_array = np.expand_dims(nump_array, axis=-1)
     nump_array = np.rollaxis(nump_array, 2)
     tensor[i] += torch.from_numpy(nump_array)
   return tensor, targets
 
 
-
 class KeyCollate:
   def __init__(self, *keys):
     self.keys = keys
 
   def __call__(self, samples):
-    return tuple(
-      torch.stack(
-        [s[k] for s in samples]
-      ) for k in self.keys
-    )
\ No newline at end of file
+    return tuple(torch.stack([s[k] for s in samples]) for k in self.keys)
diff --git a/yann/data/collection.py b/yann/data/collection.py
index 1628a97..5c4d663 100644
--- a/yann/data/collection.py
+++ b/yann/data/collection.py
@@ -1,4 +1,4 @@
-from collections import defaultdict, Counter
+from collections import Counter, defaultdict
 from itertools import chain
 
 
@@ -25,7 +25,6 @@ def count(*args):
   return Counter(chain(*args))
 
 
-
 class Collection:
   def __init__(self, items):
     self.items = items
@@ -47,9 +46,7 @@ class Collection:
       yield from (tuple(getattr(x, a) for a in attrs) for x in self.items)
 
   def filter(self, condition):
-    return Collection(
-      x for x in self if condition(x)
-    )
+    return Collection(x for x in self if condition(x))
 
   def map(self, f):
     return Collection(f(x) for x in self)
@@ -59,43 +56,38 @@ class Collection:
       return sorted(
         self.items,
         key=lambda x: tuple(getattr(x, p) for p in props),
-        reverse=reverse
+        reverse=reverse,
       )
 
-    return sorted(
-      self.items,
-      key=key,
-      reverse=reverse
-    )
+    return sorted(self.items, key=key, reverse=reverse)
 
   def __getattr__(self, name: str):
-    if name.startswith('by_unique_'):
-      x = by(self.items, name[len('by_unique_'):], unique=True)
+    if name.startswith("by_unique_"):
+      x = by(self.items, name[len("by_unique_") :], unique=True)
       setattr(self, name, x)
       return x
 
-    if name.startswith('by_'):
+    if name.startswith("by_"):
       x = by(self.items, name[3:])
       setattr(self, name, x)
       return x
 
-    if name.endswith('_counts'):
-      attr = name[:-len('_counts')]
+    if name.endswith("_counts"):
+      attr = name[: -len("_counts")]
       x = count(getattr(x, attr) for x in self.items)
       setattr(self, name, x)
       return x
 
-    if name.endswith('_set'):
-      attr = name[:-len('_set')]
+    if name.endswith("_set"):
+      attr = name[: -len("_set")]
       x = set(getattr(x, attr) for x in self.items)
       setattr(self, name, x)
       return x
 
-    if '_to_' in name:
-      src, dst = name.split('_to_')
+    if "_to_" in name:
+      src, dst = name.split("_to_")
       x = {getattr(x, src): getattr(x, dst) for x in self.items}
       setattr(self, name, x)
       return x
 
     raise AttributeError(name)
-
diff --git a/yann/data/containers.py b/yann/data/containers.py
index 2045cda..97efceb 100644
--- a/yann/data/containers.py
+++ b/yann/data/containers.py
@@ -1,13 +1,10 @@
-from collections import OrderedDict
-from collections import abc
+from collections import OrderedDict, abc
 from typing import List
 
 
 class Container(abc.MutableMapping):
   def __init__(self, *args, **kwargs):
-    items = OrderedDict(
-      ('_arg' + str(n), v) for n, v in enumerate(args)
-    )
+    items = OrderedDict(("_arg" + str(n), v) for n, v in enumerate(args))
     items.update(kwargs)
 
     self.__dict__.update(items)
@@ -62,4 +59,3 @@ class Samples:
 
   def __iter__(self):
     return (*self.inputs, *self.targets)
-
diff --git a/yann/data/images.py b/yann/data/images.py
index 81373ae..de8339f 100644
--- a/yann/data/images.py
+++ b/yann/data/images.py
@@ -1,8 +1,9 @@
-from PIL import Image
 import io
 
+from PIL import Image
+
 
-def image_to_bytes(image: Image.Image, format='jpeg'):
+def image_to_bytes(image: Image.Image, format="jpeg"):
   buff = io.BytesIO()
   image.save(buff, format=format)
   return buff.getvalue()
@@ -14,4 +15,5 @@ def image_from_bytes(buffer):
 
 def enable_loading_truncated_images():
   from PIL import ImageFile
-  ImageFile.LOAD_TRUNCATED_IMAGES = True
\ No newline at end of file
+
+  ImageFile.LOAD_TRUNCATED_IMAGES = True
diff --git a/yann/data/io/__init__.py b/yann/data/io/__init__.py
index b84d094..26b3440 100644
--- a/yann/data/io/__init__.py
+++ b/yann/data/io/__init__.py
@@ -1,10 +1,10 @@
+import csv
+import gzip
 import json
 import os
 import pickle as pkl
 import tarfile
 from collections import namedtuple
-import csv
-import gzip
 from pathlib import Path
 from typing import Union
 
@@ -13,16 +13,18 @@ import torch
 
 class Loader:
   """
-    gs://bucket/file.th
-    ./foo/**/*.jpg
+  gs://bucket/file.th
+  ./foo/**/*.jpg
   """
 
-  def __call__(self, path, format=None, deserialize=None, filesystem=None, **kwargs):
+  def __call__(
+    self, path, format=None, deserialize=None, filesystem=None, **kwargs
+  ):
     path = Path(path)
     format = format or path.suffix[1:]
     if hasattr(self, format):
       return getattr(self, format)(str(path), **kwargs)
-    raise ValueError(f'File format not supported ({format})')
+    raise ValueError(f"File format not supported ({format})")
 
   def th(self, path, **kwargs):
     return torch.load(path, **kwargs)
@@ -35,23 +37,28 @@ class Loader:
 
   def parquet(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_parquet(path, **kwargs)
 
   def csv(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_csv(path, **kwargs)
 
   def tsv(self, path, **kwargs):
     import pandas as pd
+
     return pd.read_csv(path, **kwargs)
 
   def yaml(self, path, **kwargs):
     import yaml
-    with open(path, 'r') as f:
+
+    with open(path, "r") as f:
       return yaml.load(f, yaml.SafeLoader)
 
   def image(self, path, **kwargs):
     import PIL.Image
+
     return PIL.Image.open(path)
 
   png = image
@@ -68,18 +75,21 @@ load = Loader()
 
 def to_pyarrow_table(x):
   import pyarrow as pa
+
   try:
     import pandas as pd
+
     if isinstance(x, pd.DataFrame):
       x = pa.Table.from_pandas(x)
   except ImportError:
     pass
 
   if not isinstance(x, pa.Table):
-    raise ValueError(f'unsupported type {type(x)}')
+    raise ValueError(f"unsupported type {type(x)}")
 
   return x
 
+
 class Saver:
   def __call__(
     self, x, path, format=None, serialize=None, filesystem=None, **kwargs
@@ -88,10 +98,10 @@ class Saver:
     format = format or path.suffix[1:]
     if hasattr(self, format):
       return getattr(self, format)(x, path, **kwargs)
-    raise ValueError(f'File format not supported ({format})')
+    raise ValueError(f"File format not supported ({format})")
 
   def txt(self, x, path):
-    with open(path, 'w') as f:
+    with open(path, "w") as f:
       f.write(x)
 
   def th(self, x, path, **kwargs):
@@ -102,28 +112,30 @@ class Saver:
 
   def yaml(self, x, path, **kwargs):
     import yaml
-    with open(path, 'w') as f:
+
+    with open(path, "w") as f:
       yaml.dump(x, f, sort_keys=False)
 
   def csv(self, x, path, **kwargs):
     import pyarrow.csv as csv
+
     x = to_pyarrow_table(x)
     csv.write_csv(x, path, **kwargs)
 
   def parquet(
-      self, x: Union['pandas.Dataframe', 'pyarrow.Table'],
-      path,
-      **kwargs
+    self, x: Union["pandas.Dataframe", "pyarrow.Table"], path, **kwargs
   ):
-    import pyarrow.parquet as pq
     import pyarrow as pa
+    import pyarrow.parquet as pq
 
     x = to_pyarrow_table(x)
 
     if isinstance(x, pa.Table):
       pq.write_table(x, path, **kwargs)
     else:
-      raise ValueError(f'Unsupported type {type(x)} expected pandas.Dataframe or pyarrow.Table')
+      raise ValueError(
+        f"Unsupported type {type(x)} expected pandas.Dataframe or pyarrow.Table"
+      )
 
   def pickle(self, x, path, **kwargs):
     return save_pickle(x, path, **kwargs)
@@ -133,26 +145,27 @@ class Saver:
   pt = th
   pth = th
 
+
 save = Saver()
 
 
-def save_pickle(obj, path, mode='wb'):
+def save_pickle(obj, path, mode="wb"):
   with open(str(path), mode) as f:
     pkl.dump(obj, f)
 
 
-def load_pickle(path, mode='rb'):
+def load_pickle(path, mode="rb"):
   with open(str(path), mode) as f:
     return pkl.load(f)
 
 
-def save_json(obj, path, mode='w'):
+def save_json(obj, path, mode="w"):
   with open(str(path), mode) as f:
     json.dump(obj, f)
   return path
 
 
-def load_json(path, mode='r'):
+def load_json(path, mode="r"):
   with open(str(path), mode) as f:
     return json.load(f)
 
@@ -161,7 +174,7 @@ def tar_dir(path, dest=None, zip=True):
   path = str(path)
   dest = str(dest or path)
 
-  ext, mode = ('.tar.gz', 'w:gz') if zip else ('.tar', 'w')
+  ext, mode = (".tar.gz", "w:gz") if zip else (".tar", "w")
 
   if not dest.endswith(ext):
     dest = os.path.splitext(dest)[0] + ext
@@ -170,12 +183,12 @@ def tar_dir(path, dest=None, zip=True):
     tar.add(path)
 
 
-def lines(path, mode='r'):
+def lines(path, mode="r"):
   with open(path, mode=mode) as f:
     yield from f
 
 
-def write_lines(items, path, mode='w'):
+def write_lines(items, path, mode="w"):
   with open(path, mode=mode) as f:
     f.writelines(items)
 
@@ -187,18 +200,19 @@ def untar(path):
 
 def unzip(zip, dest):
   import zipfile
-  with zipfile.ZipFile(zip, 'r') as f:
-      f.extractall(dest)
+
+  with zipfile.ZipFile(zip, "r") as f:
+    f.extractall(dest)
 
 
-def iter_csv(path, header=True, tuples=True, sep=',', quote='"', **kwargs):
+def iter_csv(path, header=True, tuples=True, sep=",", quote='"', **kwargs):
   with open(path) as f:
     reader = csv.reader(f, delimiter=sep, quotechar=quote, **kwargs)
     if header:
       if tuples:
         reader = iter(reader)
         h = next(reader)
-        Row = namedtuple('Row', h)
+        Row = namedtuple("Row", h)
 
         for r in reader:
           yield Row(*r)
@@ -212,7 +226,7 @@ def iter_csv(path, header=True, tuples=True, sep=',', quote='"', **kwargs):
 
 
 def write_csv(data, path, header=None):
-  with gzip.open(path, 'wt') if path.endswith('.gz') else open(path, 'w') as f:
+  with gzip.open(path, "wt") if path.endswith(".gz") else open(path, "w") as f:
     writer = csv.writer(f)
     if header:
       writer.writerow(header)
diff --git a/yann/data/io/download.py b/yann/data/io/download.py
index d380037..d081288 100644
--- a/yann/data/io/download.py
+++ b/yann/data/io/download.py
@@ -1,9 +1,9 @@
+import os
+import pathlib
 import urllib.request
 from concurrent import futures
-import pathlib
 from pathlib import Path
 from urllib.parse import urlparse
-import os
 
 from ...utils import progress
 
@@ -13,7 +13,7 @@ class CachedExecutor:
     self._executor = futures.ThreadPoolExecutor(max_workers=workers)
 
     self.pending = {}  # key => future
-    self.results = {} # key => local path
+    self.results = {}  # key => local path
     self.errors = {}  # key => error
 
     self.error_callbacks = []
@@ -88,7 +88,7 @@ class CachedExecutor:
   def enqueue(self, key, *args, **kwargs):
     if key in self.results or key in self.pending:
       return
-    return self.submit(key,  *args, **kwargs)
+    return self.submit(key, *args, **kwargs)
 
   def get(self, key):
     if key in self.results:
@@ -101,12 +101,8 @@ class CachedExecutor:
   def __getitem__(self, key):
     return self.get(key)
 
-  def __contains__(self,key):
-    return (
-      key in self.results
-      or key in self.errors
-      or key in self.pending
-    )
+  def __contains__(self, key):
+    return key in self.results or key in self.errors or key in self.pending
 
   def __delitem__(self, key):
     self.results.pop(key)
@@ -115,8 +111,9 @@ class CachedExecutor:
     if p:
       p.cancel()
 
+
 class Downloader(CachedExecutor):
-  def __init__(self, local_root='./', workers=8):
+  def __init__(self, local_root="./", workers=8):
     super(Downloader, self).__init__(workers=workers)
     self.local_root = local_root
 
@@ -135,15 +132,15 @@ class Downloader(CachedExecutor):
     return download(uri, path, root=self.local_root)
 
 
-def download(url, dest=None, skip_existing=True, nest=True, root='./'):
+def download(url, dest=None, skip_existing=True, nest=True, root="./"):
   """
   Returns: (local_path, headers), headers will be None if file exists
   """
   if not dest:
     root = os.path.abspath(root)
-    dest = (urlparse(url).path if nest else os.path.basename(urlparse(url).path))
-    dest = os.path.join(root, dest[1:] if dest[0] == '/' else dest)
-  elif hasattr(dest, '__call__'):
+    dest = urlparse(url).path if nest else os.path.basename(urlparse(url).path)
+    dest = os.path.join(root, dest[1:] if dest[0] == "/" else dest)
+  elif hasattr(dest, "__call__"):
     dest = dest(url)
 
   os.makedirs(os.path.dirname(dest), exist_ok=True)
@@ -153,8 +150,9 @@ def download(url, dest=None, skip_existing=True, nest=True, root='./'):
   return urllib.request.urlretrieve(url, dest)
 
 
-def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
-                  max_workers=12):
+def download_urls(
+  urls, dest=None, skip_existing=True, nest=True, root="./", max_workers=12
+):
   results, errors = [], []
   with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
     queued_futures = {
@@ -164,7 +162,7 @@ def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
         dest=dest,
         skip_existing=skip_existing,
         nest=nest,
-        root=root
+        root=root,
       ): url
       for url in urls
     }
@@ -173,4 +171,4 @@ def download_urls(urls, dest=None, skip_existing=True, nest=True, root='./',
         results.append(f.result())
       except Exception as e:
         errors.append((queued_futures[f], e, f))
-  return results, errors
\ No newline at end of file
+  return results, errors
diff --git a/yann/data/loaders.py b/yann/data/loaders.py
index 83f5b05..c6fa467 100644
--- a/yann/data/loaders.py
+++ b/yann/data/loaders.py
@@ -1,9 +1,16 @@
-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset
-from typing import Union, Iterable, Optional, Callable
+from typing import Callable, Iterable, Optional, Union
+
+from torch.utils.data import (
+  DataLoader,
+  Dataset,
+  RandomSampler,
+  SequentialSampler,
+)
 
-from ..datasets import TransformDataset
 import yann
 
+from ..datasets import TransformDataset
+
 
 class LoopedDataLoader(DataLoader):
   """
@@ -14,6 +21,7 @@ class LoopedDataLoader(DataLoader):
 
   # might be fixed here https://github.com/pytorch/pytorch/pull/35795
   """
+
   def __init__(self, *args, **kwargs):
     super(DataLoader, self).__init__(*args, **kwargs)
     self.__initialized = False
@@ -40,14 +48,15 @@ class LoopSampler:
 
 class TransformLoader(DataLoader):
   def __init__(self, dataset, transform, **kwargs):
-    super(TransformLoader, self
-         ).__init__(TransformDataset(dataset, transform), **kwargs)
+    super(TransformLoader, self).__init__(
+      TransformDataset(dataset, transform), **kwargs
+    )
 
 
 def loader(
   data: Union[str, Iterable, Dataset, DataLoader],
   transform: Optional[Callable] = None,
-  **kwargs
+  **kwargs,
 ):
   """instantiate a loader from a dataset name, dataset or loader"""
   if isinstance(data, DataLoader):
diff --git a/yann/data/metrics.py b/yann/data/metrics.py
index 8d2a3e3..f11617b 100644
--- a/yann/data/metrics.py
+++ b/yann/data/metrics.py
@@ -1,5 +1,6 @@
-from time import time as get_time
 from statistics import mean
+from time import time as get_time
+
 
 def padded_insert(items, index, value, null_val=None):
   """
@@ -34,15 +35,17 @@ class PaddedList(list):
 
 class MetricStore:
   def __init__(self, names=None, null_val=None, cast_value=None):
-    self.values = {n: PaddedList(null_val=null_val) for n in names} if names else {}
+    self.values = (
+      {n: PaddedList(null_val=null_val) for n in names} if names else {}
+    )
     self.times = PaddedList(null_val=null_val)
 
     self.null_val = null_val
     self.cast_value = cast_value
 
-  def update(self, step='next', time='now', **values):
-    step = len(self.times) if step == 'next' else step
-    time = get_time() if time == 'now' else time
+  def update(self, step="next", time="now", **values):
+    step = len(self.times) if step == "next" else step
+    time = get_time() if time == "now" else time
 
     if self.cast_value:
       values = {k: self.cast_value(v) for (k, v) in values.items()}
@@ -81,12 +84,14 @@ class MetricStore:
 
   def to_pandas(self):
     import pandas as pd
-    return pd.DataFrame({'times': self.times, **self.values})
+
+    return pd.DataFrame({"times": self.times, **self.values})
 
   def plot(self, metrics=None, time=False, clear=False):
     if clear:
       try:
         from IPython.display import clear_output
+
         clear_output(wait=True)
       except:
         pass
@@ -101,9 +106,8 @@ class MetricStore:
         pass
     return s
 
-
   def __repr__(self):
-    return  f"MetricStore({', '.join(f'{k}=(min={min(v)}, max={max(v)})' for k,v in self.values.items())}, len={len(self)})"
+    return f"MetricStore({', '.join(f'{k}=(min={min(v)}, max={max(v)})' for k, v in self.values.items())}, len={len(self)})"
 
 
 class EventStore(list):
@@ -112,10 +116,10 @@ class EventStore(list):
 
 
 class Event:
-  __slots__ = ['key', 'value', 'step', 'time']
+  __slots__ = ["key", "value", "step", "time"]
 
   def __init__(self, key=None, value=None, step=None, time=None):
     self.key = key
     self.value = value
     self.step = step
-    self.time = time
\ No newline at end of file
+    self.time = time
diff --git a/yann/data/place.py b/yann/data/place.py
index f9fc1d2..0795b11 100644
--- a/yann/data/place.py
+++ b/yann/data/place.py
@@ -18,6 +18,7 @@ class Place:
     place = Place({'inputs': dict(device='cuda', memory_format=torch.channels_last), 'targets': 'cpu'})
     place(dict(inputs=t1, targets=t2, keys=[1,2,3])
   """
+
   def __init__(self, placements=None, **kwargs):
     if isinstance(placements, abc.Sequence):
       self.placements = dict(enumerate(placements))
@@ -38,13 +39,14 @@ class Place:
         for n, x in enumerate(batch)
       )
     elif isinstance(batch, abc.Mapping):
-      return batch.__class__({
-        k: x.to(**self.placements[n]) if n in self.placements else x
-        for k, x in batch.items()
-        })
-    elif self.placements is None and hasattr(batch, 'to'):
+      return batch.__class__(
+        {
+          k: x.to(**self.placements[n]) if n in self.placements else x
+          for k, x in batch.items()
+        }
+      )
+    elif self.placements is None and hasattr(batch, "to"):
       return batch.to(**self.kwargs)
     else:
       # TODO: support dataclasses
-      raise ValueError('Batch must be a collection or mappable type')
-
+      raise ValueError("Batch must be a collection or mappable type")
diff --git a/yann/data/sampler.py b/yann/data/sampler.py
index b42f0d5..aac78c2 100644
--- a/yann/data/sampler.py
+++ b/yann/data/sampler.py
@@ -1,7 +1,8 @@
-from torch.utils.data.sampler import Sampler
 import random
 from collections import defaultdict
 
+from torch.utils.data.sampler import Sampler
+
 from . import batches
 
 
@@ -13,7 +14,7 @@ class BalancedTargetSampler(Sampler):
 
     self.target_to_indices = defaultdict(list)
 
-    if not targets and hasattr(dataset, 'targets'):
+    if not targets and hasattr(dataset, "targets"):
       targets = dataset.targets
 
     if targets is not None:
@@ -69,4 +70,4 @@ class BatchShuffleSampler(Sampler):
   def __iter__(self):
     bs = list(batches(range(len(self)), size=self.batch_size))
     random.shuffle(bs)
-    return (s for b in bs for s in b)
\ No newline at end of file
+    return (s for b in bs for s in b)
diff --git a/yann/data/search/annoy.py b/yann/data/search/annoy.py
index 2a8e1a3..29948a3 100644
--- a/yann/data/search/annoy.py
+++ b/yann/data/search/annoy.py
@@ -3,12 +3,12 @@ import os.path
 
 from annoy import AnnoyIndex
 
-from ..io import save_json, load_json
+from ..io import load_json, save_json
 from .base import VectorIndex
 
 
 class Annoy(VectorIndex):
-  def __init__(self, path, dims=None, metric='angular', build_on_disk=True):
+  def __init__(self, path, dims=None, metric="angular", build_on_disk=True):
     self.path = path
     self.is_mutable = None
     self.is_built = None
@@ -16,47 +16,48 @@ class Annoy(VectorIndex):
     self.metric = metric
 
     if os.path.isfile(self.path):
-      logging.debug(f'Loading existing index: {self.path}')
+      logging.debug(f"Loading existing index: {self.path}")
       self.load_meta()
-      assert self.dims == dims or not dims, \
-        'Passed path to existing index but dims do not match'
-      assert self.metric == metric or not metric, \
-        'Passed path to existing index but metrics do not match'
+      assert self.dims == dims or not dims, (
+        "Passed path to existing index but dims do not match"
+      )
+      assert self.metric == metric or not metric, (
+        "Passed path to existing index but metrics do not match"
+      )
       self.index = AnnoyIndex(self.dims, metric=self.metric)
     elif dims:
       logging.debug(
-        f'Creating new index with {dims} dimensions and {self.metric} metric')
+        f"Creating new index with {dims} dimensions and {self.metric} metric"
+      )
       self.dims = dims
       self.index = AnnoyIndex(self.dims, metric=self.metric)
       if build_on_disk:
         self.index.on_disk_build(self.path)
     else:
-      logging.debug(f'Loading existing index: {self.path}')
+      logging.debug(f"Loading existing index: {self.path}")
       self.load_meta()
       self.index = AnnoyIndex(self.dims, metric=self.metric)
 
   @property
   def meta_path(self):
-    return self.path + '.meta.json'
+    return self.path + ".meta.json"
 
   @property
   def files(self):
     return [self.path, self.meta_path]
 
   def load_meta(self):
-    self.__dict__.update(
-      load_json(self.meta_path)
-    )
+    self.__dict__.update(load_json(self.meta_path))
 
   def save_meta(self):
     d = {**self.__dict__}
-    d.pop('index')
+    d.pop("index")
     save_json(d, self.meta_path)
 
   def build(self, num_trees=10):
-    logging.debug(f'staring to build index: {self.path}')
+    logging.debug(f"staring to build index: {self.path}")
     self.index.build(num_trees)
-    logging.debug(f'finished building index: {self.path}')
+    logging.debug(f"finished building index: {self.path}")
     self.is_mutable = False
     self.is_built = True
     self.save_meta()
diff --git a/yann/data/search/inverted_index.py b/yann/data/search/inverted_index.py
index 599266f..3285a91 100644
--- a/yann/data/search/inverted_index.py
+++ b/yann/data/search/inverted_index.py
@@ -1,6 +1,5 @@
-
-from collections import defaultdict
 import operator
+from collections import defaultdict
 from functools import reduce
 
 
@@ -24,4 +23,4 @@ class InvertedIndex:
     if not_vals:
       return self[vals] - self[not_vals]
     else:
-      return self[vals]
\ No newline at end of file
+      return self[vals]
diff --git a/yann/data/serialize.py b/yann/data/serialize.py
index 95a4c09..77a89bc 100644
--- a/yann/data/serialize.py
+++ b/yann/data/serialize.py
@@ -1,12 +1,11 @@
 import pyarrow
 
-
 ctx = pyarrow.serialization.default_serialization_context()
 
+
 def enable_torch_serialization(context=ctx):
-  pyarrow.serialization.register_torch_serialization_handlers(
-    context
-  )
+  pyarrow.serialization.register_torch_serialization_handlers(context)
+
 
 # enable by default
 enable_torch_serialization()
@@ -26,8 +25,8 @@ deserialize = deserialize_arrow
 
 
 def to_bytes(string: str) -> bytes:
-  return string.encode(encoding='utf-8', errors='strict')
+  return string.encode(encoding="utf-8", errors="strict")
 
 
 def to_unicode(b: bytes) -> str:
-  return b.decode(encoding='utf-8', errors='strict')
\ No newline at end of file
+  return b.decode(encoding="utf-8", errors="strict")
diff --git a/yann/data/stats.py b/yann/data/stats.py
index 86229f3..300c044 100644
--- a/yann/data/stats.py
+++ b/yann/data/stats.py
@@ -1,5 +1,6 @@
 import torch
 
+
 class TensorStats:
   def __init__(self, device=None, dim=0):
     self.total = None
@@ -25,8 +26,12 @@ class TensorStats:
     else:
       self.total += sum
       self.count += batch.shape[self.dim]
-      self.max = torch.maximum(self.max, batch.max(dim=self.dim)[0], out=self.max)
-      self.min = torch.minimum(self.min, batch.min(dim=self.dim)[0], out=self.min)
+      self.max = torch.maximum(
+        self.max, batch.max(dim=self.dim)[0], out=self.max
+      )
+      self.min = torch.minimum(
+        self.min, batch.min(dim=self.dim)[0], out=self.min
+      )
 
   @property
   def mean(self):
diff --git a/yann/data/storage/lmdb.py b/yann/data/storage/lmdb.py
index c920035..6224073 100644
--- a/yann/data/storage/lmdb.py
+++ b/yann/data/storage/lmdb.py
@@ -1,10 +1,11 @@
-import lmdb
-import pickle
 import json
+import pickle
 from contextlib import contextmanager
 
-from ..serialize import serialize_arrow, deserialize_arrow, to_bytes, to_unicode
-from ..images import image_to_bytes, image_from_bytes
+import lmdb
+
+from ..images import image_from_bytes, image_to_bytes
+from ..serialize import deserialize_arrow, serialize_arrow, to_bytes, to_unicode
 
 
 class LMDB:
@@ -20,7 +21,7 @@ class LMDB:
     return self.db.stat()
 
   def __len__(self):
-    return self.stats['entries']
+    return self.stats["entries"]
 
   def close(self):
     self.db.close()
@@ -35,14 +36,18 @@ class LMDB:
   def __getitem__(self, key):
     # TODO: support indexing multiple values
     if self._current_transaction:
-      return self.deserialize(self._current_transaction.get(self.serialize_key(key)))
+      return self.deserialize(
+        self._current_transaction.get(self.serialize_key(key))
+      )
     else:
       with self.db.begin(write=False) as t:
         return self.deserialize(t.get(self.serialize_key(key)))
 
   def __setitem__(self, key, value):
     if self._current_transaction:
-      return self._current_transaction.put(self.serialize_key(key), self.serialize(value))
+      return self._current_transaction.put(
+        self.serialize_key(key), self.serialize(value)
+      )
     else:
       with self.db.begin(write=True) as t:
         return t.put(self.serialize_key(key), self.serialize(value))
@@ -106,19 +111,19 @@ class ArrowLMDB(LMDB):
   """
 
   @staticmethod
-  def serialize_key(x): 
+  def serialize_key(x):
     return to_bytes(x)
 
   @staticmethod
-  def deserialize_key(x): 
+  def deserialize_key(x):
     return to_unicode(x)
 
   @staticmethod
-  def serialize(x): 
+  def serialize(x):
     return serialize_arrow(x)
 
   @staticmethod
-  def deserialize(x): 
+  def deserialize(x):
     return deserialize_arrow(x)
 
 
@@ -141,7 +146,7 @@ class PickleLMDB(LMDB):
 
 
 class ImageLMDB(LMDB):
-  format = 'jpeg'
+  format = "jpeg"
 
   def serialize_key(self, x):
     return to_bytes(x)
@@ -171,4 +176,4 @@ class JSONLMDB(LMDB):
 
   @staticmethod
   def deserialize(x):
-    return json.loads(to_unicode(x))
\ No newline at end of file
+    return json.loads(to_unicode(x))
diff --git a/yann/data/storage/parquet.py b/yann/data/storage/parquet.py
index 2f1d87e..5f89295 100644
--- a/yann/data/storage/parquet.py
+++ b/yann/data/storage/parquet.py
@@ -1,6 +1,6 @@
-from pyarrow import parquet as pq
-import pyarrow as pa
 import pandas as pd
+import pyarrow as pa
+from pyarrow import parquet as pq
 
 
 def write_parquet(dest, data, columns=None, **kwargs):
@@ -16,8 +16,8 @@ def write_parquet(dest, data, columns=None, **kwargs):
     with pq.ParquetWriter(dest, schema=table.schema, **kwargs) as writer:
       writer.write_table(table)
       for d in next(data):
-        writer.write_table(pa.Table.from_pandas(pd.DataFrame(d))
+        writer.write_table(pa.Table.from_pandas(pd.DataFrame(d)))
 
 
 def read_parquet():
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/data/transform.py b/yann/data/transform.py
index 1ecda99..d6a8b3c 100644
--- a/yann/data/transform.py
+++ b/yann/data/transform.py
@@ -1,2 +1,2 @@
 # for backwards compatibility, since it was moved to yann.transforms
-from yann.transforms import *
\ No newline at end of file
+from yann.transforms import *
diff --git a/yann/data/utils.py b/yann/data/utils.py
index b1989d2..9af4db1 100644
--- a/yann/data/utils.py
+++ b/yann/data/utils.py
@@ -3,7 +3,8 @@ import torch
 
 
 def pad(tensor, shape, value=0):
-  if tensor.shape == shape: return tensor
+  if tensor.shape == shape:
+    return tensor
 
   if isinstance(tensor, np.ndarray):
     padded = np.zeros(shape, dtype=tensor.dtype)
@@ -18,4 +19,4 @@ def pad(tensor, shape, value=0):
 
 def pad_to_largest(tensors, value=0):
   shape = tuple(max(dim) for dim in zip(*(t.shape for t in tensors)))
-  return [pad(t, shape, value) for t in tensors]
\ No newline at end of file
+  return [pad(t, shape, value) for t in tensors]
diff --git a/yann/datasets/__init__.py b/yann/datasets/__init__.py
index e0ddc6d..23614ef 100644
--- a/yann/datasets/__init__.py
+++ b/yann/datasets/__init__.py
@@ -1,26 +1,31 @@
 import os
 from glob import iglob
-from typing import Union, Iterable, Any
+from typing import Any, Iterable, Union
 
 import torch
 from torch.utils import data
 
-from .wrappers import LookupCache, DatasetWrapper, TransformDataset, IncludeIndex, IndexedView, Subset
 from ..data.classes import Classes
+from .wrappers import (
+  DatasetWrapper,
+  IncludeIndex,
+  IndexedView,
+  LookupCache,
+  Subset,
+  TransformDataset,
+)
 
 
 class Dataset(data.Dataset):
   def state_dict(self):
-    return {
-      'name': self.__class__.__name__
-    }
+    return {"name": self.__class__.__name__}
 
 
 class SupervisedDataset(Dataset):
   def __init__(self):
-    if not hasattr(self, 'inputs'):
+    if not hasattr(self, "inputs"):
       self.inputs = None
-    if not hasattr(self, 'targets'):
+    if not hasattr(self, "targets"):
       self.targets = None
 
   def __getitem__(self, idx):
@@ -37,7 +42,7 @@ class ClassificationDataset(SupervisedDataset):
 
 
 class GlobDataset(Dataset):
-  def __init__(self, pattern='**/*.*', limit=None):
+  def __init__(self, pattern="**/*.*", limit=None):
     paths = []
     for n, p in enumerate(iglob(pattern, recursive=True)):
       if os.path.getsize(p) < 4000:
@@ -78,15 +83,14 @@ class TinyDigits(data.TensorDataset):
 
   def __init__(self, num_classes=10):
     from sklearn.datasets import load_digits
+
     digits = load_digits(num_classes)
     super().__init__(
       torch.from_numpy(digits.images).unsqueeze(1).float(),
-      torch.Tensor(digits.target).long()
+      torch.Tensor(digits.target).long(),
     )
 
 
-
 # from .voc import VOCMultilabel
 # from .coco import CocoMultilabel
 # from .imagenette import Imagenette, Imagewoof
-
diff --git a/yann/datasets/coco.py b/yann/datasets/coco.py
index d684198..80e792b 100644
--- a/yann/datasets/coco.py
+++ b/yann/datasets/coco.py
@@ -1,17 +1,18 @@
+import os
+
 from torchvision.datasets import CocoDetection
-from yann.data import Classes
+
 from yann.config.defaults import default
+from yann.data import Classes
 
-import os
 
 class CocoMultilabel(CocoDetection):
-  def __init__(
-      self,
-      root=None,
-      annFile=None
-  ):
-    root = root or default.datasets_root / 'mscoco/train2017'
-    annFile = annFile or default.datasets_root / 'mscoco/annotations/instances_train2017.json'
+  def __init__(self, root=None, annFile=None):
+    root = root or default.datasets_root / "mscoco/train2017"
+    annFile = (
+      annFile
+      or default.datasets_root / "mscoco/annotations/instances_train2017.json"
+    )
 
     super(CocoMultilabel, self).__init__(
       root=root,
@@ -23,23 +24,24 @@ class CocoMultilabel(CocoDetection):
     # print(self.cat2cat)
 
     self.category_id_to_name = {
-      x['id']: x['name']
-      for x in self.coco.cats.values()
+      x["id"]: x["name"] for x in self.coco.cats.values()
     }
 
     self.labels = {}  # map from image id to label name
     for image_id in self.ids:
       annotation_ids = self.coco.getAnnIds(imgIds=image_id)
       annotations = self.coco.loadAnns(annotation_ids)
-      self.labels[image_id] = list({
-        self.category_id_to_name[annotation['category_id']]
-        for annotation in annotations
-      })
+      self.labels[image_id] = list(
+        {
+          self.category_id_to_name[annotation["category_id"]]
+          for annotation in annotations
+        }
+      )
 
     self.classes = Classes.from_labels(self.labels.values())
 
   def __getitem__(self, index):
     img_id = self.ids[index]
-    file_name = self.coco.loadImgs(img_id)[0]['file_name']
+    file_name = self.coco.loadImgs(img_id)[0]["file_name"]
     path = os.path.join(self.root, file_name)
-    return path, self.labels[img_id]
\ No newline at end of file
+    return path, self.labels[img_id]
diff --git a/yann/datasets/dataframe.py b/yann/datasets/dataframe.py
index 83887cb..dee8eae 100644
--- a/yann/datasets/dataframe.py
+++ b/yann/datasets/dataframe.py
@@ -1,20 +1,16 @@
 import pathlib
 
+import pandas as pd
 from torch.utils.data import Dataset
+
 import yann
 from yann.data import Classes
-import pandas as pd
 
 
 class DataFrame(Dataset):
   data: pd.DataFrame
 
-  def __init__(
-      self,
-      source,
-      columns=None,
-      target_col=None
-  ):
+  def __init__(self, source, columns=None, target_col=None):
     if isinstance(source, (str, pathlib.Path)):
       source = yann.load(source)
     self.data = source
@@ -25,7 +21,6 @@ class DataFrame(Dataset):
     if target_col:
       self.classes = Classes.from_labels(self.data[target_col])
 
-
   def __len__(self):
     return len(self.data)
 
@@ -33,4 +28,4 @@ class DataFrame(Dataset):
     row = self.data.iloc[index]
     if self.columns:
       return row[self.columns]
-    return row
\ No newline at end of file
+    return row
diff --git a/yann/datasets/folder.py b/yann/datasets/folder.py
index 82ee170..f67ee41 100644
--- a/yann/datasets/folder.py
+++ b/yann/datasets/folder.py
@@ -1,4 +1,5 @@
-from torchvision.datasets.folder import ImageFolder as IF, default_loader
+from torchvision.datasets.folder import ImageFolder as IF
+from torchvision.datasets.folder import default_loader
 
 from yann.data import Classes
 
@@ -10,14 +11,14 @@ class ImageFolder(IF):
     transform=None,
     target_transform=None,
     loader=default_loader,
-    is_valid_file=None
+    is_valid_file=None,
   ):
     super(ImageFolder, self).__init__(
       root=root,
       transform=transform,
       target_transform=target_transform,
       loader=loader,
-      is_valid_file=is_valid_file
+      is_valid_file=is_valid_file,
     )
 
-    self.classes = Classes(self.classes)
\ No newline at end of file
+    self.classes = Classes(self.classes)
diff --git a/yann/datasets/food101.py b/yann/datasets/food101.py
index b66ebb8..fb4d282 100644
--- a/yann/datasets/food101.py
+++ b/yann/datasets/food101.py
@@ -12,26 +12,17 @@ from yann.data.classes import Classes
 
 def extract(src, dst=None):
   dst = dst or os.path.dirname(src)
-  with tarfile.open(src, 'r:gz') as tar:
+  with tarfile.open(src, "r:gz") as tar:
     tar.extractall(path=dst)
 
 
 class Food101(Dataset):
-  url = 'http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz'
-  filename = 'food-101.tar.gz'
-
-  splits = {
-    'train',
-    'test'
-  }
-
-  def __init__(
-      self,
-      root=None,
-      split='train',
-      download=False,
-      shuffle=True
-  ):
+  url = "http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz"
+  filename = "food-101.tar.gz"
+
+  splits = {"train", "test"}
+
+  def __init__(self, root=None, split="train", download=False, shuffle=True):
     assert split in self.splits
     self.root = Path(root) if root else yann.default.dataset_root(Food101)
     self.root = self.root.expanduser()
@@ -40,10 +31,9 @@ class Food101(Dataset):
     if not self.root.exists() and download:
       self.download()
 
-    with open(self.meta_path, 'r') as f:
+    with open(self.meta_path, "r") as f:
       self.samples = [
-        (self.get_image_path(name.strip()), name.split('/')[0])
-        for name in f
+        (self.get_image_path(name.strip()), name.split("/")[0]) for name in f
       ]
 
     if shuffle:
@@ -53,10 +43,10 @@ class Food101(Dataset):
 
   @property
   def meta_path(self):
-    return f'{self.root}/meta/{self.split}.txt'
+    return f"{self.root}/meta/{self.split}.txt"
 
   def get_image_path(self, name):
-    return f'{self.root}/images/{name.strip()}.jpg'
+    return f"{self.root}/images/{name.strip()}.jpg"
 
   def __getitem__(self, index):
     return self.samples[index]
@@ -70,11 +60,11 @@ class Food101(Dataset):
     extract(self.root / self.filename, self.root)
 
 
-
 class Food101N(Food101):
   """
   https://kuanghuei.github.io/Food-101N/
   """
-  url = 'https://iudata.blob.core.windows.net/food101/Food-101N_release.zip'
-  filename = 'Food-101N_release.zip'
-  pass
\ No newline at end of file
+
+  url = "https://iudata.blob.core.windows.net/food101/Food-101N_release.zip"
+  filename = "Food-101N_release.zip"
+  pass
diff --git a/yann/datasets/imagenet.py b/yann/datasets/imagenet.py
index 0408e96..e594acf 100644
--- a/yann/datasets/imagenet.py
+++ b/yann/datasets/imagenet.py
@@ -1,5 +1,5 @@
 def get_classes():
-  """Returns classes ordered """
+  """Returns classes ordered"""
   return (
     ("n01440764", "tench"),
     ("n01443537", "goldfish"),
@@ -1000,9 +1000,9 @@ def get_classes():
     ("n13052670", "hen-of-the-woods"),
     ("n13054560", "bolete"),
     ("n13133613", "ear"),
-    ("n15075141", "toilet_tissue")
+    ("n15075141", "toilet_tissue"),
   )
 
 
 mean = [0.485, 0.456, 0.406]
-std = [0.229, 0.224, 0.225]
\ No newline at end of file
+std = [0.229, 0.224, 0.225]
diff --git a/yann/datasets/imagenette.py b/yann/datasets/imagenette.py
index 2609c3e..dcce1c1 100644
--- a/yann/datasets/imagenette.py
+++ b/yann/datasets/imagenette.py
@@ -1,8 +1,9 @@
-from pathlib import Path
 import os
+import random
 import tarfile
 from glob import iglob
-import random
+from pathlib import Path
+
 from torchvision.datasets import utils
 
 from . import ClassificationDataset
@@ -10,33 +11,42 @@ from . import ClassificationDataset
 
 def extract(src, dst=None):
   dst = dst or os.path.dirname(src)
-  with tarfile.open(src, 'r:gz') as tar:
-      tar.extractall(path=dst)
+  with tarfile.open(src, "r:gz") as tar:
+    tar.extractall(path=dst)
 
 
 class Imagenette(ClassificationDataset):
   """
   https://github.com/fastai/imagenette
   """
+
   urls = {
-    160: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz',
-    320: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz',
-    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz'
+    160: "https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz",
+    320: "https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz",
+    None: "https://s3.amazonaws.com/fast-ai-imageclas/imagenette.tgz",
   }
 
   @classmethod
   def get_dirname(cls, url):
-    return url.split('/')[-1][:-4]
+    return url.split("/")[-1][:-4]
 
   @classmethod
   def get_filename(cls, url):
-    return url.split('/')[-1]
-
-  def __init__(self, root='./datasets/', size=None, split='train', download=True, shuffle=True):
+    return url.split("/")[-1]
+
+  def __init__(
+    self,
+    root="./datasets/",
+    size=None,
+    split="train",
+    download=True,
+    shuffle=True,
+  ):
     if size not in self.urls:
       raise ValueError(
         f"Unsupported size '{size}', "
-        f"must be one of '{', '.join(self.urls.keys())}'")
+        f"must be one of '{', '.join(self.urls.keys())}'"
+      )
     self.size = size
     self.root = Path(root).expanduser()
     self.size_root = self.root / self.get_dirname(self.urls[self.size])
@@ -46,16 +56,16 @@ class Imagenette(ClassificationDataset):
     if not self.size_root.exists():
       if not download:
         raise ValueError(
-          f'Could not find dataset at provided root ({self.size_root}) and download=False'
+          f"Could not find dataset at provided root ({self.size_root}) and download=False"
         )
       self.download()
 
-    paths = list(iglob(f'{self.size_root}/{self.split}/**/*.*', recursive=True))
+    paths = list(iglob(f"{self.size_root}/{self.split}/**/*.*", recursive=True))
     if shuffle:
       random.shuffle(paths)
-    self.inputs, self.targets = list(zip(*(
-      (p, p.split('/')[-2]) for p in paths
-    )))
+    self.inputs, self.targets = list(
+      zip(*((p, p.split("/")[-2]) for p in paths))
+    )
 
     super(Imagenette, self).__init__(classes=sorted(set(self.targets)))
 
@@ -69,55 +79,49 @@ class Imagenette(ClassificationDataset):
     root = self.root
     filename = self.get_filename(url)
     utils.download_url(url, root, filename)
-    print('extracting file', root / filename)
+    print("extracting file", root / filename)
     extract(root / filename)
 
 
 class Imagenette160(Imagenette):
   size = 160
+
   def __init__(self, **kwargs):
-    if 'size' in kwargs and kwargs['size'] != self.__class__.size:
-      raise ValueError('size is not a valid argument')
-    super(Imagenette160, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    if "size" in kwargs and kwargs["size"] != self.__class__.size:
+      raise ValueError("size is not a valid argument")
+    super(Imagenette160, self).__init__(size=self.__class__.size, **kwargs)
+
 
 class Imagenette320(Imagenette):
   size = 320
+
   def __init__(self, **kwargs):
-    if 'size' in kwargs and kwargs['size'] != self.__class__.size:
-      raise ValueError('size is not a valid argument')
-    super(Imagenette320, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    if "size" in kwargs and kwargs["size"] != self.__class__.size:
+      raise ValueError("size is not a valid argument")
+    super(Imagenette320, self).__init__(size=self.__class__.size, **kwargs)
+
 
 class Imagewoof(Imagenette):
   urls = {
-    160: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160.tgz',
-    320: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-320.tgz',
-    None: 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof.tgz'
+    160: "https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160.tgz",
+    320: "https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-320.tgz",
+    None: "https://s3.amazonaws.com/fast-ai-imageclas/imagewoof.tgz",
   }
 
 
 class Imagewoof160(Imagenette):
   size = 160
+
   def __init__(self, **kwargs):
-    if 'size' in kwargs and kwargs['size'] != self.__class__.size:
-      raise ValueError('size is not a valid argument')
-    super(Imagewoof160, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
+    if "size" in kwargs and kwargs["size"] != self.__class__.size:
+      raise ValueError("size is not a valid argument")
+    super(Imagewoof160, self).__init__(size=self.__class__.size, **kwargs)
 
 
 class Imagewoof320(Imagenette):
   size = 320
+
   def __init__(self, **kwargs):
-    if 'size' in kwargs and kwargs['size'] != self.__class__.size:
-      raise ValueError('size is not a valid argument')
-    super(Imagewoof320, self).__init__(
-      size=self.__class__.size,
-      **kwargs
-    )
\ No newline at end of file
+    if "size" in kwargs and kwargs["size"] != self.__class__.size:
+      raise ValueError("size is not a valid argument")
+    super(Imagewoof320, self).__init__(size=self.__class__.size, **kwargs)
diff --git a/yann/datasets/voc.py b/yann/datasets/voc.py
index 144350d..6c071fc 100644
--- a/yann/datasets/voc.py
+++ b/yann/datasets/voc.py
@@ -1,24 +1,25 @@
 import collections
-from typing import Any, Callable, Dict, Optional, Tuple, List
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
-from torchvision.datasets.voc import _VOCBase, ET_Element, ET_parse
+from torchvision.datasets.voc import ET_Element, ET_parse, _VOCBase
 
 from yann.data import Classes
 
+
 class VOCMultilabel(_VOCBase):
   _SPLITS_DIR = "Main"
   _TARGET_DIR = "Annotations"
   _TARGET_FILE_EXT = ".xml"
 
   def __init__(
-      self,
-      root: str,
-      year: str = "2012",
-      image_set: str = "train",
-      download: bool = False,
-      transform: Optional[Callable] = None,
-      target_transform: Optional[Callable] = None,
-      transforms: Optional[Callable] = None,
+    self,
+    root: str,
+    year: str = "2012",
+    image_set: str = "train",
+    download: bool = False,
+    transform: Optional[Callable] = None,
+    target_transform: Optional[Callable] = None,
+    transforms: Optional[Callable] = None,
   ):
     super(VOCMultilabel, self).__init__(
       root=root,
@@ -27,12 +28,11 @@ class VOCMultilabel(_VOCBase):
       download=download,
       transform=transform,
       target_transform=target_transform,
-      transforms=transforms
+      transforms=transforms,
     )
 
     self.classes = Classes.from_labels(x[1] for x in self)
 
-
   @property
   def annotations(self) -> List[str]:
     return self.targets
@@ -47,7 +47,7 @@ class VOCMultilabel(_VOCBase):
     """
     img = self.images[index]
     target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())
-    target = list(set(a['name'] for a in target['annotation']['object']))
+    target = list(set(a["name"] for a in target["annotation"]["object"]))
 
     return img, target
 
@@ -61,11 +61,13 @@ class VOCMultilabel(_VOCBase):
           def_dic[ind].append(v)
       if node.tag == "annotation":
         def_dic["object"] = [def_dic["object"]]
-      voc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in
-                             def_dic.items()}
-                  }
+      voc_dict = {
+        node.tag: {
+          ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()
+        }
+      }
     if node.text:
       text = node.text.strip()
       if not children:
         voc_dict[node.tag] = text
-    return voc_dict
\ No newline at end of file
+    return voc_dict
diff --git a/yann/datasets/wrappers.py b/yann/datasets/wrappers.py
index 21c8eb8..fa6e719 100644
--- a/yann/datasets/wrappers.py
+++ b/yann/datasets/wrappers.py
@@ -1,9 +1,9 @@
-from itertools import zip_longest
 import logging
-from typing import Union
+import math
 import typing
+from itertools import zip_longest
+from typing import Union
 
-import math
 import numpy as np
 import torch
 
@@ -62,16 +62,20 @@ class Sliceable(DatasetWrapper):
 
 class Subset(DatasetWrapper):
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, indices: Union[np.ndarray, torch.Tensor]):
-    ...
+  def __init__(
+    self, dataset: typing.Mapping, indices: Union[np.ndarray, torch.Tensor]
+  ): ...
 
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, end: Union[float, int]):
-    ...
+  def __init__(self, dataset: typing.Mapping, end: Union[float, int]): ...
 
   @typing.overload
-  def __init__(self, dataset: typing.Mapping, start: Union[float, int], end: Union[float, int]):
-    ...
+  def __init__(
+    self,
+    dataset: typing.Mapping,
+    start: Union[float, int],
+    end: Union[float, int],
+  ): ...
 
   def __init__(self, dataset, *args):
     super(Subset, self).__init__(dataset)
@@ -83,7 +87,11 @@ class Subset(DatasetWrapper):
     if len(args) == 1:
       if isinstance(args[0], int):
         self.start = 0
-        self.end = args[0] if not (0 < args[0] < 1) else math.floor(len(dataset) * args[0])
+        self.end = (
+          args[0]
+          if not (0 < args[0] < 1)
+          else math.floor(len(dataset) * args[0])
+        )
       elif isinstance(args[0], Union[np.ndarray, torch.Tensor]):
         self.indices = args[0]
     elif len(args) == 2:
@@ -101,7 +109,7 @@ class Subset(DatasetWrapper):
 
   def __getitem__(self, index):
     if index >= len(self):
-      raise IndexError(f'Index out of bounds {index}')
+      raise IndexError(f"Index out of bounds {index}")
     if self.indices is not None:
       return self.dataset[self.indices[index]]
     else:
@@ -117,7 +125,7 @@ class Slice(DatasetWrapper):
 
   def __getitem__(self, idx):
     if idx >= len(self):
-      raise IndexError(f'Index out of bounds {idx}')
+      raise IndexError(f"Index out of bounds {idx}")
     return self.dataset[self.start + idx]
 
   def __len__(self):
@@ -127,6 +135,7 @@ class Slice(DatasetWrapper):
 # class Subset(DatasetWrapper):
 #   pass
 
+
 class IndexedView(DatasetWrapper):
   def __init__(self, dataset, indices):
     super(IndexedView, self).__init__(dataset)
@@ -180,8 +189,9 @@ class LookupCache(DatasetWrapper):
 class TransformDataset(DatasetWrapper):
   def __init__(self, dataset, transform):
     super().__init__(dataset)
-    self.transforms = transform if isinstance(transform, tuple) else (
-      transform,)
+    self.transforms = (
+      transform if isinstance(transform, tuple) else (transform,)
+    )
 
   def __getitem__(self, idx):
     return tuple(
@@ -191,10 +201,11 @@ class TransformDataset(DatasetWrapper):
 
   def __repr__(self):
     return (
-      f'{self.__class__.__name__}('
-      f'\nDataset: {repr(self.dataset)}'
-      f'\nTransforms: {repr(self.transforms)}'
-      f'\n)')
+      f"{self.__class__.__name__}("
+      f"\nDataset: {repr(self.dataset)}"
+      f"\nTransforms: {repr(self.transforms)}"
+      f"\n)"
+    )
 
 
 # class Noisy(DatasetWrapper):
@@ -237,4 +248,3 @@ class VariableLength(DatasetWrapper):
 
   def __getitem__(self, idx):
     return self.dataset[idx % len(self.dataset)]
-
diff --git a/yann/distributed.py b/yann/distributed.py
index b04a839..ea87d34 100644
--- a/yann/distributed.py
+++ b/yann/distributed.py
@@ -1,23 +1,31 @@
-from torch import distributed as dist
-import torch
 import os
 from typing import NamedTuple, Union
 
+import torch
+from torch import distributed as dist
+
 
 class Dist:
   """
   torch.distributed wrapper that also supports non distributed mode
   """
-  def __init__(self, backend='nccl', init_method='env://', world_size=None, rank=None):
+
+  def __init__(
+    self, backend="nccl", init_method="env://", world_size=None, rank=None
+  ):
     self.backend = backend
     self.init_method = init_method
 
     self.world_size = int(
-        world_size if world_size is not None else
-        os.environ.get('WORLD_SIZE', torch.cuda.device_count() if torch.cuda.is_available() else 1)
+      world_size
+      if world_size is not None
+      else os.environ.get(
+        "WORLD_SIZE",
+        torch.cuda.device_count() if torch.cuda.is_available() else 1,
+      )
     )
-    self.rank = rank if rank is not None else int(os.environ.get('RANK', 0))
-    self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
+    self.rank = rank if rank is not None else int(os.environ.get("RANK", 0))
+    self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
 
   def initialize(self):
     if not self.is_enabled or not self.is_available() or self.is_initialized():
@@ -27,10 +35,10 @@ class Dist:
       backend=self.backend,
       init_method=self.init_method,
       world_size=self.world_size,
-      rank=self.rank
+      rank=self.rank,
     )
 
-    if self.backend == 'nccl':
+    if self.backend == "nccl":
       torch.cuda.set_device(self.local_rank)
 
   def cleanup(self):
@@ -74,14 +82,11 @@ class Dist:
     )"""
 
 
-
-
 class DistPlacement(NamedTuple):
   rank: Union[int, None] = None
   local_rank: Union[int, None] = None
 
 
-
 def matches(placement: Union[int, DistPlacement, None], dist: Dist):
   if placement is None:
     return True
@@ -93,4 +98,4 @@ def matches(placement: Union[int, DistPlacement, None], dist: Dist):
       return rank == dist.rank
     if local_rank is not None:
       return local_rank == dist.local_rank
-    return True
\ No newline at end of file
+    return True
diff --git a/yann/evaluation/__init__.py b/yann/evaluation/__init__.py
index 9efbcce..bd3f750 100644
--- a/yann/evaluation/__init__.py
+++ b/yann/evaluation/__init__.py
@@ -1,5 +1,6 @@
 import torch
 
+
 def evaluate_metrics(targets=None, outputs=None, metrics=None):
   values = {}
   with torch.inference_mode():
@@ -46,4 +47,4 @@ class RegressionEvaluator(Evaluator):
 
 
 class RetrievalEvaluator(Evaluator):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/exceptions.py b/yann/exceptions.py
index 6db398d..45a2cab 100644
--- a/yann/exceptions.py
+++ b/yann/exceptions.py
@@ -1,5 +1,3 @@
-
-
 class YannException(Exception):
   pass
 
@@ -9,4 +7,4 @@ class ShapeInferenceError(YannException):
 
 
 class CheckFailure(YannException):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/export.py b/yann/export.py
index eecd2b1..7e9e032 100644
--- a/yann/export.py
+++ b/yann/export.py
@@ -8,79 +8,81 @@ import torch
 
 from .data import Classes
 from .data.io import (
-  load_pickle, load_json, save_json, save_pickle, tar_dir,
-  untar
+  load_json,
+  load_pickle,
+  save_json,
+  save_pickle,
+  tar_dir,
+  untar,
 )
 
 
 # TODO: add way to pass validation data to check model outputs when loaded again
 def export(
-    model=None,
-    preprocess=None,
-    postprocess=None,
-    predict=None,
-    classes=None,
-    trace=False,
-    state_dict=False,
-    path=None,
-    # validation=None,
-    meta=None,
-    tar=False,
-    **kwargs
+  model=None,
+  preprocess=None,
+  postprocess=None,
+  predict=None,
+  classes=None,
+  trace=False,
+  state_dict=False,
+  path=None,
+  # validation=None,
+  meta=None,
+  tar=False,
+  **kwargs,
 ):
   os.makedirs(path, exist_ok=True)
   if os.listdir(path):
     raise ValueError(
-      f'Failed to export because {path} already exists and is not empty')
+      f"Failed to export because {path} already exists and is not empty"
+    )
 
   path = Path(path)
 
   if model:
     if trace is not False and trace is not None:
       from torch import jit
+
       traced = jit.trace(model, trace)
-      traced.save(os.path.join(path, 'model.traced.th'))
+      traced.save(os.path.join(path, "model.traced.th"))
     else:
       if not state_dict:
-        torch.save(
-          model,
-          os.path.join(path, 'model.th')
-        )
+        torch.save(model, os.path.join(path, "model.th"))
       else:
         torch.save(
-          model.state_dict(),
-          os.path.join(path, 'model.state_dict.th')
+          model.state_dict(), os.path.join(path, "model.state_dict.th")
         )
   if preprocess:
-    save_pickle(preprocess, path / 'preprocess.pkl')
+    save_pickle(preprocess, path / "preprocess.pkl")
 
   if postprocess:
-    save_pickle(postprocess, path / 'postprocess.pkl')
+    save_pickle(postprocess, path / "postprocess.pkl")
 
   if meta:
-    save_json(meta, path / 'meta.json')
+    save_json(meta, path / "meta.json")
 
   if classes:
     save_json(
       classes.state_dict() if isinstance(classes, Classes) else classes,
-      path / 'classes.json'
+      path / "classes.json",
     )
 
   if kwargs:
-    save_pickle(kwargs, path / 'kwargs.pkl')
+    save_pickle(kwargs, path / "kwargs.pkl")
 
   if predict:
-    save_pickle(predict, path / 'predict.pkl')
+    save_pickle(predict, path / "predict.pkl")
 
   try:
     subprocess.call(
-      ['conda', 'env', 'export', '-f', os.path.join(path, 'env.yml')]
+      ["conda", "env", "export", "-f", os.path.join(path, "env.yml")]
     )
   except:
-    pass # FIXME
+    pass  # FIXME
 
   subprocess.call(
-    ['pip', 'freeze'], stdout=open(os.path.join(path, 'requirements.txt'), 'w')
+    ["pip", "freeze"], stdout=open(os.path.join(path, "requirements.txt"), "w")
   )
 
   if tar:
@@ -110,39 +112,40 @@ def load(path, eval=True):
   p = Predictor()
 
   # TODO: read from tarfile directly instead
-  if str(path).endswith('.tar.gz'):
+  if str(path).endswith(".tar.gz"):
     untar(path)
-    path = Path(str(path).rstrip('.tar.gz'))
+    path = Path(str(path).rstrip(".tar.gz"))
 
   p.model = None
-  if (path / 'model.th').exists():
-    p.model = torch.load(str(path / 'model.th'))
-  elif (path / 'model.traced.th').exists():
+  if (path / "model.th").exists():
+    p.model = torch.load(str(path / "model.th"))
+  elif (path / "model.traced.th").exists():
     from torch import jit
-    p.model = jit.load(str(path / 'model.traced.th'))
+
+    p.model = jit.load(str(path / "model.traced.th"))
 
   if p.model and eval:
     p.model.eval()
 
   with suppress(FileNotFoundError):
-    p.model_state_dict = torch.load(str(path / 'model.state_dict.th'))
+    p.model_state_dict = torch.load(str(path / "model.state_dict.th"))
 
   with suppress(FileNotFoundError):
-    p.classes = load_json(path / 'classes.json')
+    p.classes = load_json(path / "classes.json")
 
   with suppress(FileNotFoundError):
-    p.preprocess = load_pickle(path / 'preprocess.pkl')
+    p.preprocess = load_pickle(path / "preprocess.pkl")
 
   with suppress(FileNotFoundError):
-    p.postprocess = load_pickle(path / 'postprocess.pkl')
+    p.postprocess = load_pickle(path / "postprocess.pkl")
 
   with suppress(FileNotFoundError):
-    p.predict = load_pickle(path / 'predict.pkl')
+    p.predict = load_pickle(path / "predict.pkl")
 
   with suppress(FileNotFoundError):
-    p.meta = load_json(path / 'meta.json')
+    p.meta = load_json(path / "meta.json")
 
   with suppress(FileNotFoundError):
-    p.kwargs = load_pickle(path / 'kwargs.pkl')
+    p.kwargs = load_pickle(path / "kwargs.pkl")
 
   return p
diff --git a/yann/hooks.py b/yann/hooks.py
index e059055..2285394 100644
--- a/yann/hooks.py
+++ b/yann/hooks.py
@@ -6,7 +6,6 @@ def zero_out_nans(module, grad_input, grad_output):
     grad[grad != grad] = 0  # technically shouldn't modify inputs
 
 
-
 class Hook:
   def __init__(self, module):
     self.module = module
@@ -27,25 +26,27 @@ class Hook:
 
 
 def shape_hook(name, depth=None, show=False):
-    depth = depth if depth is not None else name.count('.')
-    indent = '  ' * depth if name else ''
+  depth = depth if depth is not None else name.count(".")
+  indent = "  " * depth if name else ""
+
+  def print_shapes(m, input, output):
+    print(
+      f"{f'{indent}{name}:': <20}",
+      f"{f'({m.__class__.__name__})': <15}",
+      ", ".join(f"{str(tuple(x.shape)): <15}" for x in input),
+      "=>",
+      f"{str(tuple(output.shape)): <15}",
+    )
 
-    def print_shapes(m, input, output):
-      print(
-        f'{f"{indent}{name}:": <20}',
-        f"{f'({m.__class__.__name__})': <15}",
-        ', '.join(f"{str(tuple(x.shape)): <15}" for x in input),
-        "=>",
-        f"{str(tuple(output.shape)): <15}")
+    if show is not False:
+      import yann
 
-      if show is not False:
-        import yann
-        if show is True:
-          yann.show(output)
-        else:
-          yann.show(output[show])
+      if show is True:
+        yann.show(output)
+      else:
+        yann.show(output[show])
 
-    return print_shapes
+  return print_shapes
 
 
 class ShapeHook(Hook):
@@ -53,12 +54,13 @@ class ShapeHook(Hook):
     self.show = show
     super().__init__(module)
 
-
   def register(self, module):
     self.handles = []
     for n, m in module.named_modules():
-      self.handles.append(m.register_forward_hook(shape_hook(n, show=self.show)))
+      self.handles.append(
+        m.register_forward_hook(shape_hook(n, show=self.show))
+      )
 
   def remove(self):
     for h in self.handles:
-      h.remove()
\ No newline at end of file
+      h.remove()
diff --git a/yann/inference/core.py b/yann/inference/core.py
index dfaffb6..57d65ff 100644
--- a/yann/inference/core.py
+++ b/yann/inference/core.py
@@ -1,37 +1,40 @@
-from typing import Union, Callable, Iterable
 import time
+from typing import Callable, Iterable, Union
+
+import torch
+import torch.jit
 from torch import nn
-from torch.utils.data.dataset import Dataset
 from torch.utils.data.dataloader import DataLoader
-import torch.jit
-import torch
+from torch.utils.data.dataset import Dataset
+
 import yann
 
 from ..data.loaders import TransformLoader
 
 
 def inference_stream(
-    model: Union[nn.Module, Callable, str],
-    data: Union[Dataset, DataLoader, Iterable, str],
-    device=None,
-    transform=None,
-    batch_size=64,
-    parallel=False,
-    num_workers=1,
-    pin_memory=False,
-    shuffle=False,
-    progress=10,
-    eval=True,
+  model: Union[nn.Module, Callable, str],
+  data: Union[Dataset, DataLoader, Iterable, str],
+  device=None,
+  transform=None,
+  batch_size=64,
+  parallel=False,
+  num_workers=1,
+  pin_memory=False,
+  shuffle=False,
+  progress=10,
+  eval=True,
 ):
   device = device or yann.default.device
 
   if isinstance(model, str):
-    model = torch.jit.load(model, 'cpu')
+    model = torch.jit.load(model, "cpu")
 
   if isinstance(model, nn.Module):
     if parallel:
       model = nn.DataParallel(model)
-    if eval: model.eval()
+    if eval:
+      model.eval()
     model.to(device)
 
   if isinstance(data, str):
@@ -44,7 +47,7 @@ def inference_stream(
       pin_memory=pin_memory,
       batch_size=batch_size,
       shuffle=shuffle,
-      num_workers=num_workers
+      num_workers=num_workers,
     )
 
   try:
@@ -62,7 +65,8 @@ def inference_stream(
       yield (inputs, *rest, outputs)
 
       if progress and idx % progress == 0:
-        print(f"[{idx} / {size}] ({time.time() - begin}, total: {time.time() - start})")
+        print(
+          f"[{idx} / {size}] ({time.time() - begin}, total: {time.time() - start})"
+        )
 
       begin = time.time()
-
diff --git a/yann/inference/predict.py b/yann/inference/predict.py
index d570b00..56a2e7f 100644
--- a/yann/inference/predict.py
+++ b/yann/inference/predict.py
@@ -9,7 +9,6 @@ class Predictor:
   def __init__(self, model):
     self.model: torch.nn.Module = model
 
-
   def __call__(self, *args, **kwargs):
     x = self.load(*args, **kwargs)
     x = self.transform(x)
diff --git a/yann/init.py b/yann/init.py
index 31d88b1..eca0de1 100644
--- a/yann/init.py
+++ b/yann/init.py
@@ -1,3 +1,5 @@
+import math
+
 from torch import nn
 from torch.nn import init
 
@@ -5,7 +7,7 @@ from torch.nn import init
 def kaiming(model: nn.Module):
   for module in model.modules():
     if isinstance(module, nn.Conv2d):
-      init.kaiming_normal(module.weight, mode='fan_out')
+      init.kaiming_normal(module.weight, mode="fan_out")
       if module.bias is not None:
         init.constant(module.bias, 0)
 
@@ -20,3 +22,8 @@ def kaiming(model: nn.Module):
 
 
 msr = kaiming
+
+
+def linear_zero_bias(linear: nn.Module, num_classes):
+  init.zeros_(linear.weight)
+  init.constant_(linear.bias, -math.log(num_classes))
diff --git a/yann/lr.py b/yann/lr.py
index aae70a1..ef7a9c7 100644
--- a/yann/lr.py
+++ b/yann/lr.py
@@ -1,30 +1,40 @@
 import os
 
 from . import set_param
-from .train import train, Trainer
-
-
-
-def lr_range_test(trainer: Trainer,  min_lr=.00001, max_lr=1, steps=None,
-                  step=None, log_freq=None, restore=True):
+from .train import Trainer, train
+
+
+def lr_range_test(
+  trainer: Trainer,
+  min_lr=0.00001,
+  max_lr=1,
+  steps=None,
+  step=None,
+  log_freq=None,
+  restore=True,
+):
   # assert max_lr > min_lr
   if restore:
-    checkpoint_path = trainer.checkpoint(name='lr-range-test')
+    checkpoint_path = trainer.checkpoint(name="lr-range-test")
   else:
     checkpoint_path = None
 
   steps = steps or len(trainer.loader)
   step = step or ((max_lr - min_lr) / steps)
 
-  set_param(trainer.optimizer, 'lr', min_lr)
+  set_param(trainer.optimizer, "lr", min_lr)
   try:
     cur_lr = min_lr
     cur_step = 0
 
     while cur_step < steps:
-      for x, y, pred, loss in train(trainer.model, trainer.loader,
-                                    trainer.optimizer, trainer.loss,
-                                    trainer.device):
+      for x, y, pred, loss in train(
+        trainer.model,
+        trainer.loader,
+        trainer.optimizer,
+        trainer.loss,
+        trainer.device,
+      ):
         yield (cur_lr, loss)
 
         if log_freq and cur_step % log_freq == 0:
@@ -32,18 +42,10 @@ def lr_range_test(trainer: Trainer,  min_lr=.00001, max_lr=1, steps=None,
 
         cur_lr += step
         cur_step += 1
-        set_param(trainer.optimizer, 'lr', cur_lr)
-
-
+        set_param(trainer.optimizer, "lr", cur_lr)
 
   finally:
     if restore:
-      print('loading checkpoint')
+      print("loading checkpoint")
       trainer.load_checkpoint(checkpoint_path)
       os.remove(checkpoint_path)
-
-
-
-
-
-
diff --git a/yann/metrics.py b/yann/metrics.py
index 98ccd71..0e82a12 100644
--- a/yann/metrics.py
+++ b/yann/metrics.py
@@ -1,16 +1,16 @@
-import torch
-import numpy as np
-from sklearn import metrics
-from functools import partial
 from collections import deque
+from functools import partial
 
-from .utils import to_numpy
+import numpy as np
+import torch
 
+from .utils import to_numpy
 
 
-def threshold_targets(metric, threshold=.5, **defaults):
+def threshold_targets(metric, threshold=0.5, **defaults):
   def m(preds, targets, **kwargs):
     return metric(preds, targets > threshold, **defaults, **kwargs)
+
   return m
 
 
@@ -36,14 +36,14 @@ def top_k_accuracy(targets, preds, k=1):
     _, targets = torch.max(targets, dim=1)
   scores, preds = preds.topk(k, 1, True, True)
   preds = preds.t()
-  correct = (preds == targets.view(1, -1).expand_as(preds))
+  correct = preds == targets.view(1, -1).expand_as(preds)
 
   return correct.sum().float() / len(targets)
 
 
-def mAP(targs, preds, pos_thresh=.5):
-  preds = preds.to('cpu').numpy()
-  targs = (targs.to('cpu') > pos_thresh).float().numpy()
+def mAP(targs, preds, pos_thresh=0.5):
+  preds = preds.to("cpu").numpy()
+  targs = (targs.to("cpu") > pos_thresh).float().numpy()
   if np.size(preds) == 0:
     return 0
   ap = np.zeros((preds.shape[1]))
@@ -76,10 +76,12 @@ def average_precision(output, target):
 
   return precision_at_i
 
+
 top_3_accuracy = partial(top_k_accuracy, k=3)
 top_5_accuracy = partial(top_k_accuracy, k=5)
 top_10_accuracy = partial(top_k_accuracy, k=10)
 
+
 def precision_at_k(targets, outputs, k=5):
   scores, top_preds = top_k(outputs, k=k)
   raise NotImplementedError()
@@ -109,14 +111,57 @@ def label_ranking_average_precision(targets, preds, target_threshold=0):
   targets, preds = to_numpy(targets), to_numpy(preds)
   if target_threshold is not None:
     targets = targets > target_threshold
-  return metrics.label_ranking_average_precision_score(targets, preds)
+
+  # Inlined NumPy implementation
+  n_samples, n_labels = targets.shape
+  scores = np.zeros(n_samples)
+
+  for i in range(n_samples):
+    true_indices = np.flatnonzero(targets[i])
+    if len(true_indices) == 0:
+      continue
+
+    pred_ranking = np.argsort(preds[i])[::-1]
+    ranks = np.empty_like(pred_ranking)
+    ranks[pred_ranking] = np.arange(1, n_labels + 1)  # 1-based rank
+
+    true_ranks = ranks[true_indices]
+    relevant_ranks_sorted = np.sort(true_ranks)
+
+    precisions = np.arange(1, len(true_indices) + 1) / relevant_ranks_sorted
+    scores[i] = np.mean(precisions)
+
+  return np.mean(scores)
 
 
 def coverage_error(targets, preds, target_threshold=0):
   targets, preds = to_numpy(targets), to_numpy(preds)
   if target_threshold is not None:
     targets = targets > target_threshold
-  return metrics.coverage_error(targets, preds)
+
+  # Inlined NumPy implementation
+  n_samples, n_labels = targets.shape
+  max_ranks = np.zeros(n_samples)
+
+  for i in range(n_samples):
+    true_indices = np.flatnonzero(targets[i])
+    if len(true_indices) == 0:
+      # Assign 0 coverage error if no true labels, consistent with sklearn
+      max_ranks[i] = 0
+      continue
+
+    pred_ranking = np.argsort(preds[i])[::-1]
+    ranks = np.empty_like(pred_ranking)
+    ranks[pred_ranking] = np.arange(1, n_labels + 1)  # 1-based rank
+
+    true_ranks = ranks[true_indices]
+    max_ranks[i] = np.max(true_ranks)
+
+  # sklearn definition: average over samples of (max_rank - 1) / n_labels
+  # Let's match that definition for consistency if users expect it
+  # The definition actually seems to be just mean(max_rank) / n_labels for scikit-learn.
+  # Let's stick to that.
+  return np.mean(max_ranks) / n_labels if n_labels > 0 else 0
 
 
 def average_precision_at_k(targets, preds, k=5):
@@ -127,31 +172,26 @@ def mean_average_precision_at_k(targets, preds, k=5):
   raise NotImplementedError()
 
 
-def evaluate_multiclass(
-    targets,
-    outputs,
-    preds=None,
-    classes=None
-):
+def evaluate_multiclass(targets, outputs, preds=None, classes=None):
   preds = preds or get_preds(outputs)
   targets, outputs, preds = (
     to_numpy(targets),
     to_numpy(outputs),
-    to_numpy(preds)
+    to_numpy(preds),
   )
   raise NotImplementedError()
 
 
 def evaluate_multilabel(
-    targets,
-    outputs,
-    preds=None,
-    classes=None,
+  targets,
+  outputs,
+  preds=None,
+  classes=None,
 ):
   targets, outputs, preds = (
     to_numpy(targets),
     to_numpy(outputs),
-    to_numpy(preds)
+    to_numpy(preds),
   )
   raise NotImplementedError()
 
@@ -205,16 +245,17 @@ class WindowMeter:
 
   @property
   def average(self):
-    if not self.values: return None
+    if not self.values:
+      return None
     return sum(self.values) / len(self.values)
 
 
-def exp_moving_avg(cur, prev=None, alpha=.05, steps=None):
+def exp_moving_avg(cur, prev=None, alpha=0.05, steps=None):
   """exponential moving average"""
   if prev is None:
     return cur
   avg = alpha * cur + prev * (1 - alpha)
-  return avg / (1 - alpha ** steps) if steps else avg
+  return avg / (1 - alpha**steps) if steps else avg
 
 
 def moving_average(data, window=10):
diff --git a/yann/models/classifier.py b/yann/models/classifier.py
index a824369..ba38004 100644
--- a/yann/models/classifier.py
+++ b/yann/models/classifier.py
@@ -17,15 +17,9 @@ class Classifier(nn.Module):
     raise NotImplementedError()
 
 
-
 class LinearClassifier(Classifier):
   def __init__(
-      self,
-      in_features,
-      classes,
-      bias=True,
-      activation=None,
-      test_activation=None
+    self, in_features, classes, bias=True, activation=None, test_activation=None
   ):
     super(Classifier, self).__init__()
 
@@ -33,4 +27,4 @@ class LinearClassifier(Classifier):
     self.classes = classes
 
     self.activation = activation
-    self.test_activation = test_activation or activation
\ No newline at end of file
+    self.test_activation = test_activation or activation
diff --git a/yann/models/model.py b/yann/models/model.py
index c509423..768962b 100644
--- a/yann/models/model.py
+++ b/yann/models/model.py
@@ -7,10 +7,7 @@ class ModelMixin:
   def __init__(self, *args, **kwargs):
     # Need to store this for when we save the model,
     # so that we can restore it later without knowing them
-    self._init_args = {
-      'args': args,
-      'kwargs': kwargs
-    }
+    self._init_args = {"args": args, "kwargs": kwargs}
     super().__init__(*args, **kwargs)
 
   def predict(self, inputs) -> Outputs:
@@ -20,7 +17,7 @@ class ModelMixin:
     return None
 
   @classmethod
-  def load(cls, *args, **kwargs) -> 'Model':
+  def load(cls, *args, **kwargs) -> "Model":
     return cls(*args, **kwargs)
 
 
diff --git a/yann/models/vision/base.py b/yann/models/vision/base.py
index 72aa1cc..b9d7811 100644
--- a/yann/models/vision/base.py
+++ b/yann/models/vision/base.py
@@ -1,8 +1,8 @@
+from typing import Callable
+
 from torch import nn
 
 from ...data import Classes
-from typing import Callable
-
 
 
 class CNN(nn.Module):
@@ -17,14 +17,12 @@ class RecognitionModel(nn.Module):
   input_shape = (None, None, None, None)
   classes: Classes
 
-
   backbone: Callable
   # "Global" pooling layer that converts features into an embedding
   pool_features: Callable
 
   classifier: Callable
 
-
   def get_features(self, inputs):
     raise NotImplementedError()
 
@@ -44,4 +42,4 @@ class RecognitionModel(nn.Module):
     pass
 
   def replace_classifier(self, linear=None, num_classes=None, init=None):
-    raise NotImplementedError()
\ No newline at end of file
+    raise NotImplementedError()
diff --git a/yann/models/vision/vgg.py b/yann/models/vision/vgg.py
index 44b51ff..63edd9c 100644
--- a/yann/models/vision/vgg.py
+++ b/yann/models/vision/vgg.py
@@ -1,5 +1,6 @@
 from torch import nn
-from yann.modules import Stack, Flatten
+
+from yann.modules import Flatten, Stack
 
 
 class VGG(nn.Module):
@@ -12,14 +13,7 @@ class VGG(nn.Module):
 
   in_channels = 3
 
-  channels = [
-    (64,),
-    (128,),
-    (256,),
-    (512,),
-    (512,)
-
-  ]
+  channels = [(64,), (128,), (256,), (512,), (512,)]
 
   def __init__(self, num_classes):
     super(VGG, self).__init__()
@@ -27,8 +21,7 @@ class VGG(nn.Module):
     self.backbone = self.build_backbone()
 
     self.activations_to_features = Stack(
-      pool=self.Reduce(kernel_size=1, stride=1),
-      flatten=Flatten()
+      pool=self.Reduce(kernel_size=1, stride=1), flatten=Flatten()
     )
 
     self.project = nn.Linear(self.channels[-1][-1], self.num_classes)
@@ -49,7 +42,7 @@ class VGG(nn.Module):
           Stack(
             conv=self.Conv(prev_channels, c, kernel_size=3, padding=1),
             norm=self.Norm(c),
-            activation=self.Activation(inplace=True)
+            activation=self.Activation(inplace=True),
           )
         )
         prev_channels = c
@@ -95,4 +88,3 @@ class VGG19(VGG):
     (512,) * 4,
     (512,) * 4,
   ]
-
diff --git a/yann/modules/__init__.py b/yann/modules/__init__.py
index ab95d01..6c323d4 100644
--- a/yann/modules/__init__.py
+++ b/yann/modules/__init__.py
@@ -4,15 +4,24 @@ from .conv import (
   ConvBlock,
   ConvBlock1x1,
   ConvBlock3x3,
+  DepthwiseConv2d,
+  DepthwiseSeparableConv2d,
+  EfficientChannelAttention,
   MixConv,
   SqueezeExcitation,
-  EfficientChannelAttention,
-  DepthwiseSeparableConv2d,
-  DepthwiseConv2d
 )
-from .shape import View, Flatten, Infer, Squeeze, Reshape, Permute, Transpose, FlattenSequences
-from .stack import Stack
 from .residual import Residual
+from .shape import (
+  Flatten,
+  FlattenSequences,
+  Infer,
+  Permute,
+  Reshape,
+  Squeeze,
+  Transpose,
+  View,
+)
+from .stack import Stack
 
 
 class Init:
@@ -25,7 +34,6 @@ class Init:
     return self.cls(*self.args, *args, **{**self.kwargs, **kwargs})
 
 
-
 class TrainEvalSwitch(nn.Module):
   def __init__(self, train=None, eval=None):
     super().__init__()
diff --git a/yann/modules/conv/__init__.py b/yann/modules/conv/__init__.py
index 176ba1f..47d8c1c 100644
--- a/yann/modules/conv/__init__.py
+++ b/yann/modules/conv/__init__.py
@@ -1,5 +1,5 @@
-from .mixconv import MixConv
+from .attention import EfficientChannelAttention
 from .conv import ConvBlock, ConvBlock1x1, ConvBlock3x3
 from .depthwise import DepthwiseConv2d, DepthwiseSeparableConv2d
+from .mixconv import MixConv
 from .squeeze_excitation import SqueezeExcitation
-from .attention import EfficientChannelAttention
\ No newline at end of file
diff --git a/yann/modules/conv/attention.py b/yann/modules/conv/attention.py
index ae0ce6e..1a1713d 100644
--- a/yann/modules/conv/attention.py
+++ b/yann/modules/conv/attention.py
@@ -5,22 +5,23 @@ class EfficientChannelAttention(nn.Module):
   """
   https://github.com/BangguWu/ECANet
   """
+
   def __init__(self, kernel_size=3):
     super().__init__()
     self.pool = nn.AdaptiveAvgPool2d(1)
     self.conv = nn.Conv1d(
-      1, 1,
-      kernel_size=kernel_size,
-      padding=(kernel_size - 1) // 2,
-      bias=False
+      1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False
     )
     self.sigmoid = nn.Sigmoid()
 
   def forward(self, input):
     x = self.pool(input)
-    x = self.conv(x.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
+    x = (
+      self.conv(x.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
+    )
     x = self.sigmoid(x)
 
     return input * x.expand_as(input)
 
+
 ECA = EfficientChannelAttention
diff --git a/yann/modules/conv/conv.py b/yann/modules/conv/conv.py
index 4013c23..330654a 100644
--- a/yann/modules/conv/conv.py
+++ b/yann/modules/conv/conv.py
@@ -1,9 +1,9 @@
 from torch import nn
+
 from ..stack import Stack
 
 
 class ConvBlock(Stack):
-
   class default:
     conv = nn.Conv2d
     norm = nn.BatchNorm2d
@@ -19,12 +19,12 @@ class ConvBlock(Stack):
     dilation=1,
     groups=1,
     bias=None,
-    padding_mode='zeros',
+    padding_mode="zeros",
     conv=True,
     norm=None,
     activation=True,
-    order=('conv', 'norm', 'activation'),
-    **extra
+    order=("conv", "norm", "activation"),
+    **extra,
   ):
     if conv is True:
       if bias is None:
@@ -39,38 +39,43 @@ class ConvBlock(Stack):
         padding=padding,
         dilation=dilation,
         groups=groups,
-        padding_mode=padding_mode
+        padding_mode=padding_mode,
       )
     if norm is True:
       if conv is not None:
         norm = self.default.norm(
           num_features=conv.in_channels
-          if order.index('conv') > order.index('norm') else conv.out_channels
+          if order.index("conv") > order.index("norm")
+          else conv.out_channels
         )
       else:
         norm = self.default.norm(in_channels)
 
     if activation is True:
-      activation = self.default.activation() if isinstance(
-        self.default.activation, type
-      ) else self.default.activation
+      activation = (
+        self.default.activation()
+        if isinstance(self.default.activation, type)
+        else self.default.activation
+      )
 
     d = dict(**extra, conv=conv, norm=norm, activation=activation)
     # NOTE: this depends on dict ordering
     super(ConvBlock, self).__init__(**{k: d[k] for k in order})
 
 
-
-
 class ConvBlock1x1(ConvBlock):
   def __init__(self, **kwargs):
-    if kwargs.get('kernel_size', 1) != 1:
-      raise ValueError(f'kernel_size must be `1` if provided as an argument, got kernel_size={kwargs["kernel_size"]}')
+    if kwargs.get("kernel_size", 1) != 1:
+      raise ValueError(
+        f"kernel_size must be `1` if provided as an argument, got kernel_size={kwargs['kernel_size']}"
+      )
     super(ConvBlock1x1, self).__init__(kernel_size=1, padding=0, **kwargs)
 
 
 class ConvBlock3x3(ConvBlock):
   def __init__(self, **kwargs):
-    if kwargs.get('kernel_size', 3) != 3:
-      raise ValueError(f'kernel_size must be `3` if provided as an argument, got kernel_size={kwargs["kernel_size"]}')
-    super(ConvBlock3x3, self).__init__(kernel_size=3, padding=1, **kwargs)
\ No newline at end of file
+    if kwargs.get("kernel_size", 3) != 3:
+      raise ValueError(
+        f"kernel_size must be `3` if provided as an argument, got kernel_size={kwargs['kernel_size']}"
+      )
+    super(ConvBlock3x3, self).__init__(kernel_size=3, padding=1, **kwargs)
diff --git a/yann/modules/conv/depthwise.py b/yann/modules/conv/depthwise.py
index 938704b..253eeb0 100644
--- a/yann/modules/conv/depthwise.py
+++ b/yann/modules/conv/depthwise.py
@@ -2,11 +2,19 @@ from torch import nn
 
 
 class DepthwiseConv2d(nn.Conv2d):
-  def __init__(self, in_channels, out_channels, kernel_size, stride=1,
-                 padding=0, dilation=1,
-                 bias=True, padding_mode='zeros'):
+  def __init__(
+    self,
+    in_channels,
+    out_channels,
+    kernel_size,
+    stride=1,
+    padding=0,
+    dilation=1,
+    bias=True,
+    padding_mode="zeros",
+  ):
     if out_channels % in_channels:
-      raise ValueError('out_channels must be a multiple of in_channels')
+      raise ValueError("out_channels must be a multiple of in_channels")
     super(DepthwiseConv2d, self).__init__(
       in_channels=in_channels,
       out_channels=out_channels,
@@ -16,27 +24,37 @@ class DepthwiseConv2d(nn.Conv2d):
       dilation=dilation,
       bias=bias,
       padding_mode=padding_mode,
-      groups=in_channels
+      groups=in_channels,
     )
 
 
 class DepthwiseSeparableConv2d(nn.Module):
-  def __init__(self, in_channels, out_channels, kernel_size, stride=1,
-                 padding=0, dilation=1,
-                 bias=True, padding_mode='zeros'):
+  def __init__(
+    self,
+    in_channels,
+    out_channels,
+    kernel_size,
+    stride=1,
+    padding=0,
+    dilation=1,
+    bias=True,
+    padding_mode="zeros",
+  ):
     super(DepthwiseSeparableConv2d, self).__init__()
 
-    self.depthwise = DepthwiseConv2d(in_channels=in_channels,
+    self.depthwise = DepthwiseConv2d(
+      in_channels=in_channels,
       out_channels=out_channels,
       kernel_size=kernel_size,
       stride=stride,
       padding=padding,
       dilation=dilation,
       bias=bias,
-      padding_mode=padding_mode)
+      padding_mode=padding_mode,
+    )
 
     self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)
 
   def forward(self, input):
     x = self.depthwise(input)
-    return self.pointwise(x)
\ No newline at end of file
+    return self.pointwise(x)
diff --git a/yann/modules/conv/mixconv.py b/yann/modules/conv/mixconv.py
index 18f781d..6d5f6bc 100644
--- a/yann/modules/conv/mixconv.py
+++ b/yann/modules/conv/mixconv.py
@@ -1,5 +1,5 @@
-from torch import nn
 import torch
+from torch import nn
 
 from .utils import get_same_padding
 
@@ -17,12 +17,9 @@ class MixConv(nn.Module):
   so kernel_size = 7 might lead to crashes on CPUs (seeing this on a Mac) 
   https://github.com/pytorch/pytorch/issues/20583
   """
+
   def __init__(
-      self,
-      in_channels,
-      out_channels,
-      kernel_size=None,
-      depthwise=True
+    self, in_channels, out_channels, kernel_size=None, depthwise=True
   ):
     super(MixConv, self).__init__()
 
@@ -32,35 +29,46 @@ class MixConv(nn.Module):
       self.kernel_sizes = list(kernel_size)
     elif kernel_size is None:
       if not isinstance(in_channels, (list, tuple)):
-        raise ValueError('kernel_size must be provided if in_channels is not an iterable')
+        raise ValueError(
+          "kernel_size must be provided if in_channels is not an iterable"
+        )
       self.kernel_sizes = [3 + 2 * n for n in range(len(in_channels))]
 
     if isinstance(in_channels, (list, tuple)):
       self.input_channel_counts = in_channels
     else:
-      self.input_channel_counts = self.split_groups(in_channels, len(self.kernel_sizes))
+      self.input_channel_counts = self.split_groups(
+        in_channels, len(self.kernel_sizes)
+      )
     if isinstance(out_channels, (list, tuple)):
       self.output_channel_counts = out_channels
     else:
-      self.output_channel_counts = self.split_groups(out_channels, len(self.kernel_sizes))
+      self.output_channel_counts = self.split_groups(
+        out_channels, len(self.kernel_sizes)
+      )
 
     if len(self.input_channel_counts) != len(self.output_channel_counts):
       raise ValueError(
-        f'in_channels and out_channels should have same number of groups,'
-        f' but got {len(self.input_channel_counts)} and {len(self.output_channel_counts)}'
+        f"in_channels and out_channels should have same number of groups,"
+        f" but got {len(self.input_channel_counts)} and {len(self.output_channel_counts)}"
       )
 
-    self.convs = nn.ModuleList([
-      nn.Conv2d(
-        in_channels=ic,
-        out_channels=oc,
-        kernel_size=ks,
-        groups=min(ic, oc) if depthwise else 1,
-        padding=get_same_padding(ks)
-      )
-      for ic, oc, ks
-      in zip(self.input_channel_counts, self.output_channel_counts, self.kernel_sizes)
-    ])
+    self.convs = nn.ModuleList(
+      [
+        nn.Conv2d(
+          in_channels=ic,
+          out_channels=oc,
+          kernel_size=ks,
+          groups=min(ic, oc) if depthwise else 1,
+          padding=get_same_padding(ks),
+        )
+        for ic, oc, ks in zip(
+          self.input_channel_counts,
+          self.output_channel_counts,
+          self.kernel_sizes,
+        )
+      ]
+    )
 
   def __repr__(self):
     return f"MixConv({self.input_channel_counts}, {self.output_channel_counts}, kernel_sizes={self.kernel_sizes}, convs={self.convs})"
@@ -76,5 +84,3 @@ class MixConv(nn.Module):
     parts = torch.split(input, self.input_channel_counts, 1)
     outputs = [conv(part) for conv, part in zip(self.convs, parts)]
     return torch.cat(outputs, 1)
-
-
diff --git a/yann/modules/conv/squeeze_excitation.py b/yann/modules/conv/squeeze_excitation.py
index 4af3db1..bc70183 100644
--- a/yann/modules/conv/squeeze_excitation.py
+++ b/yann/modules/conv/squeeze_excitation.py
@@ -1,10 +1,10 @@
 from torch import nn
-from ..stack import Stack
+
 from ..residual import Residual
+from ..stack import Stack
 
 
 class SqueezeExcitation(Residual):
-
   def __init__(self, channels: int, reduction: int):
     """
     Args:
@@ -18,9 +18,9 @@ class SqueezeExcitation(Residual):
         nn.Conv2d(channels, inner_channels, kernel_size=1, padding=0),
         nn.ReLU(inplace=True),
         nn.Conv2d(inner_channels, channels, kernel_size=1, padding=0),
-        nn.Sigmoid()
+        nn.Sigmoid(),
       )
     )
 
 
-SE = SqueezeExcitation
\ No newline at end of file
+SE = SqueezeExcitation
diff --git a/yann/modules/conv/utils.py b/yann/modules/conv/utils.py
index 5743d24..dd4de49 100644
--- a/yann/modules/conv/utils.py
+++ b/yann/modules/conv/utils.py
@@ -1,33 +1,38 @@
 import math
 
 
-def get_tf_same_padding(
-    tensor,
-    kernel_size,
-    stride=1,
-    dilation=1
-):
+def get_tf_same_padding(tensor, kernel_size, stride=1, dilation=1):
   if isinstance(kernel_size, int):
     return tuple(
-      *tf_same_pad(tensor.shape[2], kernel_size, stride=stride, dilation=dilation),
-      *tf_same_pad(tensor.shape[3], kernel_size, stride=stride, dilation=dilation)
+      *tf_same_pad(
+        tensor.shape[2], kernel_size, stride=stride, dilation=dilation
+      ),
+      *tf_same_pad(
+        tensor.shape[3], kernel_size, stride=stride, dilation=dilation
+      ),
     )
   else:
     return tuple(
-      *tf_same_pad(tensor.shape[2], kernel_size[0], stride=stride, dilation=dilation),
-      *tf_same_pad(tensor.shape[3], kernel_size[1], stride=stride, dilation=dilation)
+      *tf_same_pad(
+        tensor.shape[2], kernel_size[0], stride=stride, dilation=dilation
+      ),
+      *tf_same_pad(
+        tensor.shape[3], kernel_size[1], stride=stride, dilation=dilation
+      ),
     )
 
+
 def tf_same_pad(size, kernel_size, stride=1, dilation=1):
   pad = max(
     0,
     (math.ceil(size / stride) - 1) * stride
-      + (kernel_size - 1) * dilation
-      + 1 - size
+    + (kernel_size - 1) * dilation
+    + 1
+    - size,
   )
   left_pad = pad // 2
   return left_pad, pad - left_pad
 
 
 def get_same_padding(kernel_size, stride=1, dilation=1):
-  return ((stride - 1) + dilation * (kernel_size - 1)) // 2
\ No newline at end of file
+  return ((stride - 1) + dilation * (kernel_size - 1)) // 2
diff --git a/yann/modules/heads/ml_decoder.py b/yann/modules/heads/ml_decoder.py
index fc34f82..9351833 100644
--- a/yann/modules/heads/ml_decoder.py
+++ b/yann/modules/heads/ml_decoder.py
@@ -1,23 +1,22 @@
 import torch
 from torch import nn
 
-from yann.modules import Stack, Residual
-
+from yann.modules import Residual, Stack
 
 
 class MLTransformerDecoderLayer(Stack):
   def __init__(
-      self,
-      embed_dim,
-      num_heads=8,
-      feedforward_dim=2048,
-      dropout=0.1,
-      layer_norm_eps=1e-5,
+    self,
+    embed_dim,
+    num_heads=8,
+    feedforward_dim=2048,
+    dropout=0.1,
+    layer_norm_eps=1e-5,
   ):
     super().__init__(
       norm1=Stack(
         Residual(nn.Dropout(dropout)),
-        nn.LayerNorm(embed_dim, eps=layer_norm_eps)
+        nn.LayerNorm(embed_dim, eps=layer_norm_eps),
       ),
       attention=nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout),
       norm2=Stack(
@@ -28,12 +27,12 @@ class MLTransformerDecoderLayer(Stack):
         nn.Linear(embed_dim, feedforward_dim),
         nn.ReLU(),
         nn.Dropout(dropout),
-        nn.Linear(feedforward_dim, embed_dim)
+        nn.Linear(feedforward_dim, embed_dim),
       ),
       norm3=Stack(
         Residual(nn.Dropout(dropout)),
         nn.LayerNorm(embed_dim, eps=layer_norm_eps),
-      )
+      ),
     )
 
   def forward(self, x, memory, **kwargs):
@@ -44,22 +43,21 @@ class MLTransformerDecoderLayer(Stack):
     return self.norm3(x)
 
 
-
 @torch.jit.script
 class GroupFC(object):
   def __init__(self, embed_len_decoder: int):
     self.embed_len_decoder = embed_len_decoder
 
   def __call__(
-      self,
-      h: torch.Tensor,
-      duplicate_pooling: torch.Tensor,
-      out_extrap: torch.Tensor
+    self,
+    h: torch.Tensor,
+    duplicate_pooling: torch.Tensor,
+    out_extrap: torch.Tensor,
   ):
-      for i in range(self.embed_len_decoder):
-          h_i = h[:, i, :]
-          w_i = duplicate_pooling[i, :, :]
-          out_extrap[:, i, :] = torch.matmul(h_i, w_i)
+    for i in range(self.embed_len_decoder):
+      h_i = h[:, i, :]
+      w_i = duplicate_pooling[i, :, :]
+      out_extrap[:, i, :] = torch.matmul(h_i, w_i)
 
 
 class GroupFullyConnectedPooling(nn.Module):
@@ -81,26 +79,29 @@ class GroupFullyConnectedPooling(nn.Module):
     torch.nn.init.constant_(self.bias, 0)
 
   def forward(self, h):
-    out_extrap = torch.zeros(h.shape[0], h.shape[1], self.duplicate_factor,
+    out_extrap = torch.zeros(
+      h.shape[0],
+      h.shape[1],
+      self.duplicate_factor,
       device=h.device,
-      dtype=h.dtype
+      dtype=h.dtype,
     )
     self.group_fc(h, self.duplicate_pooling, out_extrap)
-    logits = out_extrap.flatten(1)[:, :self.num_classes]
+    logits = out_extrap.flatten(1)[:, : self.num_classes]
     logits += self.bias
     return logits
 
 
 class MLDecoder(nn.Module):
   def __init__(
-      self,
-      num_classes,
-      num_groups=None,
-      decoder_embed_dim=768,
-      initial_num_features=2048,
-      feedforward_dim=1024,
-      num_heads=8,
-      dropout=0.1
+    self,
+    num_classes,
+    num_groups=None,
+    decoder_embed_dim=768,
+    initial_num_features=2048,
+    feedforward_dim=1024,
+    num_heads=8,
+    dropout=0.1,
   ):
     super().__init__()
     num_groups = num_groups or num_classes
@@ -115,8 +116,7 @@ class MLDecoder(nn.Module):
     self.query_embed.requires_grad_(False)
 
     self.embed_input = Stack(
-      nn.Linear(initial_num_features, decoder_embed_dim),
-      nn.ReLU(inplace=True)
+      nn.Linear(initial_num_features, decoder_embed_dim), nn.ReLU(inplace=True)
     )
 
     self.decoder = nn.TransformerDecoder(
@@ -124,15 +124,15 @@ class MLDecoder(nn.Module):
         embed_dim=decoder_embed_dim,
         feedforward_dim=feedforward_dim,
         num_heads=num_heads,
-        dropout=dropout
+        dropout=dropout,
       ),
-      num_layers=1
+      num_layers=1,
     )
 
     self.group_pooling = GroupFullyConnectedPooling(
       num_classes=num_classes,
       embed_len_decoder=num_groups,
-      decoder_embedding=decoder_embed_dim
+      decoder_embedding=decoder_embed_dim,
     )
 
   def forward(self, x: torch.Tensor):
@@ -142,8 +142,9 @@ class MLDecoder(nn.Module):
     x = self.embed_input(x)
     # no allocation of memory with expand
     target = self.query_embed.weight.unsqueeze(1).expand(-1, x.shape[0], -1)
-    h = self.decoder(target, x.transpose(0, 1))  # [embed_len_decoder, batch, 768]
+    h = self.decoder(
+      target, x.transpose(0, 1)
+    )  # [embed_len_decoder, batch, 768]
     h = h.transpose(0, 1)
 
     return self.group_pooling(h)
-
diff --git a/yann/modules/loss/__init__.py b/yann/modules/loss/__init__.py
index 0fae1f5..b95d815 100644
--- a/yann/modules/loss/__init__.py
+++ b/yann/modules/loss/__init__.py
@@ -1,7 +1,7 @@
 import torch
 import torch.nn.functional as F
-from torch.nn.modules.loss import _Loss, _WeightedLoss
 from torch import nn
+from torch.nn.modules.loss import _Loss, _WeightedLoss
 
 from yann.data.classes import smooth as label_smoothing
 
@@ -9,25 +9,21 @@ from yann.data.classes import smooth as label_smoothing
 def _reduce(x, reduce=True, reduction=None):
   if not reduce:
     return x
-  if reduction == 'mean' or reduction == 'elementwise_mean':
+  if reduction == "mean" or reduction == "elementwise_mean":
     return x.mean()
-  if reduction == 'sum':
+  if reduction == "sum":
     return x.sum()
-  if reduction == 'batch_mean':
+  if reduction == "batch_mean":
     return x.sum() / x.shape[0]
-  if reduction is None or reduction == 'none':
+  if reduction is None or reduction == "none":
     return x
-  raise ValueError(f'Unsupported reduction')
+  raise ValueError(f"Unsupported reduction")
 
 
 def soft_target_cross_entropy(
-    inputs,
-    targets,
-    smooth=None,
-    reduce=True,
-    dim=1,
-    reduction='mean'):
-  """"like cross_entropy but using soft targets"""
+  inputs, targets, smooth=None, reduce=True, dim=1, reduction="mean"
+):
+  """ "like cross_entropy but using soft targets"""
   if smooth:
     targets = label_smoothing(targets, smooth)
 
@@ -35,9 +31,8 @@ def soft_target_cross_entropy(
   return _reduce(vals, reduce=reduce, reduction=reduction)
 
 
-
 class SoftTargetCrossEntropyLoss(_Loss):
-  def __init__(self, smooth=None, reduce=True, dim=1, reduction='mean'):
+  def __init__(self, smooth=None, reduce=True, dim=1, reduction="mean"):
     super().__init__(reduce=reduce, reduction=reduction)
     self.reduce = reduce
     self.reduction = reduction
@@ -51,11 +46,19 @@ class SoftTargetCrossEntropyLoss(_Loss):
       smooth=self.smooth,
       reduce=self.reduce,
       dim=self.dim,
-      reduction=self.reduction)
-
+      reduction=self.reduction,
+    )
 
 
-def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, reduce=True, reduction='mean'):
+def binary_focal_loss(
+  logits,
+  targets,
+  gamma=2,
+  alpha=None,
+  pos_weight=None,
+  reduce=True,
+  reduction="mean",
+):
   """
   Binary focal loss (with sigmoids)
 
@@ -93,7 +96,9 @@ def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, red
   # TODO try this numerically stable version https://github.com/richardaecn/class-balanced-loss/issues/1
 
   probs = torch.sigmoid(logits)
-  bce = F.binary_cross_entropy_with_logits(logits, targets, pos_weight=pos_weight, reduction='none')
+  bce = F.binary_cross_entropy_with_logits(
+    logits, targets, pos_weight=pos_weight, reduction="none"
+  )
 
   pt = targets * probs + (1 - targets) * (1 - probs)
   modulate = 1 if gamma is None else (1 - pt) ** gamma
@@ -109,7 +114,9 @@ def binary_focal_loss(logits, targets, gamma=2, alpha=None, pos_weight=None, red
 
 
 class BinaryFocalLoss(_Loss):
-  def __init__(self, gamma=2, alpha=None, pos_weight=None, reduce=True, reduction='mean'):
+  def __init__(
+    self, gamma=2, alpha=None, pos_weight=None, reduce=True, reduction="mean"
+  ):
     super(BinaryFocalLoss, self).__init__()
 
     self.gamma = gamma
@@ -125,14 +132,15 @@ class BinaryFocalLoss(_Loss):
       gamma=self.gamma,
       alpha=self.alpha,
       pos_weight=self.pos_weight,
-      reduction=self.reduction
+      reduction=self.reduction,
     )
 
+
 class ClassWeighted(_Loss):
-  def __init__(self, loss, weights=None, reduce=True, reduction='mean'):
+  def __init__(self, loss, weights=None, reduce=True, reduction="mean"):
     super(ClassWeighted, self).__init__(reduce=reduce, reduction=reduction)
-    self.loss = loss(reduction='none') if issubclass(loss, _Loss) else loss
-    self.register_buffer('weights', weights)
+    self.loss = loss(reduction="none") if issubclass(loss, _Loss) else loss
+    self.register_buffer("weights", weights)
     self.reduce = reduce
     self.reduction = reduction
 
@@ -151,15 +159,15 @@ def triplet_loss():
   pass
 
 
-
-
 def tempered_log(x, temperature=1):
-  if temperature == 1: return torch.log(x)
+  if temperature == 1:
+    return torch.log(x)
   return (x ** (1 - temperature) - 1) / (1 - temperature)
 
 
 def tempered_exp(x, temperature=1):
-  if temperature == 1: return torch.exp(x)
+  if temperature == 1:
+    return torch.exp(x)
   return torch.relu(1 + (1 - temperature) * x) ** (1 / (1 - temperature))
 
 
@@ -175,7 +183,6 @@ def bi_tempered_binary_logistic_loss():
   pass
 
 
-
 class WeightedLoss(_WeightedLoss):
   def __init__(self, loss, weight=1, **kwargs):
     super(WeightedLoss, self).__init__(weight=weight, **kwargs)
@@ -190,7 +197,7 @@ class CombinedLoss(_Loss):
   def __init__(self, losses, weights, *args, **kwargs):
     super(CombinedLoss, self).__init__(*args, **kwargs)
     self.losses = nn.ModuleList(
-      WeightedLoss(loss, weight, reduction='none')
+      WeightedLoss(loss, weight, reduction="none")
       for loss, weight in zip(losses, weights)
     )
 
@@ -206,12 +213,14 @@ MultiTaskLoss = CombinedLoss
 
 
 class KeepK(_Loss):
-  def __init__(self, loss, top=None, bottom=None, reduce=True, reduction='mean', **kwargs):
+  def __init__(
+    self, loss, top=None, bottom=None, reduce=True, reduction="mean", **kwargs
+  ):
     super().__init__(reduce=reduce, reduction=reduction, **kwargs)
     self.loss = loss
-    if hasattr(self.loss, 'reduction'):
-      self.loss.reduction = 'none'
-    if hasattr(self.loss, 'reduce'):
+    if hasattr(self.loss, "reduction"):
+      self.loss.reduction = "none"
+    if hasattr(self.loss, "reduce"):
       self.loss.reduce = False
 
     if top and bottom:
@@ -236,14 +245,15 @@ class KeepK(_Loss):
     return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
 
 
-
 class KeepRange(_Loss):
-  def __init__(self, loss, min=None, max=None, reduce=True, reduction='mean', **kwargs):
+  def __init__(
+    self, loss, min=None, max=None, reduce=True, reduction="mean", **kwargs
+  ):
     super().__init__(reduce=reduce, reduction=reduction, **kwargs)
     self.loss = loss
-    if hasattr(self.loss, 'reduction'):
-      self.loss.reduction = 'none'
-    if hasattr(self.loss, 'reduce'):
+    if hasattr(self.loss, "reduction"):
+      self.loss.reduction = "none"
+    if hasattr(self.loss, "reduce"):
       self.loss.reduce = False
 
     self.min = min
@@ -264,4 +274,4 @@ class KeepRange(_Loss):
     if mask is not None:
       losses = losses[mask]
 
-    return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
\ No newline at end of file
+    return _reduce(losses, reduction=self.reduction, reduce=self.reduce)
diff --git a/yann/modules/loss/asymmetric.py b/yann/modules/loss/asymmetric.py
index c372703..fbfe5b7 100644
--- a/yann/modules/loss/asymmetric.py
+++ b/yann/modules/loss/asymmetric.py
@@ -1,8 +1,8 @@
-from torch import nn
 import torch
-from yann.typedefs import Logits, MultiLabelOneHot
+from torch import nn
 
 from yann.modules.loss import _reduce
+from yann.typedefs import Logits, MultiLabelOneHot
 
 
 class AsymmetricLoss(nn.Module):
@@ -25,7 +25,9 @@ class AsymmetricLoss(nn.Module):
     https://github.com/Alibaba-MIIL/ASL
   """
 
-  def __init__(self, neg_decay=4, pos_decay=1, prob_shift=0.05, eps=1e-8, reduction='mean'):
+  def __init__(
+    self, neg_decay=4, pos_decay=1, prob_shift=0.05, eps=1e-8, reduction="mean"
+  ):
     super(AsymmetricLoss, self).__init__()
     self.pos_decay = pos_decay
     self.neg_decay = neg_decay
@@ -36,7 +38,6 @@ class AsymmetricLoss(nn.Module):
     self.reduction = reduction
     self.calculate_focal_loss_gradients = True
 
-
   def forward(self, inputs: Logits, targets: MultiLabelOneHot, reduction=None):
     pos_probs = torch.sigmoid(inputs)
     neg_probs = 1 - pos_probs
@@ -47,7 +48,7 @@ class AsymmetricLoss(nn.Module):
       # paper claims this also helps handling mislabeled negative examples
       neg_probs = (neg_probs + self.prob_shift).clamp(max=1)
 
-    neg_targets = (1 - targets)
+    neg_targets = 1 - targets
 
     pos_losses = targets * torch.log(pos_probs.clamp(min=self.eps))
     neg_losses = neg_targets * torch.log(neg_probs.clamp(min=self.eps))
@@ -66,7 +67,7 @@ class AsymmetricLoss(nn.Module):
 class AsymmetricLossOptimized(AsymmetricLoss):
   def forward(self, inputs: Logits, targets: MultiLabelOneHot, reduction=None):
     self.targets = targets
-    self.neg_targets = (1 - self.targets)
+    self.neg_targets = 1 - self.targets
 
     self.pos_probs = torch.sigmoid(inputs)
     self.neg_probs = 1 - self.pos_probs
@@ -83,8 +84,8 @@ class AsymmetricLossOptimized(AsymmetricLoss):
         self.neg_probs.mul_(self.neg_targets)
         weights = torch.pow(
           1 - self.pos_probs - self.neg_probs,
-          self.pos_decay * self.targets + self.neg_decay * self.neg_targets
+          self.pos_decay * self.targets + self.neg_decay * self.neg_targets,
         )
       self.losses *= weights
 
-    return _reduce(-self.losses, reduction=reduction or self.reduction)
\ No newline at end of file
+    return _reduce(-self.losses, reduction=reduction or self.reduction)
diff --git a/yann/modules/loss/multilabel.py b/yann/modules/loss/multilabel.py
index ebdf2a6..d6a0623 100644
--- a/yann/modules/loss/multilabel.py
+++ b/yann/modules/loss/multilabel.py
@@ -1,18 +1,19 @@
+import math
+
 import torch
 import torch.nn.functional as F
-import math
 
 from yann.modules.loss import _reduce
 
 
 class LargeLossNegativeRejection(torch.nn.Module):
   def __init__(
-      self,
-      loss=F.binary_cross_entropy_with_logits,
-      threshold=None,
-      percent=None,
-      reduction: str = 'mean',
-      pos_thresh=.5
+    self,
+    loss=F.binary_cross_entropy_with_logits,
+    threshold=None,
+    percent=None,
+    reduction: str = "mean",
+    pos_thresh=0.5,
   ):
     """
     Args:
@@ -27,10 +28,11 @@ class LargeLossNegativeRejection(torch.nn.Module):
     # need to disable reduction on wrapped loss
     self._loss_args = {}
     import inspect
-    if hasattr(self.loss, 'reduction'):
-      self.loss.reduction = 'none'
-    elif 'reduction' in inspect.getfullargspec(self.loss).args:
-      self._loss_args['reduction'] = 'none'
+
+    if hasattr(self.loss, "reduction"):
+      self.loss.reduction = "none"
+    elif "reduction" in inspect.getfullargspec(self.loss).args:
+      self._loss_args["reduction"] = "none"
 
     self.threshold = threshold
     self.percent = percent
@@ -38,7 +40,13 @@ class LargeLossNegativeRejection(torch.nn.Module):
 
     self.pos_thresh = pos_thresh
 
-  def forward(self, preds: torch.Tensor, targets: torch.Tensor, percent=None, threshold=None):
+  def forward(
+    self,
+    preds: torch.Tensor,
+    targets: torch.Tensor,
+    percent=None,
+    threshold=None,
+  ):
     percent = percent or self.percent
     threshold = threshold or self.threshold
 
@@ -49,7 +57,9 @@ class LargeLossNegativeRejection(torch.nn.Module):
     if percent is not None and percent > 0:
       unobserved_count = torch.count_nonzero(unobserved_losses)
       k = torch.ceil(unobserved_count * percent)
-      largest_unobserved_losses, _ = torch.topk(unobserved_losses.flatten(), int(k))
+      largest_unobserved_losses, _ = torch.topk(
+        unobserved_losses.flatten(), int(k)
+      )
       keep_mask = (unobserved_losses < largest_unobserved_losses[-1]).float()
       losses = losses * keep_mask
 
diff --git a/yann/modules/norm.py b/yann/modules/norm.py
index a4e0e81..e9d071f 100644
--- a/yann/modules/norm.py
+++ b/yann/modules/norm.py
@@ -2,7 +2,6 @@ import torch
 
 
 def normalize(batch, p=2, eps=1e-8):
-  return (
-      batch
-      / (torch.norm(batch, p=p, dim=1, keepdim=True) + eps).expand_as(batch)
+  return batch / (torch.norm(batch, p=p, dim=1, keepdim=True) + eps).expand_as(
+    batch
   )
diff --git a/yann/modules/pool.py b/yann/modules/pool.py
index e15d4dc..1c1ef03 100644
--- a/yann/modules/pool.py
+++ b/yann/modules/pool.py
@@ -1,11 +1,13 @@
-from torch.nn import functional as F
 import torch
 from torch import nn
+from torch.nn import functional as F
+
 
 def mac(batch):
   """MAC Pooling"""
   return F.adaptive_max_pool2d(batch, (1, 1))
 
+
 def spoc(batch):
   """SPoC Pooling"""
   return F.adaptive_avg_pool2d(batch, (1, 1))
@@ -18,13 +20,10 @@ def generalized_mean(batch, p=3, eps=1e-8):
 
   larger p leads to more localized (max) features
   """
-  return F.adaptive_avg_pool2d(
-    batch.clamp(min=eps) ** p,
-    (1, 1)
-  ) ** (1 / p)
+  return F.adaptive_avg_pool2d(batch.clamp(min=eps) ** p, (1, 1)) ** (1 / p)
 
-gem = generalized_mean
 
+gem = generalized_mean
 
 
 class GeM(nn.Module):
@@ -34,4 +33,4 @@ class GeM(nn.Module):
     self.eps = eps
 
   def forward(self, x):
-    return gem(x, p=self.p, eps=self.eps)
\ No newline at end of file
+    return gem(x, p=self.p, eps=self.eps)
diff --git a/yann/modules/residual.py b/yann/modules/residual.py
index 182ef1d..97f9d46 100644
--- a/yann/modules/residual.py
+++ b/yann/modules/residual.py
@@ -1,6 +1,5 @@
 from torch import nn
 
-# TODO: handle downsampling logic
 
 def residual(input, block, identity=None):
   p = block(input)
@@ -10,9 +9,11 @@ def residual(input, block, identity=None):
 
 
 class Residual(nn.Module):
-  def __init__(self, block, identity=None, activation=None):
+  def __init__(self, *block, identity=None, activation=None):
     super().__init__()
-    self.block = block
+    from . import Stack
+
+    self.block = block[0] if len(block) == 1 else Stack(*block)
     self.identity = identity
     self.activation = activation
 
@@ -25,4 +26,4 @@ class Residual(nn.Module):
     if self.activation:
       return self.activation(input)
     else:
-      return input
\ No newline at end of file
+      return input
diff --git a/yann/modules/selective_backprop.py b/yann/modules/selective_backprop.py
index 1b744df..4c24e71 100644
--- a/yann/modules/selective_backprop.py
+++ b/yann/modules/selective_backprop.py
@@ -1,7 +1,6 @@
 import torch
 
 
-
 class SelectiveBackprop(torch.nn.Module):
   def __init__(self, model, loss, k=None, percent=None, min=None):
     super(SelectiveBackprop, self).__init__()
@@ -17,7 +16,7 @@ class SelectiveBackprop(torch.nn.Module):
       indices = None
       with torch.inference_mode():
         outputs = self.model(inputs)
-        losses = self.loss(outputs, targets, reduction='none')
+        losses = self.loss(outputs, targets, reduction="none")
 
         if self.k:
           _, indices = torch.topk(losses, k=self.k)
@@ -25,13 +24,9 @@ class SelectiveBackprop(torch.nn.Module):
           indices = losses >= min
         # elif self.percent:
 
-
       inputs, targets = inputs[indices], targets[indices]
       outputs = self.model(inputs)
       loss = self.loss(outputs, targets)
       return loss
     else:
       return self.model(inputs)
-
-
-
diff --git a/yann/modules/shape.py b/yann/modules/shape.py
index 8138f38..46ab3e2 100644
--- a/yann/modules/shape.py
+++ b/yann/modules/shape.py
@@ -1,6 +1,8 @@
 from torch.nn import Module
+
 from ..exceptions import ShapeInferenceError
 
+
 class Reshape(Module):
   method = None
 
@@ -11,29 +13,27 @@ class Reshape(Module):
   def forward(self, input):
     return getattr(input, self.method)(*self.dims)
 
-  def state_dict(self, destination=None, prefix='', keep_vars=False):
-    return {
-      'dims': self.dims
-    }
+  def state_dict(self, destination=None, prefix="", keep_vars=False):
+    return {"dims": self.dims}
 
   def load_state_dict(self, state_dict, strict=True):
-    self.dims = state_dict['dims']
+    self.dims = state_dict["dims"]
 
 
 class Squeeze(Reshape):
-  method = 'squeeze'
+  method = "squeeze"
 
 
 class Permute(Reshape):
-  method = 'permute'
+  method = "permute"
 
 
 class Transpose(Reshape):
-  method = 'transpose'
+  method = "transpose"
 
 
 class View(Reshape):
-  method = 'view'
+  method = "view"
 
 
 class Flatten(Reshape):
@@ -58,7 +58,7 @@ def flatten_sequence(seq_batch):
 class Infer(Module):
   def __init__(self, cls, *args, **kwargs):
     super(Infer, self).__init__()
-    self.shape_dim = kwargs.pop('shape_dim', 1)
+    self.shape_dim = kwargs.pop("shape_dim", 1)
     self.cls = cls
     self.args = args
     self.kwargs = kwargs
@@ -72,12 +72,16 @@ class Infer(Module):
   def forward(self, x):
     if self.module is None:
       try:
-        self.module = self.cls(x.shape[self.shape_dim], *self.args, **self.kwargs)
+        self.module = self.cls(
+          x.shape[self.shape_dim], *self.args, **self.kwargs
+        )
       except IndexError as e:
-        raise ShapeInferenceError(f"Improper shape dim ({self.shape_dim}) selected for {self.cls} with input of shape {x.shape}")
+        raise ShapeInferenceError(
+          f"Improper shape dim ({self.shape_dim}) selected for {self.cls} with input of shape {x.shape}"
+        )
     return self.module(x)
 
   @classmethod
   def shed(cls, module):
     # TODO: modify the model to drop the Infer nodes and replace them with the initialized module
-    raise NotImplementedError()
\ No newline at end of file
+    raise NotImplementedError()
diff --git a/yann/modules/stack.py b/yann/modules/stack.py
index b4d10b4..bac6f73 100644
--- a/yann/modules/stack.py
+++ b/yann/modules/stack.py
@@ -58,7 +58,7 @@ class Stack(nn.Module):
         stop = layers.index(x.stop)
 
       return self.__class__(
-        **dict(list(self.named_children())[start:stop:x.step])
+        **dict(list(self.named_children())[start : stop : x.step])
       )
     elif isclass(x):
       return [m for m in self.children() if isinstance(m, x)]
@@ -77,4 +77,4 @@ class Stack(nn.Module):
   def upto(self, module):
     layers = list(self.children())
     stop = layers.index(module)
-    return self[:stop + 1]
\ No newline at end of file
+    return self[: stop + 1]
diff --git a/yann/optim/__init__.py b/yann/optim/__init__.py
index 9c3a500..c4e6e52 100644
--- a/yann/optim/__init__.py
+++ b/yann/optim/__init__.py
@@ -1 +1 @@
-from .clip import GradClipper, clip_grad_, clip_grad_adaptive_
\ No newline at end of file
+from .clip import GradClipper, clip_grad_, clip_grad_adaptive_
diff --git a/yann/optim/clip.py b/yann/optim/clip.py
index 7845653..50113be 100644
--- a/yann/optim/clip.py
+++ b/yann/optim/clip.py
@@ -1,6 +1,7 @@
 """
 Adapted from https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/agc.py
 """
+
 import torch
 from torch.nn.utils import clip_grad_norm_, clip_grad_value_
 
@@ -12,7 +13,7 @@ def unitwise_norm(x: torch.Tensor, p=2.0):
     return x.norm(p, dim=tuple(range(1, x.ndim)), keepdim=True)
 
 
-def clip_grad_adaptive_(parameters, value=.01, norm_type=2.0, eps=1e-3):
+def clip_grad_adaptive_(parameters, value=0.01, norm_type=2.0, eps=1e-3):
   """
   Adaptive grad clipping
   """
@@ -20,44 +21,38 @@ def clip_grad_adaptive_(parameters, value=.01, norm_type=2.0, eps=1e-3):
     parameters = [parameters]
   parameters = [p for p in parameters if p.grad is not None]
   if len(parameters) == 0:
-    return torch.tensor(0.)
+    return torch.tensor(0.0)
   for param in parameters:
     weights, grads = param.detach(), param.grad.detach()
-    max_norm = (
-      unitwise_norm(weights, p=norm_type)
-        .clamp_(min=eps)
-        .mul_(value)
-    )
+    max_norm = unitwise_norm(weights, p=norm_type).clamp_(min=eps).mul_(value)
     grad_norm = unitwise_norm(grads, p=norm_type)
     clipped_grad = grads * (max_norm / grad_norm.clamp(min=1e-6))
     new_grads = torch.where(grad_norm < max_norm, grads, clipped_grad)
     param.grad.detach().copy_(new_grads)
 
 
-def clip_grad_(parameters, value, norm_type=2.0, mode='adaptive'):
-  if mode == 'adaptive':
+def clip_grad_(parameters, value, norm_type=2.0, mode="adaptive"):
+  if mode == "adaptive":
     return clip_grad_adaptive_(
-      parameters=parameters,
-      value=value,
-      norm_type=norm_type
+      parameters=parameters, value=value, norm_type=norm_type
     )
-  elif mode == 'norm':
+  elif mode == "norm":
     return clip_grad_norm_(
-      parameters=parameters,
-      max_norm=value,
-      norm_type=norm_type
+      parameters=parameters, max_norm=value, norm_type=norm_type
     )
-  elif mode == 'value':
+  elif mode == "value":
     return clip_grad_value_(
       parameters=parameters,
       clip_value=value,
     )
   else:
-    raise ValueError(f'Unsupported mode={mode}, must be adaptive, norm or value')
+    raise ValueError(
+      f"Unsupported mode={mode}, must be adaptive, norm or value"
+    )
 
 
 class GradClipper:
-  def __init__(self, value, norm_type=2.0, mode='adaptive'):
+  def __init__(self, value, norm_type=2.0, mode="adaptive"):
     """
 
     Args:
@@ -74,11 +69,11 @@ class GradClipper:
       parameters=parameters,
       value=self.value,
       norm_type=self.norm_type,
-      mode=self.mode
+      mode=self.mode,
     )
 
   def state_dict(self):
     return self.__dict__
 
   def load_state_dict(self, dict):
-    self.__dict__.update(dict)
\ No newline at end of file
+    self.__dict__.update(dict)
diff --git a/yann/params.py b/yann/params.py
index d32f76d..788e930 100644
--- a/yann/params.py
+++ b/yann/params.py
@@ -30,21 +30,23 @@
 
 """
 
+import logging
+import typing
 from abc import ABCMeta
 from collections import OrderedDict
 from copy import deepcopy
 from functools import wraps
 from typing import Dict
-import typing
-import logging
-from .utils import get_arg_parser
 
+from .utils import get_arg_parser
 
 log = logging.getLogger(__name__)
 
+
 class ValidationError(ValueError):
   pass
 
+
 class Field:
   def __init__(
     self,
@@ -54,7 +56,7 @@ class Field:
     type=None,
     required=False,
     default=None,
-    choices=None
+    choices=None,
   ):
     self.name = name
     self.help = help
@@ -67,18 +69,24 @@ class Field:
     try:
       if self.type and not isinstance(val, self.type):
         raise ValidationError(
-          f'Failed to validate {self.name}, the type ({type(val)} does is not a subclass of {self.type}')
+          f"Failed to validate {self.name}, the type ({type(val)} does is not a subclass of {self.type}"
+        )
     except TypeError as e:
-      log.debug(f'skipping type validation due to unresolved forwardref for {self.name} and expected type {self.type}')
+      log.debug(
+        f"skipping type validation due to unresolved forwardref for {self.name} and expected type {self.type}"
+      )
     if self.choices:
       assert val in self.choices
 
-
   def __repr__(self):
-    return f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    return (
+      f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    )
 
   def __str__(self):
-    return f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    return (
+      f"{self.__class__.__name__}(type={self.type}, default={self.default})"
+    )
 
 
 class Choice(Field):
@@ -108,7 +116,7 @@ class HyperParamsBase:
         setattr(self, k, v)
       else:
         raise ValueError(
-          f'Unknown parameter: {k}, should be one of {", ".join(self.__fields__)}'
+          f"Unknown parameter: {k}, should be one of {', '.join(self.__fields__)}"
         )
 
   def validate(self):
@@ -121,15 +129,14 @@ class HyperParamsBase:
   @classmethod
   def from_command(cls, cmd=None, validate=False, **kwargs):
     parser = get_arg_parser(cls.__fields__, **kwargs)
-    parsed = parser.parse_args(
-      cmd.split() if isinstance(cmd, str) else cmd
-    )
+    parsed = parser.parse_args(cmd.split() if isinstance(cmd, str) else cmd)
     params = cls(**vars(parsed))
 
     if validate:
       params.validate()
 
     return params
+
   #
   # @classmethod
   # def from_env(cls, prefix=''):
@@ -149,11 +156,12 @@ class HyperParamsBase:
       try:
         data = yann.utils.dynamic_import(uri)
       except:
-        raise ValueError('uri must be a file or fully qualified python path')
+        raise ValueError("uri must be a file or fully qualified python path")
     return cls(**data)
 
   def save(self, path):
     import yann
+
     yann.save(dict(self), path)
 
   def on_change(self, callback):
@@ -180,24 +188,27 @@ class HyperParamsBase:
     return len(self.__fields__)
 
   def __eq__(self, other):
-    return len(self) == len(other) and self.keys() == other.keys(
-    ) and all(self[k] == other[k] for k in self.keys())
+    return (
+      len(self) == len(other)
+      and self.keys() == other.keys()
+      and all(self[k] == other[k] for k in self.keys())
+    )
 
   def fork(self, **args):
     return HyperParams(**{**self.items(), **args})
 
   def __repr__(self):
     return (
-      f'{self.__class__.__name__}('
+      f"{self.__class__.__name__}("
       f"{', '.join(f'{k}={v}' for k, v in self.items())}"
-      ')'
+      ")"
     )
 
   def __str__(self):
     return (
-      f'{self.__class__.__name__}(\n' +
-      ',\n'.join('  {}={}'.format(k, v) for k, v in self.items()) +
-      '\n)'
+      f"{self.__class__.__name__}(\n"
+      + ",\n".join("  {}={}".format(k, v) for k, v in self.items())
+      + "\n)"
     )
 
   def __contains__(self, key):
@@ -226,7 +237,7 @@ class HyperParamsBase:
     scope=None,
     types=(int, str, float, bool),
     upper_only=True,
-    lowercase=True
+    lowercase=True,
   ):
     scope = globals() if scope is None else scope
 
@@ -242,7 +253,6 @@ class HyperParamsBase:
     return cls(**d)
 
 
-
 class MetaHyperParams(ABCMeta):
   def __new__(metaclass, class_name, bases, namespace):
     fields = OrderedDict()
@@ -251,7 +261,7 @@ class MetaHyperParams(ABCMeta):
       if issubclass(base, HyperParamsBase) and base != HyperParamsBase:
         fields.update(
           # deepcopy(
-            base.__fields__
+          base.__fields__
           # )
         )
 
@@ -260,11 +270,10 @@ class MetaHyperParams(ABCMeta):
     new_attributes = {
       k: v
       for (k, v) in namespace.items()
-      if not k.startswith('_') and
-      not callable(v)
+      if not k.startswith("_") and not callable(v)
     }
 
-    for name, annotation in namespace.get('__annotations__', {}).items():
+    for name, annotation in namespace.get("__annotations__", {}).items():
       if name not in new_attributes:
         continue
       if isinstance(annotation, Field):
@@ -285,9 +294,9 @@ class MetaHyperParams(ABCMeta):
       class_name,
       bases,
       {
-        '__fields__': fields,
+        "__fields__": fields,
         **namespace,
-      }
+      },
     )
 
 
@@ -299,28 +308,26 @@ class HyperParams(HyperParamsBase, metaclass=MetaHyperParams):
     self.__dict__.update(state)
 
   def __reduce__(self):
-    return (self.__class__, )
-
+    return (self.__class__,)
 
 
 def to_argparse(params: HyperParams, **kwargs):
   return get_arg_parser(params.__fields__, **kwargs)
 
 
-
 def bind(params, mapping=None):
   def decorator(function):
-
     import inspect
+
     sig = inspect.signature(function)
 
     _mapping = mapping or {}
     for p in sig.parameters:
       if p not in _mapping and p in params:
         _mapping[p] = p
+
     @wraps(function)
     def bound(*args, **kwargs):
-
       for k, p in _mapping.items():
         if k in kwargs:
           params[p] = kwargs[k]
@@ -334,24 +341,25 @@ def bind(params, mapping=None):
   return decorator
 
 
-
 def from_signature(function, params: HyperParams = None):
   import inspect
+
   sig = inspect.signature(function)
 
   params = params or HyperParams()
   for k, p in sig.parameters.items():
-
     default = p.default if p.default is not p.empty else None
 
     params.__fields__[k] = Field(
       name=p.name,
       default=default,
-      type=p.annotation if p.annotation is not p.empty else (type(p.default) if p.default is not p.empty else None)
+      type=p.annotation
+      if p.annotation is not p.empty
+      else (type(p.default) if p.default is not p.empty else None),
     )
     params[k] = default
   return params
 
 
 def register():
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/perf.py b/yann/perf.py
index 1e3a62d..e597c9a 100644
--- a/yann/perf.py
+++ b/yann/perf.py
@@ -7,20 +7,22 @@ def optimize(
   quantize=False,
   freeze=False,
   example=None,
-  device=None
+  device=None,
 ):
   if benchmark:
     from torch.backends import cudnn
+
     cudnn.benchmark = True
 
   if model:
     if example:
       if isinstance(example, tuple):
         import torch
+
         example = torch.randn(*example)
 
       if jit:
         import torch.jit
+
         model = torch.jit.trace(model, example)
   return model
-
diff --git a/yann/schedule.py b/yann/schedule.py
index 4c79b40..e8fe044 100644
--- a/yann/schedule.py
+++ b/yann/schedule.py
@@ -1,9 +1,7 @@
-
 from functools import update_wrapper
 from typing import Any
 
 
-
 class Scheduler:
   def __init__(self):
     self.index = 0
@@ -60,10 +58,8 @@ def scheduled(*, get_step=None, **params):
   Returns:
 
   """
+
   def decorator(func):
     return Scheduled(func, **params, get_step=get_step)
 
   return decorator
-
-
-
diff --git a/yann/testing.py b/yann/testing.py
index 46a0262..5600a9a 100644
--- a/yann/testing.py
+++ b/yann/testing.py
@@ -6,12 +6,12 @@ TODO: snapshot testing
 TODO: acceptance criteria / validation against test set
 """
 
+from contextlib import contextmanager
 
 import torch
-from contextlib import contextmanager
 
-from .utils.debug import iter_allocated_tensors
 from .exceptions import CheckFailure
+from .utils.debug import iter_allocated_tensors
 
 
 def check_tensor(
@@ -34,21 +34,18 @@ def check_tensor(
   gt=None,
   lte=None,
   gte=None,
-  none=False
+  none=False,
 ):
   if share_memory is not None:
     assert t.storage().data_ptr() == share_memory.storage().data_ptr()
   if not_share_memory is not None:
-    assert t.storage().data_ptr() != not_share_memory.storage(
-    ).data_ptr()
+    assert t.storage().data_ptr() != not_share_memory.storage().data_ptr()
   if different is not None:
     assert different is not t
   if same is not None:
     assert same is t
   if like is not None:
-    check_tensor(
-      t, device=like.device, shape=like.shape, dtype=like.shape
-    )
+    check_tensor(t, device=like.device, shape=like.shape, dtype=like.shape)
   if not none:
     assert t is not None
   if device:
@@ -109,12 +106,10 @@ def newly_allocated_tensors(count=None, max=None):
   diff = len(new_tensors)
 
   if count is not None and count != diff:
-    raise CheckFailure(
-      f'Expected {count} tensor allocations but got {diff}'
-    )
+    raise CheckFailure(f"Expected {count} tensor allocations but got {diff}")
   if max is not None:
     raise CheckFailure(
-      f'Expected at most {count} tensor allocations but got {diff}'
+      f"Expected at most {count} tensor allocations but got {diff}"
     )
 
 
@@ -158,13 +153,7 @@ class Checker:
 
 
 def rand_image_tensor(
-  height,
-  width=None,
-  channels=3,
-  min=0,
-  max=1,
-  dtype=None,
-  device=None
+  height, width=None, channels=3, min=0, max=1, dtype=None, device=None
 ):
   if width is None:
     width = height
@@ -173,20 +162,11 @@ def rand_image_tensor(
 
 
 def rand_image_batch(
-  height,
-  width=None,
-  channels=3,
-  num=16,
-  min=0,
-  max=1,
-  dtype=None,
-  device=None
+  height, width=None, channels=3, num=16, min=0, max=1, dtype=None, device=None
 ):
   if width is None:
     width = height
-  t = torch.rand(
-    num, channels, height, width, dtype=dtype, device=device
-  )
+  t = torch.rand(num, channels, height, width, dtype=dtype, device=device)
   return (min - max) * t + max
 
 
@@ -198,11 +178,13 @@ def check_model(
   output_shape=None,
   # loss=None,
   device=None,
-  dtype=None
+  dtype=None,
 ):
   device = device or model.device
-  input = input if input is not None else torch.rand(
-    input_shape, device=device, dtype=dtype
+  input = (
+    input
+    if input is not None
+    else torch.rand(input_shape, device=device, dtype=dtype)
   )
 
   output = model(input)
diff --git a/yann/train/__init__.py b/yann/train/__init__.py
index 263220c..926992c 100644
--- a/yann/train/__init__.py
+++ b/yann/train/__init__.py
@@ -1,16 +1,16 @@
-
 from pathlib import Path
-from ..data.io import load_json
+
 from ..data import flatten
+from ..data.io import load_json
+from .functional import step, train
 from .trainer import Trainer
 
-from .functional import train, step
-
 
-def collect_summaries(root='.', name='summary.json', pandas=True):
-  s = [load_json(f) for f in Path(root).glob(f'**/*{name}')]
+def collect_summaries(root=".", name="summary.json", pandas=True):
+  s = [load_json(f) for f in Path(root).glob(f"**/*{name}")]
   if pandas:
     import pandas as pd
+
     return pd.DataFrame([flatten(x) for x in s])
   else:
     return s
diff --git a/yann/train/base.py b/yann/train/base.py
index 34b1a70..437a081 100644
--- a/yann/train/base.py
+++ b/yann/train/base.py
@@ -23,11 +23,3 @@ class BaseTrainer:
   @classmethod
   def from_checkpoint(cls, path):
     pass
-
-
-
-
-
-
-
-
diff --git a/yann/train/functional.py b/yann/train/functional.py
index c3c2f49..3f3dfc9 100644
--- a/yann/train/functional.py
+++ b/yann/train/functional.py
@@ -1,6 +1,11 @@
-
-
-def step(model, inputs, targets, optimizer, loss, callback: 'yann.callbacks.callback.Callback' = None):
+def step(
+  model,
+  inputs,
+  targets,
+  optimizer,
+  loss,
+  callback: "yann.callbacks.callback.Callback" = None,
+):
   model.train()
   optimizer.zero_grad()
 
@@ -13,8 +18,16 @@ def step(model, inputs, targets, optimizer, loss, callback: 'yann.callbacks.call
   return inputs, targets, pred, loss
 
 
-def train(model, batches, optimizer, loss, device=None, step=step, callback: 'yann.callbacks.callback.Callback'=None):
+def train(
+  model,
+  batches,
+  optimizer,
+  loss,
+  device=None,
+  step=step,
+  callback: "yann.callbacks.callback.Callback" = None,
+):
   for inputs, targets in batches:
     if device:
       inputs, targets = inputs.to(device), targets.to(device)
-    yield step(model, inputs, targets, optimizer, loss)
\ No newline at end of file
+    yield step(model, inputs, targets, optimizer, loss)
diff --git a/yann/train/paths.py b/yann/train/paths.py
index b16d295..3e18df2 100644
--- a/yann/train/paths.py
+++ b/yann/train/paths.py
@@ -1,20 +1,20 @@
 import pathlib
-from yann.utils import print_tree
 
+from yann.utils import print_tree
 
 
 class Paths:
   def __init__(self, root):
     self.root = pathlib.Path(root)
 
-    self.checkpoint_format = '{}{}'
+    self.checkpoint_format = "{}{}"
 
   def create(self):
     self.root.mkdir(parents=True, exist_ok=True)
 
   @property
   def checkpoints(self):
-    path = self.root / 'checkpoints'
+    path = self.root / "checkpoints"
     path.mkdir(parents=True, exist_ok=True)
     return path
 
@@ -23,43 +23,43 @@ class Paths:
 
   @property
   def tensorboard(self):
-    return self.root / 'tensorboard'
+    return self.root / "tensorboard"
 
   @property
   def logs(self):
-    return self.root / 'logs'
+    return self.root / "logs"
 
   @property
   def evals(self):
-    return self.root / 'evals'
+    return self.root / "evals"
 
   @property
   def plots(self):
-    return self.root / 'plots'
+    return self.root / "plots"
 
   @property
   def outputs(self):
-    return self.root / 'outputs'
+    return self.root / "outputs"
 
   @property
   def exports(self):
-    return self.root / 'exports'
+    return self.root / "exports"
 
   @property
   def summary(self):
-    return self.root / 'summary.yaml'
+    return self.root / "summary.yaml"
 
   @property
   def profile(self):
-      return self.root / 'profile'
+    return self.root / "profile"
 
   @property
   def git_diff(self):
-    return self.root / 'git.diff'
+    return self.root / "git.diff"
 
   @property
   def requirements(self):
-    return self.root / 'requirements.txt'
+    return self.root / "requirements.txt"
 
   def tree(self, **kwargs):
-    print_tree(self.root, **kwargs)
\ No newline at end of file
+    print_tree(self.root, **kwargs)
diff --git a/yann/train/track.py b/yann/train/track.py
index b8a35a6..465af3a 100644
--- a/yann/train/track.py
+++ b/yann/train/track.py
@@ -1,4 +1,4 @@
-from typing import Mapping, Any
+from typing import Any, Mapping
 
 import yann
 
@@ -7,25 +7,28 @@ class Tracker:
   """
   Simple callable that takes a trainer instance as input and returns a dict of values to log
   """
+
   freq: int = None
 
-  def __call__(self, trainer: 'yann.train.Trainer') -> Mapping[str, Any]:
+  def __call__(self, trainer: "yann.train.Trainer") -> Mapping[str, Any]:
     raise NotImplementedError
 
 
 class OptimizerState(Tracker):
   def __init__(
-      self,
-      optimizer=None,
-      prefix='',
-      keys=('lr', 'weight_decay', 'momentum', 'betas', 'alpha')):
+    self,
+    optimizer=None,
+    prefix="",
+    keys=("lr", "weight_decay", "momentum", "betas", "alpha"),
+  ):
     self.optimizer = optimizer
     self.keys = keys
     self.prefix = prefix
 
-  def __call__(self, trainer: 'import yann.train.trainer.Trainer'):
+  def __call__(self, trainer: "import yann.train.trainer.Trainer"):
     optim = self.optimizer or trainer.optimizer
-    if not optim: return {}
+    if not optim:
+      return {}
 
     values = {}
 
@@ -35,32 +38,34 @@ class OptimizerState(Tracker):
           value = group[key]
           if isinstance(value, (tuple, list)):
             for m, v in enumerate(value):
-              values[f'{self.prefix}optimizer.param_groups.{n}.{key}.{m}'] = v
+              values[f"{self.prefix}optimizer.param_groups.{n}.{key}.{m}"] = v
           else:
-            values[f'{self.prefix}optimizer.param_groups.{n}.{key}'] = value
+            values[f"{self.prefix}optimizer.param_groups.{n}.{key}"] = value
 
     return values
 
 
 class ParamNorms(Tracker):
-  def __init__(self, model=None, key='param_norm'):
+  def __init__(self, model=None, key="param_norm"):
     self.model = model
     self.key = key
 
   def __call__(self, trainer):
     import yann
+
     model = self.model or trainer.model
 
     return {self.key: yann.param_norm(model)}
 
 
 class GradNorms(Tracker):
-  def __init__(self, model=None, key='grad_norm'):
+  def __init__(self, model=None, key="grad_norm"):
     self.model = model
     self.key = key
 
   def __call__(self, trainer):
     import yann
+
     model = self.model or trainer.model
 
     return {self.key: yann.grad_norm(model)}
@@ -78,4 +83,3 @@ class Keys(Tracker):
       except:
         pass
     return values
-
diff --git a/yann/train/trainer.py b/yann/train/trainer.py
index 75eea4e..f91bf21 100644
--- a/yann/train/trainer.py
+++ b/yann/train/trainer.py
@@ -1,31 +1,32 @@
-
 import datetime
 import inspect
 import logging
 import types
 from pathlib import Path
-from typing import Optional, Callable, Union, Dict, Mapping, Sequence
+from typing import Callable, Dict, Mapping, Optional, Sequence, Union
 
 import torch
 import torch.nn
-from torch import autocast
-from torch.cuda.amp import GradScaler, autocast
+from torch.amp import GradScaler, autocast
 from torch.optim.optimizer import Optimizer
-from torch.utils.data import Sampler, DataLoader
-from typing_extensions import Literal
-from typing_extensions import Unpack
+from torch.utils.data import DataLoader, Sampler
+from typing_extensions import Literal, Unpack
 
 import yann
-from yann.data import get_dataset_name, Classes
-from yann.datasets import TransformDataset, Subset
-from yann.distributed import Dist
 import yann.distributed
+from yann.data import Classes, get_dataset_name
+from yann.datasets import Subset, TransformDataset
+from yann.distributed import Dist
 from yann.export import export
 from yann.train.base import BaseTrainer
 from yann.train.paths import Paths
 from yann.utils import (
-  counter, timestr, hash_params, fully_qualified_name,
-  memorable_id, apply_known
+  apply_known,
+  counter,
+  fully_qualified_name,
+  hash_params,
+  memorable_id,
+  timestr,
 )
 from yann.utils.bash import git_diff, pip_freeze
 from yann.utils.timer import time
@@ -37,6 +38,7 @@ class Keys:
   """
   keys for data batch
   """
+
   ids = None
   inputs = 0
   targets = 1
@@ -56,7 +58,7 @@ class Params(yann.params.HyperParams):
   meta: Optional[Dict] = None
 
   # root directory where training runs are stored
-  root: Union[str, Path, Paths] = './runs/'
+  root: Union[str, Path, Paths] = "./runs/"
 
   model: Union[torch.nn.Module, str, None] = None
 
@@ -66,7 +68,7 @@ class Params(yann.params.HyperParams):
   classes: Union[yann.data.Classes, Sequence[str], None] = None
 
   optimizer: Union[torch.optim.Optimizer, str, None] = None
-  parameters: Union[torch.nn.ParameterList, Literal['trainable']] = 'trainable'
+  parameters: Union[torch.nn.ParameterList, Literal["trainable"]] = "trainable"
   lr: Optional[float] = None
   weight_decay: Optional[float] = None
   momentum: Optional[float] = None
@@ -92,48 +94,36 @@ class Params(yann.params.HyperParams):
   persistent_workers: Optional[bool] = True
 
   transform: Union[
-    Callable,
-    Mapping[str, Callable],
-    Sequence[Callable],
-    None
+    Callable, Mapping[str, Callable], Sequence[Callable], None
   ] = None
   transform_batch: Union[Callable, None] = None
 
   callbacks: Union[
-    Sequence['yann.callbacks.Callback'],
-    'yann.callbacks.Callbacks',
-    None
+    Sequence["yann.callbacks.Callback"], "yann.callbacks.Callbacks", None
   ] = None
   device: Union[torch.device, str, None] = None
   dtype: Optional[torch.dtype] = None
 
-
   val_dataset: Union[torch.utils.data.Dataset, float, str, None] = None
   val_subset: Optional[int] = None
   val_loader: Union[torch.utils.data.DataLoader, None] = None
   val_transform: Union[
-    Callable,
-    Mapping[str, Callable],
-    Sequence[Callable],
-    None
+    Callable, Mapping[str, Callable], Sequence[Callable], None
   ] = None
 
   metrics: Union[
-    Dict[str, Callable],
-    Sequence[Callable],
-    Sequence[str],
-    None
+    Dict[str, Callable], Sequence[Callable], Sequence[str], None
   ] = None
 
   dist: Optional[Dist] = None
-  parallel: Union[None, Literal['dp', 'ddp']] = None
+  parallel: Union[None, Literal["dp", "ddp"]] = None
 
   amp: bool = False
-  grad_scaler: Optional[torch.cuda.amp.GradScaler] = None
+  grad_scaler: Optional[torch.amp.GradScaler] = None
 
   benchmark: bool = True
   jit: bool = False
-  memory_format: Optional[str] = 'preserve_format'
+  memory_format: Optional[str] = "preserve_format"
   aot_autograd: bool = False
   cuda_graph: bool = False
 
@@ -142,10 +132,9 @@ class Params(yann.params.HyperParams):
 
   step: Optional[Callable] = None
   place: Optional[Union[Callable, dict, tuple, yann.data.place.Place]] = None
-  clip_grad: Union[Callable, 'yann.optim.clip.GradClipper', dict] = None
+  clip_grad: Union[Callable, "yann.optim.clip.GradClipper", dict] = None
   seed: Optional[int] = None
 
-
   from_checkpoint: Optional[str] = None
 
 
@@ -167,6 +156,7 @@ class Trainer(TrainState, BaseTrainer):
     self.update()
 
   """
+
   Params = Params
 
   params: Params
@@ -182,14 +172,13 @@ class Trainer(TrainState, BaseTrainer):
   sampler: Optional[Sampler] = None
 
   paths: Paths = None
-  callbacks: Optional['yann.callbacks.Callbacks'] = None
-  log: Optional['yann.callbacks.Logger'] = None
+  callbacks: Optional["yann.callbacks.Callbacks"] = None
+  log: Optional["yann.callbacks.Logger"] = None
 
   # automatic mixed precision
   grad_scaler: Optional[GradScaler] = None
 
-  history: 'yann.callbacks.History' = None
-
+  history: "yann.callbacks.History" = None
 
   # Dict with training run summary information
   summary: dict
@@ -199,130 +188,90 @@ class Trainer(TrainState, BaseTrainer):
   DataLoader = DataLoader
 
   @classmethod
-  def from_params(
-      cls,
-      params: Params,
-      **kwargs: Unpack[Params]
-  ):
-
-    return cls(
-      **{
-        **params,
-        **kwargs
-      },
-      params=params
-    )
+  def from_params(cls, params: Params, **kwargs: Unpack[Params]):
+    return cls(**{**params, **kwargs}, params=params)
 
-  @time('Initialize Trainer')
+  @time("Initialize Trainer")
   def __init__(
-      self,
-
-      # Metadata
-      id: Union[str, int, None] = None,
-      name: Union[str, None] = None,
-      description: Optional[str] = None,
-      project: Optional[str] = None,
-      meta: Optional[Dict] = None,
-
-
-      model: Union[torch.nn.Module, str, None] = None,
-      dataset: Union[torch.utils.data.Dataset, str, None] = None,
-      optimizer: Union[torch.optim.Optimizer, str, None] = None,
-      loss: Union[torch.nn.Module, str, None] = None,
-
-
-      # root directory where training runs are stored
-      root: Union[str, Path, Paths] = None,
-
-      subset: Optional[int] = None,
-      batch_size: int = None,
-      classes: Union[yann.data.Classes, Sequence[str], None] = None,
-
-      parameters: Union[torch.nn.ParameterList, Literal['trainable']] = None,
-      lr: Optional[float] = None,
-      weight_decay: Optional[float] = None,
-      momentum: Optional[float] = None,
-      lr_scheduler: Union[torch.optim.lr_scheduler._LRScheduler, None] = None,
-      lr_batch_step: bool = None,
-      none_grad: bool = None,
-
-      epochs: Optional[int] = None,
-
-      loader: Union[torch.utils.data.DataLoader, None] = None,
-      num_workers: int = 8,
-      collate: Union[Callable, None] = None,
-      pin_memory: bool = None,
-      sampler: Union[torch.utils.data.Sampler, None] = None,
-      batch_sampler: Union[torch.utils.data.BatchSampler, None] = None,
-      prefetch_factor: Optional[int] = None,
-      persistent_workers: Optional[bool] = True,
-
-      transform: Union[
-        Callable,
-        Mapping[str, Callable],
-        Sequence[Callable],
-        None
-      ] = None,
-      transform_batch: Union[Callable, None] = None,
-
-      callbacks: Union[
-        Sequence['yann.callbacks.Callback'],
-        'yann.callbacks.Callbacks',
-        None,
-        bool
-      ] = None,
-      device: Union[torch.device, str, None] = None,
-      dtype: Optional[torch.dtype] = None,
-
-      val_dataset: Union[torch.utils.data.Dataset, str, float, None] = None,
-      val_subset: Optional[int] = None,
-      val_loader: Union[torch.utils.data.DataLoader, None] = None,
-      val_transform: Union[
-        Callable,
-        Mapping[str, Callable],
-        Sequence[Callable],
-        None
-      ] = None,
-
-      metrics: Union[
-        Dict[str, Callable],
-        Sequence[Callable],
-        Sequence[str],
-        None
-      ] = None,
-
-      dist: Optional[Dist] = None,
-      parallel: Union[None, Literal['dp', 'ddp']] = None,
-
-      amp: bool = None,
-      grad_scaler: Optional[torch.cuda.amp.GradScaler] = None,
-
-      benchmark: bool = None,
-      jit: bool = None,
-      memory_format: Optional[str] = None,
-      aot_autograd: bool = None,
-      cuda_graph: bool = None,
-
-      compile: bool = None,
-      tf32: bool = None,
-
-      step: Optional[Callable] = None,
-      place: Optional[Union[Callable, dict, tuple, yann.data.place.Place]] = None,
-      clip_grad: Union[Callable, 'yann.optim.clip.GradClipper', dict] = None,
-      seed: Optional[int] = None,
-      keys: Keys = None,
-
-      from_checkpoint: Optional[str] = None,
-
-      params: Union[Params, str, None] = None,
-      **extra
+    self,
+    # Metadata
+    id: Union[str, int, None] = None,
+    name: Union[str, None] = None,
+    description: Optional[str] = None,
+    project: Optional[str] = None,
+    meta: Optional[Dict] = None,
+    model: Union[torch.nn.Module, str, None] = None,
+    dataset: Union[torch.utils.data.Dataset, str, None] = None,
+    optimizer: Union[torch.optim.Optimizer, str, None] = None,
+    loss: Union[torch.nn.Module, str, None] = None,
+    # root directory where training runs are stored
+    root: Union[str, Path, Paths] = None,
+    subset: Optional[int] = None,
+    batch_size: int = None,
+    classes: Union[yann.data.Classes, Sequence[str], None] = None,
+    parameters: Union[torch.nn.ParameterList, Literal["trainable"]] = None,
+    lr: Optional[float] = None,
+    weight_decay: Optional[float] = None,
+    momentum: Optional[float] = None,
+    lr_scheduler: Union[torch.optim.lr_scheduler._LRScheduler, None] = None,
+    lr_batch_step: bool = None,
+    none_grad: bool = None,
+    epochs: Optional[int] = None,
+    loader: Union[torch.utils.data.DataLoader, None] = None,
+    num_workers: int = 8,
+    collate: Union[Callable, None] = None,
+    pin_memory: bool = None,
+    sampler: Union[torch.utils.data.Sampler, None] = None,
+    batch_sampler: Union[torch.utils.data.BatchSampler, None] = None,
+    prefetch_factor: Optional[int] = None,
+    persistent_workers: Optional[bool] = True,
+    transform: Union[
+      Callable, Mapping[str, Callable], Sequence[Callable], None
+    ] = None,
+    transform_batch: Union[Callable, None] = None,
+    callbacks: Union[
+      Sequence["yann.callbacks.Callback"],
+      "yann.callbacks.Callbacks",
+      None,
+      bool,
+    ] = None,
+    device: Union[torch.device, str, None] = None,
+    dtype: Optional[torch.dtype] = None,
+    val_dataset: Union[torch.utils.data.Dataset, str, float, None] = None,
+    val_subset: Optional[int] = None,
+    val_loader: Union[torch.utils.data.DataLoader, None] = None,
+    val_transform: Union[
+      Callable, Mapping[str, Callable], Sequence[Callable], None
+    ] = None,
+    metrics: Union[
+      Dict[str, Callable], Sequence[Callable], Sequence[str], None
+    ] = None,
+    dist: Optional[Dist] = None,
+    parallel: Union[None, Literal["dp", "ddp"]] = None,
+    amp: bool = None,
+    grad_scaler: Optional[torch.amp.GradScaler] = None,
+    benchmark: bool = None,
+    jit: bool = None,
+    memory_format: Optional[str] = None,
+    aot_autograd: bool = None,
+    cuda_graph: bool = None,
+    compile: bool = None,
+    tf32: bool = None,
+    step: Optional[Callable] = None,
+    place: Optional[Union[Callable, dict, tuple, yann.data.place.Place]] = None,
+    clip_grad: Union[Callable, "yann.optim.clip.GradClipper", dict] = None,
+    seed: Optional[int] = None,
+    keys: Keys = None,
+    from_checkpoint: Optional[str] = None,
+    params: Union[Params, str, None] = None,
+    **extra,
   ):
     super().__init__()
 
     kwargs = {**locals()}
-    kwargs.pop('self')
-    kwargs.pop('params')
-    kwargs.pop('__class__')
+    kwargs.pop("self")
+    kwargs.pop("params")
+    kwargs.pop("__class__")
 
     self.params = params or self.Params()
 
@@ -352,21 +301,14 @@ class Trainer(TrainState, BaseTrainer):
     device = yann.default.device if device is None else device
     self.device = torch.device(device) if isinstance(device, str) else device
 
-    self.memory_format = yann.memory_formats.get(
-      memory_format,
-      memory_format
-    )
+    self.memory_format = yann.memory_formats.get(memory_format, memory_format)
     self.dtype = dtype
 
     self.parallel = parallel
     self.lr_batch_step = lr_batch_step
     self.none_grad = none_grad
 
-    self.model = yann.resolve.model(
-      model,
-      required=False,
-      validate=callable
-    )
+    self.model = yann.resolve.model(model, required=False, validate=callable)
 
     if tf32:
       torch.backends.cuda.matmul.allow_tf32 = True
@@ -383,14 +325,10 @@ class Trainer(TrainState, BaseTrainer):
       try:
         from functorch.compile import memory_efficient_fusion
       except ImportError:
-        raise ValueError('functorch must be installed for aot_autograd support')
+        raise ValueError("functorch must be installed for aot_autograd support")
       self.model = memory_efficient_fusion(self.model)
 
-    self.loss = yann.resolve.loss(
-      loss,
-      required=False,
-      validate=callable
-    )
+    self.loss = yann.resolve.loss(loss, required=False, validate=callable)
 
     self._init_parallel(**kwargs)
     self._init_optim(**kwargs)
@@ -406,6 +344,7 @@ class Trainer(TrainState, BaseTrainer):
         self.place = place
       else:
         from yann.data.place import Place
+
         self.place = Place(place)
 
     self.to(device=self.device, memory_format=self.memory_format)
@@ -418,8 +357,10 @@ class Trainer(TrainState, BaseTrainer):
       self.paths = root
     else:
       self.paths = Paths(
-        Path(root or yann.default.train_root) / self.name / timestr(
-          self.time_created))
+        Path(root or yann.default.train_root)
+        / self.name
+        / timestr(self.time_created)
+      )
     self.paths.create()
 
     if self.dist.is_main:
@@ -437,12 +378,7 @@ class Trainer(TrainState, BaseTrainer):
 
     # self.callbacks.on_init(trainer=self, kwargs=kwargs)
 
-  def _init_callbacks(
-      self,
-      metrics=None,
-      callbacks=None,
-      **kwargs
-  ):
+  def _init_callbacks(self, metrics=None, callbacks=None, **kwargs):
     from yann.callbacks import get_callbacks
     from yann.callbacks.callbacks import Callbacks
 
@@ -456,67 +392,72 @@ class Trainer(TrainState, BaseTrainer):
     #   return
 
     callbacks = get_callbacks() if callbacks is True else callbacks
-    callbacks = [c for c in callbacks if yann.distributed.matches(c.dist_placement, self.dist)]
+    callbacks = [
+      c
+      for c in callbacks
+      if yann.distributed.matches(c.dist_placement, self.dist)
+    ]
 
     self.callbacks = Callbacks(callbacks)
 
-    if 'history' not in self.callbacks:
+    if "history" not in self.callbacks:
       metrics = (metrics,) if isinstance(metrics, str) else metrics
-      self.callbacks.history = yann.callbacks.History(**metrics) \
-        if isinstance(metrics, dict) \
+      self.callbacks.history = (
+        yann.callbacks.History(**metrics)
+        if isinstance(metrics, dict)
         else yann.callbacks.History(*metrics)
+      )
 
     self.history = self.callbacks.history
-    self.callbacks.move_to_start('history')
+    self.callbacks.move_to_start("history")
 
   def _init_parallel(self, parallel=None, **kwargs):
     if self.model is not None:
-      if parallel == 'dp':
+      if parallel == "dp":
         if not isinstance(self.model, torch.nn.parallel.DataParallel):
           self.model = torch.nn.DataParallel(self.model)
-      elif parallel == 'ddp' or (parallel is None and self.dist.is_enabled):
-        self.parallel = 'ddp'
-        if not isinstance(self.model, torch.nn.parallel.DistributedDataParallel):
+      elif parallel == "ddp" or (parallel is None and self.dist.is_enabled):
+        self.parallel = "ddp"
+        if not isinstance(
+          self.model, torch.nn.parallel.DistributedDataParallel
+        ):
           self.model.to(self.device)
           self.model = torch.nn.parallel.DistributedDataParallel(
             self.model,
             device_ids=[self.dist.local_rank],
             output_device=self.dist.local_rank,
-            find_unused_parameters=yann.default.ddp_find_unused_parameters
+            find_unused_parameters=yann.default.ddp_find_unused_parameters,
           )
 
-  @time('Initialize Data Loading')
+  @time("Initialize Data Loading")
   def _init_data_loaders(
-      self,
-      dataset=None,
-      batch_size=None,
-      subset=None,
-      classes=None,
-      transform=None,
-      transform_batch=None,
-      loader=None,
-      sampler=None,
-      batch_sampler=None,
-      pin_memory=None,
-      collate=None,
-      num_workers=None,
-      prefetch_factor=2,
-      persistent_workers=True,
-      val_dataset=None,
-      val_subset=None,
-      val_transform=None,
-      val_loader=None,
-      **kwargs
+    self,
+    dataset=None,
+    batch_size=None,
+    subset=None,
+    classes=None,
+    transform=None,
+    transform_batch=None,
+    loader=None,
+    sampler=None,
+    batch_sampler=None,
+    pin_memory=None,
+    collate=None,
+    num_workers=None,
+    prefetch_factor=2,
+    persistent_workers=True,
+    val_dataset=None,
+    val_subset=None,
+    val_transform=None,
+    val_loader=None,
+    **kwargs,
   ):
     self.batch_size = batch_size or yann.default.batch_size
 
     self.dataset = yann.resolve.dataset(dataset, required=False)
     if self.dataset and subset is not None:
       self.dataset = yann.datasets.Subset(
-        self.dataset,
-        *subset
-        if isinstance(subset, tuple)
-        else (subset,)
+        self.dataset, *subset if isinstance(subset, tuple) else (subset,)
       )
 
     if isinstance(val_dataset, float):
@@ -526,11 +467,11 @@ class Trainer(TrainState, BaseTrainer):
 
     if classes:
       self.classes = (
-        classes if isinstance(classes, Classes)
-        else Classes(classes)
+        classes if isinstance(classes, Classes) else Classes(classes)
       )
-    elif hasattr(self.dataset, 'classes') and \
-        isinstance(self.dataset.classes, Classes):
+    elif hasattr(self.dataset, "classes") and isinstance(
+      self.dataset.classes, Classes
+    ):
       self.classes = self.dataset.classes
     else:
       self.classes = None
@@ -544,9 +485,7 @@ class Trainer(TrainState, BaseTrainer):
     self.sampler = sampler
     if not self.sampler and self.dist.is_enabled:
       self.sampler = torch.utils.data.distributed.DistributedSampler(
-        self.dataset,
-        num_replicas=self.dist.world_size,
-        rank=self.dist.rank
+        self.dataset, num_replicas=self.dist.world_size, rank=self.dist.rank
       )
 
     if loader is not None:
@@ -554,9 +493,10 @@ class Trainer(TrainState, BaseTrainer):
     elif self.dataset is not None:
       if batch_sampler is not None:
         loader_signature = inspect.signature(self.DataLoader)
-        if 'batch_sampler' not in loader_signature.parameters:
+        if "batch_sampler" not in loader_signature.parameters:
           raise ValueError(
-            'batch_sampler provided but DataLoader does not support it, might need to upgrade pytorch to newer version')
+            "batch_sampler provided but DataLoader does not support it, might need to upgrade pytorch to newer version"
+          )
         self.loader = self.DataLoader(
           dataset=self.dataset,
           batch_sampler=batch_sampler,
@@ -564,7 +504,7 @@ class Trainer(TrainState, BaseTrainer):
           num_workers=num_workers,
           persistent_workers=persistent_workers and num_workers > 0,
           prefetch_factor=prefetch_factor,
-          **({'collate_fn': collate} if collate else {})
+          **({"collate_fn": collate} if collate else {}),
         )
       else:
         self.loader = self.DataLoader(
@@ -576,7 +516,7 @@ class Trainer(TrainState, BaseTrainer):
           num_workers=num_workers,
           persistent_workers=persistent_workers and num_workers > 0,
           prefetch_factor=prefetch_factor,
-          **({'collate_fn': collate} if collate else {})
+          **({"collate_fn": collate} if collate else {}),
         )
 
     self.val_dataset = yann.resolve.dataset(
@@ -587,62 +527,63 @@ class Trainer(TrainState, BaseTrainer):
     if self.val_dataset and val_subset is not None:
       self.val_dataset = yann.datasets.Subset(
         self.val_dataset,
-        *val_subset
-        if isinstance(val_subset, tuple)
-        else (val_subset,)
+        *val_subset if isinstance(val_subset, tuple) else (val_subset,),
       )
 
     self.val_transform = val_transform or transform
     if self.val_transform:
       self.val_dataset = TransformDataset(self.val_dataset, self.val_transform)
 
-    self.val_loader = val_loader or (val_dataset and self.DataLoader(
-      self.val_dataset,
-      batch_size=batch_size,
-      shuffle=False,
-      pin_memory=pin_memory,
-      num_workers=num_workers
-    ))
+    self.val_loader = val_loader or (
+      val_dataset
+      and self.DataLoader(
+        self.val_dataset,
+        batch_size=batch_size,
+        shuffle=False,
+        pin_memory=pin_memory,
+        num_workers=num_workers,
+      )
+    )
 
   def _init_optim(
-      self,
-      parameters='trainable',
-      optimizer=None,
-      lr_scheduler=None,
-      lr_batch_step=None,
-      clip_grad=None,
-      lr=None,
-      weight_decay=None,
-      momentum=None,
-      **kwargs
+    self,
+    parameters="trainable",
+    optimizer=None,
+    lr_scheduler=None,
+    lr_batch_step=None,
+    clip_grad=None,
+    lr=None,
+    weight_decay=None,
+    momentum=None,
+    **kwargs,
   ):
     parameters = parameters
-    if parameters == 'trainable' and self.model:
+    if parameters == "trainable" and self.model:
       parameters = yann.trainable(self.model.parameters())
 
     self.optimizer = yann.resolve.optimizer(
       optimizer,
       args=(parameters or self.model.parameters(),),
       kwargs={
-        k: v for k, v in dict(
-          lr=lr,
-          weight_decay=weight_decay,
-          momentum=momentum
-        ).items() if v is not None
+        k: v
+        for k, v in dict(
+          lr=lr, weight_decay=weight_decay, momentum=momentum
+        ).items()
+        if v is not None
       },
       required=False,
-      validate=lambda x: hasattr(x, 'step')
+      validate=lambda x: hasattr(x, "step"),
     )
 
     self.lr_scheduler = yann.resolve.lr_scheduler(
-      lr_scheduler,
-      kwargs=dict(optimizer=self.optimizer)
+      lr_scheduler, kwargs=dict(optimizer=self.optimizer)
     )
 
     self.lr_batch_step = lr_batch_step
 
     if clip_grad:
       from yann.optim import GradClipper
+
       if clip_grad is True:
         self.clip_grad = GradClipper(value=1)
       elif isinstance(clip_grad, dict):
@@ -650,12 +591,7 @@ class Trainer(TrainState, BaseTrainer):
       else:
         self.clip_grad = clip_grad
 
-  def _init_amp(
-      self,
-      amp=None,
-      grad_scaler=None,
-      **kwargs
-  ):
+  def _init_amp(self, amp=None, grad_scaler=None, **kwargs):
     self.amp = amp
     if grad_scaler is False:
       self.grad_scaler = None
@@ -668,29 +604,38 @@ class Trainer(TrainState, BaseTrainer):
     return self.paths.root
 
   def __setattr__(self, key, value):
-    if key == 'optimizer':
-      if hasattr(self, 'lr_scheduler') and hasattr(self.lr_scheduler,
-                                                   'optimizer'):
+    if key == "optimizer":
+      if hasattr(self, "lr_scheduler") and hasattr(
+        self.lr_scheduler, "optimizer"
+      ):
         self.lr_scheduler.optimizer = value
-    if key == 'loader':
-      if hasattr(self, 'dataset') and hasattr(value, 'dataset'):
-        super(Trainer, self).__setattr__('dataset', value.dataset)
-      if hasattr(value, 'batch_size'):
-        super(Trainer, self).__setattr__('batch_size', value.batch_size)
-    if key == 'batch_size' and hasattr(self,
-                                       'batch_size') and self.batch_size != key:
-      if hasattr(self, 'loader') and self.loader:
+    if key == "loader":
+      if hasattr(self, "dataset") and hasattr(value, "dataset"):
+        super(Trainer, self).__setattr__("dataset", value.dataset)
+      if hasattr(value, "batch_size"):
+        super(Trainer, self).__setattr__("batch_size", value.batch_size)
+    if (
+      key == "batch_size"
+      and hasattr(self, "batch_size")
+      and self.batch_size != key
+    ):
+      if hasattr(self, "loader") and self.loader:
         raise ValueError(
-          'Cannot modify batch_size because a loader is defined '
-          'and modifying batch size of a loader is not supported, '
-          'try creating and setting a new loader instead'
+          "Cannot modify batch_size because a loader is defined "
+          "and modifying batch size of a loader is not supported, "
+          "try creating and setting a new loader instead"
         )
-      if key == 'dataset' and hasattr(self, 'dataset') and self.dataset \
-          and hasattr(self, 'loader') and self.loader:
+      if (
+        key == "dataset"
+        and hasattr(self, "dataset")
+        and self.dataset
+        and hasattr(self, "loader")
+        and self.loader
+      ):
         raise ValueError(
-          'Cannot modify dataset because a loader is defined '
-          'and modifying dataset of a loader is not supported, '
-          'try creating and setting a new loader instead'
+          "Cannot modify dataset because a loader is defined "
+          "and modifying dataset of a loader is not supported, "
+          "try creating and setting a new loader instead"
         )
 
     log.debug(f"setting '{key}' to {value}")
@@ -700,14 +645,14 @@ class Trainer(TrainState, BaseTrainer):
     """
     Places model, loss and optimizer on device
     """
-    self.device = kwargs.pop('device', None) or self.device
-    self.memory_format = kwargs.pop('memory_format', None) or self.memory_format
+    self.device = kwargs.pop("device", None) or self.device
+    self.memory_format = kwargs.pop("memory_format", None) or self.memory_format
     yann.to(
       (self.model, self.loss, self.optimizer),
       device=self.device,
       memory_format=self.memory_format,
       dtype=self.dtype,
-      **kwargs
+      **kwargs,
     )
     return self
 
@@ -715,8 +660,8 @@ class Trainer(TrainState, BaseTrainer):
     """
     Places batch on device
     """
-    self.device = kwargs.pop('device', None) or self.device
-    self.memory_format = kwargs.pop('memory_format', None) or self.memory_format
+    self.device = kwargs.pop("device", None) or self.device
+    self.memory_format = kwargs.pop("memory_format", None) or self.memory_format
 
     # FIXME: find better way to handle channels last for specific entries in batch
     # possibly let memory_format take a dict or list to match batch
@@ -727,23 +672,21 @@ class Trainer(TrainState, BaseTrainer):
             batch[self.keys.inputs],
             device=self.device,
             memory_format=self.memory_format,
-            **kwargs
+            **kwargs,
           ),
-          *yann.to(batch[1:], device=self.device, **kwargs)
+          *yann.to(batch[1:], device=self.device, **kwargs),
         )
 
     return yann.to(
-      batch,
-      device=self.device,
-      memory_format=self.memory_format,
-      **kwargs
+      batch, device=self.device, memory_format=self.memory_format, **kwargs
     )
 
   def on(self, event, callback=None):
     if self.callbacks is not None:
       return self.callbacks.on(event, callback)
     log.warning(
-      '.on() callback registration ignored because callbacks are not defined')
+      ".on() callback registration ignored because callbacks are not defined"
+    )
 
   @property
   def training(self):
@@ -780,7 +723,8 @@ class Trainer(TrainState, BaseTrainer):
     method = method if isinstance(method, str) else method.__name__
     if not hasattr(self, method):
       raise AttributeError(
-        f"Can't override method '{method}' because it's not defined")
+        f"Can't override method '{method}' because it's not defined"
+      )
     if function is False:
       # assume it's used as a decorator
       # @train.override('step')
@@ -799,22 +743,16 @@ class Trainer(TrainState, BaseTrainer):
     if not self.training:
       self.train_mode()
 
-    outputs, loss = self.forward(
-      inputs=inputs,
-      targets=targets
-    )
+    outputs, loss = self.forward(inputs=inputs, targets=targets)
 
-    self.update(
-      loss=loss,
-      inputs=inputs,
-      targets=targets,
-      outputs=outputs
-    )
+    self.update(loss=loss, inputs=inputs, targets=targets, outputs=outputs)
 
     return outputs, loss
 
   def forward(self, inputs=None, targets=None):
-    with autocast(enabled=self.amp):
+    with autocast(
+      device_type=self.device.type, dtype=self.dtype, enabled=self.amp
+    ):
       outputs = self.model(inputs)
       if self.loss:
         loss = self.loss(outputs, targets)
@@ -861,16 +799,11 @@ class Trainer(TrainState, BaseTrainer):
     if loader is not None:
       with torch.inference_mode():
         for inputs, targets, outputs in yann.evaluate(
-            model=self.model,
-            batches=loader,
-            device=device
+          model=self.model, batches=loader, device=device
         ):
           if self.callbacks:
             self.callbacks.on_validation_batch(
-              inputs=inputs,
-              targets=targets,
-              outputs=outputs,
-              trainer=self
+              inputs=inputs, targets=targets, outputs=outputs, trainer=self
             )
           ts.append(targets)
           os.append(outputs)
@@ -882,10 +815,7 @@ class Trainer(TrainState, BaseTrainer):
 
     if self.callbacks:
       self.callbacks.on_validation_end(
-        targets=ts,
-        outputs=os,
-        loss=loss,
-        trainer=self
+        targets=ts, outputs=os, loss=loss, trainer=self
       )
 
     return loss
@@ -900,20 +830,14 @@ class Trainer(TrainState, BaseTrainer):
         self.callbacks.on_train_start(trainer=self)
 
         for epoch_idx in self.epochs(num=epochs):
-          self.callbacks.on_epoch_start(
-            epoch=self.num_epochs,
-            trainer=self
-          )
+          self.callbacks.on_epoch_start(epoch=self.num_epochs, trainer=self)
 
-          if self.sampler is not None and hasattr(self.sampler, 'set_epoch'):
+          if self.sampler is not None and hasattr(self.sampler, "set_epoch"):
             self.sampler.set_epoch(epoch_idx)
 
           for inputs, targets in self.batches():
             self.callbacks.on_step_start(
-              index=self.num_steps,
-              inputs=inputs,
-              targets=targets,
-              trainer=self
+              index=self.num_steps, inputs=inputs, targets=targets, trainer=self
             )
             try:
               outputs, loss = self.step(inputs=inputs, targets=targets)
@@ -922,11 +846,10 @@ class Trainer(TrainState, BaseTrainer):
               break
             except Exception as e:
               self.callbacks.on_step_error(
-                index=self.num_steps,
-                error=e,
-                trainer=self
+                index=self.num_steps, error=e, trainer=self
               )
-              if self._stop: break
+              if self._stop:
+                break
               raise e
 
             self.callbacks.on_step_end(
@@ -935,7 +858,7 @@ class Trainer(TrainState, BaseTrainer):
               targets=targets,
               outputs=outputs,
               loss=loss,
-              trainer=self
+              trainer=self,
             )
 
             if self.lr_scheduler and self.lr_batch_step:
@@ -944,15 +867,18 @@ class Trainer(TrainState, BaseTrainer):
             self.num_steps += 1
             self.num_samples += len(inputs)
 
-            if self._stop: break
-          if self._stop: break
+            if self._stop:
+              break
+          if self._stop:
+            break
 
           val_loss = self.validate() if self.val_loader else None
           if self.lr_scheduler and not self.lr_batch_step:
             self._lr_scheduler_step(
               step=epoch_idx,
-              metric=self.history.metrics.running_mean('loss')
-              if val_loss is None else val_loss,
+              metric=self.history.metrics.running_mean("loss")
+              if val_loss is None
+              else val_loss,
             )
 
           self.callbacks.on_epoch_end(epoch=epoch_idx, trainer=self)
@@ -968,8 +894,7 @@ class Trainer(TrainState, BaseTrainer):
         self.save_summary()
     else:
       for epoch_idx in self.epochs(num=epochs):
-
-        if self.sampler is not None and hasattr(self.sampler, 'set_epoch'):
+        if self.sampler is not None and hasattr(self.sampler, "set_epoch"):
           self.sampler.set_epoch(epoch_idx)
 
         for inputs, targets in self.batches():
@@ -984,8 +909,9 @@ class Trainer(TrainState, BaseTrainer):
         val_loss = self.validate() if self.val_loader else None
         self._lr_scheduler_step(
           step=epoch_idx,
-          metric=self.history.metrics.running_mean('loss')
-          if val_loss is None else val_loss
+          metric=self.history.metrics.running_mean("loss")
+          if val_loss is None
+          else val_loss,
         )
       self.update_summary()
       self.save_summary()
@@ -993,8 +919,8 @@ class Trainer(TrainState, BaseTrainer):
   def _lr_scheduler_step(self, step=None, metric=None):
     if self.lr_scheduler:
       args = dict()
-      if 'metrics' in self.lr_scheduler.step.__code__.co_varnames:
-        args['metrics'] = metric
+      if "metrics" in self.lr_scheduler.step.__code__.co_varnames:
+        args["metrics"] = metric
       # if 'epoch' in self.lr_scheduler.step.__code__.co_varnames:
       #   args['epoch'] = step
       self.lr_scheduler.step(**args)
@@ -1008,23 +934,19 @@ class Trainer(TrainState, BaseTrainer):
   def checkpoint(self, name=None) -> Path:
     state = self.state_dict()
     path = self.paths.checkpoints / (
-      f"{name}.th" if name else
-      f"{timestr()}-epoch-{self.num_epochs:03d}-steps-{self.num_steps:05d}.th"
+      f"{name}.th"
+      if name
+      else f"{timestr()}-epoch-{self.num_epochs:03d}-steps-{self.num_steps:05d}.th"
     )
     torch.save(state, str(path))
-    print(f'Saved checkpoint at {path}')
+    print(f"Saved checkpoint at {path}")
     return path
 
   def load_checkpoint(
-      self,
-      path,
-      metadata=True,
-      map_location=None,
-      strict: bool = True,
-      keys=None
+    self, path, metadata=True, map_location=None, strict: bool = True, keys=None
   ):
     # TODO: add 'latest', 'best' support
-    log.info(f'Attempting to load checkpoint {path}')
+    log.info(f"Attempting to load checkpoint {path}")
     data = torch.load(path, map_location=map_location)
     self.load_state_dict(data, metadata=metadata, strict=strict, keys=keys)
 
@@ -1042,39 +964,38 @@ class Trainer(TrainState, BaseTrainer):
         root=str(self.paths.root),
         dataset=get_dataset_name(self.dataset),
         num_steps=self.num_steps,
-        **(meta or {})
-      )
+        **(meta or {}),
+      ),
     )
 
     return path
 
-
   def state_dict(self):
     data = {
-      'metadata': {
-        'id': self.id,
-        'name': self.name,
-        'num_steps': self.num_steps,
-        'num_samples': self.num_samples,
-        'num_epochs': self.num_epochs,
-        'batch_size': self.batch_size,
-
-        'time_created': self.time_created,
-        'param_hash': hash_params(self.model)
+      "metadata": {
+        "id": self.id,
+        "name": self.name,
+        "num_steps": self.num_steps,
+        "num_samples": self.num_samples,
+        "num_epochs": self.num_epochs,
+        "batch_size": self.batch_size,
+        "time_created": self.time_created,
+        "param_hash": hash_params(self.model),
       }
     }
 
     for k, v in self.__dict__.items():
-      if hasattr(v, 'state_dict'):
+      if hasattr(v, "state_dict"):
         data[k] = {
-          'state_dict': v.state_dict(),
-          'class': fully_qualified_name(v)
+          "state_dict": v.state_dict(),
+          "class": fully_qualified_name(v),
         }
 
     return data
 
-  def load_state_dict(self, data, metadata=True, strict: bool = True,
-                      keys=None):
+  def load_state_dict(
+    self, data, metadata=True, strict: bool = True, keys=None
+  ):
     """
     TODO: add a way to specify which parts should be loaded (ex: model only)
     """
@@ -1083,74 +1004,84 @@ class Trainer(TrainState, BaseTrainer):
     from inspect import getfullargspec
 
     for k, v in data.items():
-      if keys and k not in keys: continue
-      if 'state_dict' in v and hasattr(self, k):
-
+      if keys and k not in keys:
+        continue
+      if "state_dict" in v and hasattr(self, k):
         entry = getattr(self, k)
-        if 'strict' in getfullargspec(entry.load_state_dict).args:
-          entry.load_state_dict(v['state_dict'], strict=strict)
+        if "strict" in getfullargspec(entry.load_state_dict).args:
+          entry.load_state_dict(v["state_dict"], strict=strict)
         else:
-          entry.load_state_dict(v['state_dict'])
+          entry.load_state_dict(v["state_dict"])
         log.debug(f"loaded {k}")
       else:
         log.debug(f"skipped loading {k}")
         skipped.add(k)
 
-    if metadata and 'metadata' in data:
-      skipped.discard('metadata')
-      for k, v in data['metadata'].items():
+    if metadata and "metadata" in data:
+      skipped.discard("metadata")
+      for k, v in data["metadata"].items():
         try:
           setattr(self, k, v)
         except ValueError:
-          log.warning(f'failed to set {k}')
+          log.warning(f"failed to set {k}")
 
     if skipped:
-      log.warning(f'skipped {skipped} when loading checkpoint')
+      log.warning(f"skipped {skipped} when loading checkpoint")
 
   def _get_env(self):
     return yann.utils.env_info()
 
   def update_summary(self):
-    self.summary.update(dict(
-      id=self.id,
-      name=self.name,
-      path=str(self.paths.root),
-      num_steps=self.num_steps,
-      num_samples=self.num_samples,
-      num_epochs=self.num_epochs,
-      batch_size=self.batch_size,
-      device=str(self.device),
-      time_created=self.time_created,
-      # params={k: str(v) for k, v in self.params.items()},
-    ))
-
-    if 'env' not in self.summary:
-      self.summary['env'] = self._get_env()
+    self.summary.update(
+      dict(
+        id=self.id,
+        name=self.name,
+        path=str(self.paths.root),
+        num_steps=self.num_steps,
+        num_samples=self.num_samples,
+        num_epochs=self.num_epochs,
+        batch_size=self.batch_size,
+        device=str(self.device),
+        time_created=self.time_created,
+        # params={k: str(v) for k, v in self.params.items()},
+      )
+    )
+
+    if "env" not in self.summary:
+      self.summary["env"] = self._get_env()
 
     if self.dataset:
-      if 'dataset' not in self.summary:
-        self.summary['dataset'] = {}
-      self.summary['dataset'].update(dict(
-        name=get_dataset_name(self.dataset),
-        size=len(self.dataset),
-        num_classes=len(self.dataset.classes) if hasattr(self.dataset,
-                                                         'classes') else None
-      ))
+      if "dataset" not in self.summary:
+        self.summary["dataset"] = {}
+      self.summary["dataset"].update(
+        dict(
+          name=get_dataset_name(self.dataset),
+          size=len(self.dataset),
+          num_classes=len(self.dataset.classes)
+          if hasattr(self.dataset, "classes")
+          else None,
+        )
+      )
     if self.model:
-      if 'model' not in self.summary:
-        self.summary['model'] = {}
-      self.summary['model'].update(dict(
-        name=yann.get_model_name(self.model),
-        param_count=yann.param_count(self.model),
-        trainable_param_count=yann.param_count(
-          yann.trainable(self.model.parameters()))
-      ))
+      if "model" not in self.summary:
+        self.summary["model"] = {}
+      self.summary["model"].update(
+        dict(
+          name=yann.get_model_name(self.model),
+          param_count=yann.param_count(self.model),
+          trainable_param_count=yann.param_count(
+            yann.trainable(self.model.parameters())
+          ),
+        )
+      )
 
   def save_summary(self):
     try:
       yann.save(self.summary, self.paths.summary)
     except Exception as e:
-      log.warning(f'Failed to save summary, most likely due to unserializable params, {e}')
+      log.warning(
+        f"Failed to save summary, most likely due to unserializable params, {e}"
+      )
     return self.paths.summary
 
   def __str__(self):
diff --git a/yann/transforms/__init__.py b/yann/transforms/__init__.py
index 6b68d92..de89de4 100644
--- a/yann/transforms/__init__.py
+++ b/yann/transforms/__init__.py
@@ -1,16 +1,23 @@
 import base64
-import os
-from typing import Protocol, Any
 import io
-import numpy as np
+import os
 import pathlib
-import torch
 import random
+from typing import Any, Protocol
+
+import numpy as np
+import torch
 from PIL import Image
-from timm.data.mixup import rand_bbox
+from torchvision import transforms
 from torchvision import transforms as tvt
 from torchvision.transforms.functional import to_pil_image
-from torchvision import transforms
+
+try:
+  from timm.data.mixup import rand_bbox
+except ImportError:
+  HAS_TIMM = False
+else:
+  HAS_TIMM = True
 
 from ..utils import truthy
 
@@ -21,17 +28,11 @@ class Transform(Protocol):
 
 
 class FittableTransform(Transform):
-  def fit(self, x: Any):
-    ...
-
-  def transform(self, x: Any) -> Any:
-    ...
-
-  def fit_transform(self, x: Any) -> Any:
-    ...
-
+  def fit(self, x: Any): ...
 
+  def transform(self, x: Any) -> Any: ...
 
+  def fit_transform(self, x: Any) -> Any: ...
 
 
 class Transforms:
@@ -62,36 +63,34 @@ class Transforms:
       f"  load={str(self.load)}\n"
       f"  transform={str(self.transform)}\n"
       f"  to_tensor={str(self.to_tensor)}\n"
-      ")")
-
-
-
+      ")"
+    )
 
 
- # for backwards compatibility after rename, to avoid confusion with "Transformers"
+# for backwards compatibility after rename, to avoid confusion with "Transformers"
 Transformer = Transforms
 
 
 class ImageTransforms(Transforms):
   def __init__(
-      self,
-      load=None,
-      resize=None,
-      rotate=None,
-      crop=None,
-      warp=None,
-      mirror=None,
-      mean=None,
-      std=None,
-      color_jitter=None,
-      interpolation=None,
-      color_space=None,
-      transform=None,
-      to_tensor=None,
-      autoaugment=None,
-      randaugment=None,
-      trivialaugment=None,
-      erase=None
+    self,
+    load=None,
+    resize=None,
+    rotate=None,
+    crop=None,
+    warp=None,
+    mirror=None,
+    mean=None,
+    std=None,
+    color_jitter=None,
+    interpolation=None,
+    color_space=None,
+    transform=None,
+    to_tensor=None,
+    autoaugment=None,
+    randaugment=None,
+    trivialaugment=None,
+    erase=None,
   ):
     interpolation = interpolation or Image.ANTIALIAS
     self.resize = resize and tvt.Resize(resize, interpolation=interpolation)
@@ -102,10 +101,11 @@ class ImageTransforms(Transforms):
       else tvt.CenterCrop(crop)
     )
     self.mirror = mirror and tvt.RandomHorizontalFlip(
-      .5 if mirror is True else mirror)
+      0.5 if mirror is True else mirror
+    )
 
     if color_jitter is True:
-      color_jitter = (.4, .2, .1, .05)
+      color_jitter = (0.4, 0.2, 0.1, 0.05)
     self.color_jitter = color_jitter and tvt.ColorJitter(*color_jitter)
 
     self.normalize = (mean or std) and tvt.Normalize(mean=mean, std=std)
@@ -120,25 +120,25 @@ class ImageTransforms(Transforms):
     self.randaugment = randaugment and tvt.RandAugment()
     self.trivialaugment = trivialaugment and tvt.TrivialAugmentWide()
 
-
     super().__init__(
       load=load or GetImage(color_space),
-      transform=tvt.Compose(truthy([
-        self.resize,
-        transform,
-        self.autoagument,
-        self.randaugment,
-        self.trivialaugment,
-        self.rotate,
-        self.crop,
-        self.mirror,
-        self.color_jitter,
-      ])),
-      to_tensor=to_tensor or tvt.Compose(truthy([
-        tvt.ToTensor(),
-        self.normalize,
-        self.erase
-      ]))
+      transform=tvt.Compose(
+        truthy(
+          [
+            self.resize,
+            transform,
+            self.autoagument,
+            self.randaugment,
+            self.trivialaugment,
+            self.rotate,
+            self.crop,
+            self.mirror,
+            self.color_jitter,
+          ]
+        )
+      ),
+      to_tensor=to_tensor
+      or tvt.Compose(truthy([tvt.ToTensor(), self.normalize, self.erase])),
     )
 
   def state_dict(self):
@@ -148,7 +148,7 @@ class ImageTransforms(Transforms):
     pass
 
 
- # for backwards compatibility after rename, to avoid confusion with "Transformers"
+# for backwards compatibility after rename, to avoid confusion with "Transformers"
 ImageTransformer = ImageTransforms
 
 
@@ -171,7 +171,6 @@ class BatchTransforms:
     return [self.transform(x) for x in items]
 
 
-
 class GetImage:
   def __init__(self, space=None):
     self.color_space = space
@@ -197,12 +196,13 @@ def get_image(x, space=None) -> Image.Image:
     return img.convert(space) if space else img
 
   if isinstance(x, str):
-    if x.startswith('http') or x.startswith('www.'):
+    if x.startswith("http") or x.startswith("www."):
       import requests
+
       x = requests.get(x).content
-    elif x.startswith('data') and 'base64,' in x:
+    elif x.startswith("data") and "base64," in x:
       # data header for base64 encoded
-      x = x.split('base64,')[1]
+      x = x.split("base64,")[1]
       x = base64.b64decode(x)
     elif len(x) > 1024:
       # assume no header base 64 image
@@ -211,7 +211,7 @@ def get_image(x, space=None) -> Image.Image:
       except:
         pass
 
-  if hasattr(x, 'read'):
+  if hasattr(x, "read"):
     img = Image.open(io.BytesIO(x.read()))
     return img.convert(space) if space else img
 
@@ -233,7 +233,7 @@ def mixup(inputs, targets, alpha=1):
   fraction = np.random.beta(alpha, alpha)
   return (
     fraction * inputs + (1 - fraction) * inputs[shuffled_indices],
-    fraction * targets + (1 - fraction) * targets[shuffled_indices]
+    fraction * targets + (1 - fraction) * targets[shuffled_indices],
   )
 
 
@@ -245,7 +245,7 @@ class Mixup:
     return mixup(inputs=inputs, targets=targets, alpha=self.alpha)
 
 
-def cutout(img, percent=.3, value=0):
+def cutout(img, percent=0.3, value=0):
   pil_img = False
   if isinstance(img, Image.Image):
     img = np.array(img)
@@ -258,33 +258,37 @@ def cutout(img, percent=.3, value=0):
   start_h = random.randint(0, (height - mask_height))
   start_w = random.randint(0, (width - mask_width))
 
-  img[start_h:start_h + mask_height, start_w:start_w + mask_width] = value
+  img[start_h : start_h + mask_height, start_w : start_w + mask_width] = value
   return Image.fromarray(img) if pil_img else img
 
 
-def cutmix(inputs, targets, beta):
-  lam = np.random.beta(beta, beta)
-  rand_index = torch.randperm(inputs.size()[0]).cuda()
-  target_a = targets
-  target_b = targets[rand_index]
-  bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)
-  inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]
-  # adjust lambda to exactly match pixel ratio
-  lam = 1 - (
-        (bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))
-
+if HAS_TIMM:
+
+  def cutmix(inputs, targets, beta):
+    lam = np.random.beta(beta, beta)
+    rand_index = torch.randperm(inputs.size()[0]).cuda()
+    target_a = targets
+    target_b = targets[rand_index]
+    bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)
+    inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[
+      rand_index, :, bbx1:bbx2, bby1:bby2
+    ]
+    # adjust lambda to exactly match pixel ratio
+    lam = 1 - (
+      (bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2])
+    )
 
 
 def get_imagenet_transforms(
-    size=224,
-    crop_scale=(.5, 1.2),
-    val_size=None,
-    resize=256,
-    fixres=False,
-    trivial=False,
-
+  size=224,
+  crop_scale=(0.5, 1.2),
+  val_size=None,
+  resize=256,
+  fixres=False,
+  trivial=False,
 ):
-  augment = transforms.Compose([
+  augment = transforms.Compose(
+    [
       transforms.RandomResizedCrop(
         size,
         scale=crop_scale,
@@ -294,32 +298,39 @@ def get_imagenet_transforms(
       #     .3, .3, .3
       # ),
       transforms.RandomHorizontalFlip(),
-    ])
+    ]
+  )
 
   train_transform = Transforms(
-    load=GetImage('RGB'),
+    load=GetImage("RGB"),
     transform=augment,
-    to_tensor=transforms.Compose([
-      transforms.ToTensor(),
-      # transforms.RandomErasing(),
-      transforms.Normalize(
-          mean=[0.485, 0.456, 0.406],
-          std=[0.229, 0.224, 0.225])
-    ])
+    to_tensor=transforms.Compose(
+      [
+        transforms.ToTensor(),
+        # transforms.RandomErasing(),
+        transforms.Normalize(
+          mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
+        ),
+      ]
+    ),
   )
 
   test_transform = Transforms(
     load=train_transform.load,
-    transform=transforms.Compose([
-      transforms.Resize(val_size or size, interpolation=Image.ANTIALIAS),
-      transforms.CenterCrop(val_size or size)
-    ]),
-    to_tensor=transforms.Compose([
-      transforms.ToTensor(),
-      transforms.Normalize(
-          mean=[0.485, 0.456, 0.406],
-          std=[0.229, 0.224, 0.225])
-    ])
+    transform=transforms.Compose(
+      [
+        transforms.Resize(val_size or size, interpolation=Image.ANTIALIAS),
+        transforms.CenterCrop(val_size or size),
+      ]
+    ),
+    to_tensor=transforms.Compose(
+      [
+        transforms.ToTensor(),
+        transforms.Normalize(
+          mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
+        ),
+      ]
+    ),
   )
 
   return train_transform, test_transform
diff --git a/yann/typedefs.py b/yann/typedefs.py
index ffab150..9a53ad8 100644
--- a/yann/typedefs.py
+++ b/yann/typedefs.py
@@ -1,6 +1,6 @@
-import torch
 import typing
 
+import torch
 
 Logits = torch.FloatTensor
 Embedding = torch.FloatTensor
@@ -9,10 +9,9 @@ OneHot = torch.Tensor
 Probabilities = torch.FloatTensor
 LogProbabilities = torch.FloatTensor
 
-Batch = typing.NewType('Batch', torch.Tensor)
+Batch = typing.NewType("Batch", torch.Tensor)
 
 ClassIndices = torch.LongTensor
-OneHot = torch.FloatTensor
 MultiLabelOneHot = OneHot
 
 ImageTensor = torch.ShortTensor
diff --git a/yann/utils/__init__.py b/yann/utils/__init__.py
index 04a4fbe..babd1b1 100644
--- a/yann/utils/__init__.py
+++ b/yann/utils/__init__.py
@@ -1,13 +1,14 @@
 import argparse
+import datetime
 import re
 import sys
-from typing import Union, Optional, Dict, Any, TYPE_CHECKING
 import typing
 from contextlib import contextmanager
+from typing import TYPE_CHECKING, Any, Dict, Optional, Union
+
 import numpy as np
 import torch
 from PIL import Image
-import datetime
 
 if TYPE_CHECKING:
   import yann
@@ -16,12 +17,13 @@ from .ids import memorable_id
 
 
 def env_info():
-  import sys
   import os
   import socket
-  from .bash import git_hash
+  import sys
+
   import yann
 
+  from .bash import git_hash
 
   try:
     gith = git_hash()
@@ -32,23 +34,20 @@ def env_info():
     cwd=os.getcwd(),
     arguments=sys.argv,
     git_hash=gith,
-    python=dict(
-      executable=sys.executable,
-      version=sys.version,
-      path=sys.path
-    ),
+    python=dict(executable=sys.executable, version=sys.version, path=sys.path),
     torch_version=torch.__version__,
     yann_version=yann.__version__,
-    hostname=socket.gethostname()
+    hostname=socket.gethostname(),
   )
 
+
 def timestr(d=None):
   return f"{(d or datetime.datetime.utcnow()).strftime('%y-%m-%dT%H:%M:%S')}"
 
 
 def camel_to_snake(text):
-  s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', text)
-  return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
+  s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", text)
+  return re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1).lower()
 
 
 def abbreviate(text):
@@ -58,18 +57,18 @@ def abbreviate(text):
 def str2bool(v):
   if isinstance(v, bool):
     return v
-  if v.lower() in ('yes', 'true', 't', 'y', '1'):
+  if v.lower() in ("yes", "true", "t", "y", "1"):
     return True
-  elif v.lower() in ('no', 'false', 'f', 'n', '0'):
+  elif v.lower() in ("no", "false", "f", "n", "0"):
     return False
   else:
     import argparse
-    raise argparse.ArgumentTypeError('Boolean value expected.')
 
+    raise argparse.ArgumentTypeError("Boolean value expected.")
 
 
 def supports_primitive_types(t: type, types=(bool, str, int, float)):
-  if hasattr(t, '__args__'):  # handle typing.Union, typing.Optional
+  if hasattr(t, "__args__"):  # handle typing.Union, typing.Optional
     if not any(x in t.__args__ for x in types):
       return False
   elif t not in types:
@@ -77,35 +76,42 @@ def supports_primitive_types(t: type, types=(bool, str, int, float)):
 
   return True
 
+
 def get_primitive_type(t: type, types=(bool, str, int, float)):
   if t in types:
     return t
-  if hasattr(t, '__args__'):  # handle typing.Union
+  if hasattr(t, "__args__"):  # handle typing.Union
     for x in types:
       if x in t.__args__:
         return x
 
+
 def get_arg_parser(
-    x: Union[Dict[str, Dict[str, Any],], Dict[str, 'yann.params.Field']],
-    description=None,
-    epilog=None,
-    parser: Optional[argparse.ArgumentParser]=None,
-    **kwargs
+  x: Union[
+    Dict[
+      str,
+      Dict[str, Any],
+    ],
+    Dict[str, "yann.params.Field"],
+  ],
+  description=None,
+  epilog=None,
+  parser: Optional[argparse.ArgumentParser] = None,
+  **kwargs,
 ):
   from ..params import Field
+
   parser = parser or argparse.ArgumentParser(
-    description=description,
-    epilog=epilog,
-    **kwargs
+    description=description, epilog=epilog, **kwargs
   )
 
-  abbreviations = {'h'}
+  abbreviations = {"h"}
 
   v: Union[Dict, Field]
   for k, v in x.items():
     # TODO: add support for typing.Literal and typing.Sequence
 
-    T = v.get('type') if isinstance(v, dict) else v.type
+    T = v.get("type") if isinstance(v, dict) else v.type
 
     if not supports_primitive_types(T):
       continue  # skip complex types that are not handled by argparse
@@ -122,13 +128,13 @@ def get_arg_parser(
     if isinstance(v, dict):
       parser.add_argument(
         *names,
-        default=v.get('default'),
+        default=v.get("default"),
         type=str2bool if prim_type is bool else prim_type,
-        action=v.get('action'),
-        help=v.get('help'),
-        required=v.get('required'),
-        choices=v.get('choices'),
-        dest=v.get('dest')
+        action=v.get("action"),
+        help=v.get("help"),
+        required=v.get("required"),
+        choices=v.get("choices"),
+        dest=v.get("dest"),
       )
     elif isinstance(v, Field):
       parser.add_argument(
@@ -137,7 +143,7 @@ def get_arg_parser(
         type=str2bool if prim_type is bool else prim_type,
         help=f"{v.help or k} (default: {v.default}, type: {prim_type.__name__})",
         required=v.required,
-        choices=getattr(v, 'choices', None),
+        choices=getattr(v, "choices", None),
       )
     else:
       parser.add_argument(*names, default=v, type=type(v))
@@ -161,7 +167,7 @@ def equal(t1: torch.Tensor, t2: torch.Tensor):
   return torch.all(t1 == t2)
 
 
-def randimg(*shape, dtype='uint8'):
+def randimg(*shape, dtype="uint8"):
   return Image.fromarray((np.random.rand(*shape) * 255).astype(dtype))
 
 
@@ -200,7 +206,7 @@ def to_numpy(x):
   if isinstance(x, np.ndarray):
     return x
   if torch.is_tensor(x):
-    return x.to('cpu').detach().numpy()
+    return x.to("cpu").detach().numpy()
   return np.array(x)
 
 
@@ -227,7 +233,7 @@ class RangeMap(dict):
           continue
         return value
 
-    raise KeyError(f'Key `{item}` does not fall in any of the ranges')
+    raise KeyError(f"Key `{item}` does not fall in any of the ranges")
 
   def __call__(self, item):
     return self[item]
@@ -235,29 +241,30 @@ class RangeMap(dict):
 
 def pretty_size(bytes):
   num = bytes
-  for unit in ['bytes', 'KB', 'MB', 'GB', 'TB']:
+  for unit in ["bytes", "KB", "MB", "GB", "TB"]:
     if num < 1024.0:
-      return f'{num:3.1f} {unit}'
+      return f"{num:3.1f} {unit}"
     num /= 1024.0
 
 
 def print_tree(root, indent=2, depth=None, filter=None):
-  from pathlib import Path
   from datetime import datetime
+  from pathlib import Path
+
   root = Path(root)
-  for path in sorted((root, *root.rglob('*'))):
+  for path in sorted((root, *root.rglob("*"))):
     d = len(path.relative_to(root).parts)
     if depth and depth < d:
       continue
     if not path.is_dir() and filter and not path.match(filter):
       continue
     if path.is_dir():
-      print(f'{" " * (d * indent)} /{path.name}/')
+      print(f"{' ' * (d * indent)} /{path.name}/")
     else:
       print(
-        f'{" " * (d * indent)}  - {path.name:25} '
-        f'{f"({pretty_size(path.stat().st_size)})":15} '
-        f'{datetime.fromtimestamp(path.stat().st_mtime)}'
+        f"{' ' * (d * indent)}  - {path.name:25} "
+        f"{f'({pretty_size(path.stat().st_size)})':15} "
+        f"{datetime.fromtimestamp(path.stat().st_mtime)}"
       )
 
 
@@ -266,11 +273,12 @@ def fully_qualified_name(x):
   if module is None or module == str.__class__.__module__:
     return x.__class__.__name__
   else:
-    return f'{module}.{x.__class__.__name__}'
+    return f"{module}.{x.__class__.__name__}"
 
 
 def hash_params(module):
   from hashlib import sha1
+
   s = sha1()
   for p in module.parameters():
     s.update(to_numpy(p).tobytes())
@@ -284,14 +292,14 @@ def dynamic_import(qualified_name: str):
     qualified_name: fully qualified name (ex: `torch.nn.Linear`)
   """
   import importlib
-  module_name, obj_name = qualified_name.rsplit('.', maxsplit=1)
+
+  module_name, obj_name = qualified_name.rsplit(".", maxsplit=1)
   module = importlib.import_module(module_name)
   return getattr(module, obj_name)
 
 
 def source_file_import(
-    path: Union[str, 'pathlib.Path'],
-    module_name: Optional[str] = None
+  path: Union[str, "pathlib.Path"], module_name: Optional[str] = None
 ) -> "types.ModuleType":
   """
   Import python module from a source file
@@ -306,7 +314,8 @@ def source_file_import(
 
   if module_name is None:
     from pathlib import Path
-    module_name = Path(path).stem.replace('-', '_')
+
+    module_name = Path(path).stem.replace("-", "_")
 
   spec = importlib.util.spec_from_file_location(module_name, str(path))
   module = importlib.util.module_from_spec(spec)
@@ -325,6 +334,7 @@ def source_string_import(code: str, module_name: str) -> "types.ModuleType":
     imported module
   """
   import importlib.util
+
   spec = importlib.util.spec_from_loader(module_name, loader=None)
   module = importlib.util.module_from_spec(spec)
   exec(code, module.__dict__)
@@ -334,9 +344,9 @@ def source_string_import(code: str, module_name: str) -> "types.ModuleType":
 def is_notebook() -> bool:
   try:
     shell = get_ipython().__class__.__name__
-    if shell == 'ZMQInteractiveShell':
+    if shell == "ZMQInteractiveShell":
       return True
-    elif shell == 'TerminalInteractiveShell':
+    elif shell == "TerminalInteractiveShell":
       return False
     else:
       return False
@@ -344,10 +354,10 @@ def is_notebook() -> bool:
     return False
 
 
-
 @contextmanager
-def timeout(seconds, message='Exceeded time'):
+def timeout(seconds, message="Exceeded time"):
   import signal
+
   def error():
     raise TimeoutError(message)
 
@@ -365,9 +375,6 @@ def apply_known(function: typing.Callable, arguments: dict):
     that are defined in the signature
   """
   import inspect
+
   sig = inspect.signature(function)
-  return function(
-    **{k: arguments[k]
-       for k in sig.parameters
-       if k in arguments
-       })
\ No newline at end of file
+  return function(**{k: arguments[k] for k in sig.parameters if k in arguments})
diff --git a/yann/utils/bash.py b/yann/utils/bash.py
index 70e8da7..b00fb89 100644
--- a/yann/utils/bash.py
+++ b/yann/utils/bash.py
@@ -1,44 +1,44 @@
 import subprocess
 
+
 def run(command):
   out = subprocess.check_output(command, shell=True)
   if out:
-    return out.decode('utf-8').strip()
+    return out.decode("utf-8").strip()
 
 
 def git_hash():
-  return run('git rev-parse HEAD')
+  return run("git rev-parse HEAD")
 
 
-def git_commit(files='.', message='automated commit', branch=None):
+def git_commit(files=".", message="automated commit", branch=None):
   if branch:
     try:
-      run(f'git checkout {branch}')
+      run(f"git checkout {branch}")
     except subprocess.SubprocessError:
-      run(f'git checkout -b {branch}')
+      run(f"git checkout -b {branch}")
   if isinstance(files, str):
-    run(f'git add {files}')
+    run(f"git add {files}")
   else:
-    run(['git', 'add', *files])
-  run(f'git commit -m {message}')
+    run(["git", "add", *files])
+  run(f"git commit -m {message}")
 
 
 def git_diff():
-  return run('git diff')
+  return run("git diff")
 
 
 def shutdown_computer():
-  return run('sudo shutdown -h now')
+  return run("sudo shutdown -h now")
 
 
 def nvidia_smi():
-  return run('nvidia-smi')
+  return run("nvidia-smi")
 
 
 def pip_freeze():
-  return run('pip freeze')
+  return run("pip freeze")
 
 
 def conda_list(explicit=False):
-  return run(f'conda list {"--explicit" if explicit else ""}')
-
+  return run(f"conda list {'--explicit' if explicit else ''}")
diff --git a/yann/utils/debug.py b/yann/utils/debug.py
index 8bbfc9b..e8fd997 100644
--- a/yann/utils/debug.py
+++ b/yann/utils/debug.py
@@ -1,13 +1,14 @@
-import torch
 import gc
+
 import numpy as np
+import torch
 from torch import nn
 
 
 def iter_allocated_tensors():
   for x in gc.get_objects():
     try:
-      if torch.is_tensor(x) or (hasattr(x, 'data') and torch.is_tensor(x.data)):
+      if torch.is_tensor(x) or (hasattr(x, "data") and torch.is_tensor(x.data)):
         yield x
     except:
       pass
diff --git a/yann/utils/decorators.py b/yann/utils/decorators.py
index 9e05af7..8676278 100644
--- a/yann/utils/decorators.py
+++ b/yann/utils/decorators.py
@@ -1,7 +1,8 @@
 from functools import wraps
 
+
 class lazy:
-  __slots__ = 'method', 'name'
+  __slots__ = "method", "name"
 
   def __init__(self, method):
     self.method = method
@@ -34,6 +35,7 @@ def robust(x=None, exceptions=Exception, default=None):
   if callable(x):
     return RobustFunction(func=x, exceptions=exceptions, default=default)
   else:
+
     def decorator(x):
       return RobustFunction(func=x, exceptions=exceptions, default=default)
 
@@ -58,7 +60,8 @@ def track(x=None, sanitize=None):
   if callable(x):
     return FunctionTracker(func=x)
   else:
+
     def decorator(x):
       return FunctionTracker(func=x, sanitize=sanitize)
 
-    return decorator
\ No newline at end of file
+    return decorator
diff --git a/yann/utils/ids.py b/yann/utils/ids.py
index 3237e67..778af9c 100644
--- a/yann/utils/ids.py
+++ b/yann/utils/ids.py
@@ -4050,5 +4050,6 @@ nouns = [
   "sent",
 ]
 
+
 def memorable_id():
   return f"{random.choice(adjectives)}-{random.choice(adjectives)}-{random.choice(nouns)}"
diff --git a/yann/utils/profile.py b/yann/utils/profile.py
index d0e5ea3..5bceb61 100644
--- a/yann/utils/profile.py
+++ b/yann/utils/profile.py
@@ -1,28 +1,37 @@
-from torch.autograd.profiler import profile
-from typing import Union, Tuple, Callable
+from typing import Callable, Tuple, Union
+
 import torch
+from torch.autograd.profiler import profile
+
 from .timer import Timer
 
 # TODO:
 # - https://github.com/pytorch/pytorch/issues/3749#issuecomment-374006211
 # - https://pytorch.org/docs/stable/bottleneck.html
 
+
 def param_count(model):
-  return sum(p.numel() for p in (model.parameters() if isinstance(model, torch.nn.Module) else model))
+  return sum(
+    p.numel()
+    for p in (
+      model.parameters() if isinstance(model, torch.nn.Module) else model
+    )
+  )
 
 
 def profile_module(
-    module,
-    input,
-    warmup=10,
-    iterations=20,
-    sync=True,
-    timer=None,
-    task_name='iteration',
-    jit=False
+  module,
+  input,
+  warmup=10,
+  iterations=20,
+  sync=True,
+  timer=None,
+  task_name="iteration",
+  jit=False,
 ):
   if jit:
     import torch.jit
+
     module = torch.jit.trace(module, input)
 
   for n in range(warmup):
diff --git a/yann/utils/tensor.py b/yann/utils/tensor.py
index 9182eba..79841e5 100644
--- a/yann/utils/tensor.py
+++ b/yann/utils/tensor.py
@@ -1,56 +1,111 @@
 import torch
+import dataclasses
+from typing import Any, Union
+
 
 def weighted_sum(tensors, weights):
   if len(tensors) < 2:
-    raise ValueError('must pass at least 2 tensors')
+    raise ValueError("must pass at least 2 tensors")
   s = tensors[0] * weights[0]
   for t, w in zip(tensors[1:], weights[1:]):
     s.add_(w, t)
   return s
 
 
-def one_hot(targets: torch.Tensor, num_classes=None, device=None, dtype=None, normalize=False):
+def one_hot(
+  targets: torch.Tensor,
+  num_classes=None,
+  device=None,
+  dtype=None,
+  normalize=False,
+):
   if torch.is_tensor(targets):
     if len(targets.shape) == 1:
       num = targets.shape[0]
-      hot = torch.zeros(num, num_classes, device=device or targets.device, dtype=dtype)
+      hot = torch.zeros(
+        num, num_classes, device=device or targets.device, dtype=dtype
+      )
       hot.scatter_(1, targets.unsqueeze(1), 1.0)
       return hot
     elif len(targets.shape) == 2:
       pass
 
-    raise ValueError('only dim 1 tensors supported')
-
+    raise ValueError("only dim 1 tensors supported")
 
 
 def show_hist(hist):
-  chars = ' '
+  chars = "  "
   top = max(hist)
   step = (top / float(len(chars) - 1)) or 1
-  return ''.join(chars[int(round(count / step))] for count in hist)
-
-
-def describe(tensor: torch.Tensor, bins=10) -> str:
-  try:
-    stats = (
-      f"mean: {tensor.mean():.4f} std: {tensor.std():.4f} "
-    )
-  except:
-    stats = ''
-
-  try:
-    h = tensor.histc(bins=bins).int().tolist()
-    hist = (
-
-      f"hist: {show_hist(h)}\n"
-      f"      {h}\n"
-    )
-  except:
-    hist = ''
-  return f"""
-shape: {tuple(tensor.shape)} dtype: {tensor.dtype} device: {tensor.device} grad: {tensor.requires_grad} size: {tensor.numel() * tensor.element_size() / (1e6):,.5f} MB
-min: {tensor.min():.4f}  max: {tensor.max():.4f}  {stats}sum: {tensor.sum():.4f}
-{hist}
-
-{tensor}
-  """
\ No newline at end of file
+  return "".join(chars[int(round(count / step))] for count in hist)
+
+
+def describe_tensor(tensor: torch.Tensor, bins=10, indent: str = "") -> str:
+    """Describes a single tensor."""
+    try:
+        stats = f"mean: {tensor.mean():.4f} std: {tensor.std():.4f} "
+    except RuntimeError:  # Handle cases like boolean tensors where mean/std are not defined
+        stats = ""
+    try:
+        min_val = tensor.min()
+        max_val = tensor.max()
+        sum_val = tensor.sum()
+        min_max_sum = f"min: {min_val:.4f}  max: {max_val:.4f}  sum: {sum_val:.4f}"
+    except RuntimeError: # Handle non-numeric types
+         min_max_sum = ""
+
+    try:
+        # Only compute histogram for float/int tensors
+        is_float = tensor.is_floating_point()
+        is_complex = torch.is_complex(tensor)
+        is_signed_int = tensor.dtype in (torch.int8, torch.int16, torch.int32, torch.int64)
+        is_unsigned_int = tensor.dtype == torch.uint8
+
+        if is_float or is_complex or is_signed_int or is_unsigned_int:
+            # Ensure tensor is float for histc
+            float_tensor = tensor.float() if not (is_float or is_complex) else tensor
+            h = float_tensor.histc(bins=bins).int().tolist()
+            hist = f"hist: {show_hist(h)}\n{indent}      {h}"
+        else:
+            hist = "hist: (not applicable for this dtype)"
+    except RuntimeError as e:
+        hist = f"hist: (error: {e})"
+
+    # Limit tensor display for large tensors
+    tensor_str = str(tensor)
+    if len(tensor_str) > 1000:
+         tensor_str = tensor_str[:500] + "\n... (tensor truncated) ...\n" + tensor_str[-500:]
+
+
+    return f"""{indent}shape: {tuple(tensor.shape)} dtype: {tensor.dtype} device: {tensor.device} grad: {tensor.requires_grad} size: {tensor.numel() * tensor.element_size() / (1e6):,.5f} MB
+{indent}{min_max_sum}  {stats}
+{indent}{hist}
+
+{indent}{tensor_str}
+"""
+
+def describe(data: Any, bins=10, indent: str = "") -> str:
+    """
+    Recursively describes tensors found in nested structures like lists, tuples, dicts, and dataclasses.
+    """
+    if torch.is_tensor(data):
+        return describe_tensor(data, bins, indent)
+    elif isinstance(data, dict):
+        items_desc = []
+        for k, v in data.items():
+            items_desc.append(f"{indent}  '{k}':\n{describe(v, bins, indent + '    ')}")
+        return f"{indent}{{\n" + "\n".join(items_desc) + f"\n{indent}}}"
+    elif isinstance(data, (list, tuple)):
+        is_list = isinstance(data, list)
+        items_desc = [describe(item, bins, indent + '  ') for item in data]
+        start, end = ('[', ']') if is_list else ('(', ')')
+        return f"{indent}{start}\n" + "\n".join(items_desc) + f"\n{indent}{end}"
+    elif dataclasses.is_dataclass(data) and not isinstance(data, type):
+        items_desc = []
+        for field in dataclasses.fields(data):
+            value = getattr(data, field.name)
+            items_desc.append(f"{indent}  {field.name}:\n{describe(value, bins, indent + '    ')}")
+        return f"{indent}{data.__class__.__name__}(\n" + "\n".join(items_desc) + f"\n{indent})"
+    else:
+        # For other types, just return their string representation
+        return f"{indent}{str(data)}"
diff --git a/yann/utils/timer.py b/yann/utils/timer.py
index 64024d3..b3ab036 100644
--- a/yann/utils/timer.py
+++ b/yann/utils/timer.py
@@ -1,18 +1,22 @@
 from collections import OrderedDict, defaultdict
-from contextlib import contextmanager, ContextDecorator
-
+from contextlib import ContextDecorator, contextmanager
 from datetime import datetime
+
 import torch.cuda
 
 from ..viz.plot import plot_timeline
 
+
 def time(name=None, sync=False):
   return Task(name=name, sync=sync, log=True)
 
+
 class Task(ContextDecorator):
-  __slots__ = ('name', 'start_time', 'end_time', 'meta', 'sync', 'log')
+  __slots__ = ("name", "start_time", "end_time", "meta", "sync", "log")
 
-  def __init__(self, name=None, start=None, end=None, meta=None, sync=False, log=False):
+  def __init__(
+    self, name=None, start=None, end=None, meta=None, sync=False, log=False
+  ):
     self.name = name
     self.start_time = start
     self.end_time = end
@@ -29,7 +33,7 @@ class Task(ContextDecorator):
 
     self.start_time = time or datetime.now()
     if self.log:
-      print(f'starting {self.name or id(self)}')
+      print(f"starting {self.name or id(self)}")
 
   def end(self, time=None, meta=None, sync=None):
     sync = sync if sync is not None else self.sync
@@ -38,7 +42,7 @@ class Task(ContextDecorator):
 
     self.end_time = time or datetime.now()
     if self.log:
-      print(f'completed {self.name or id(self)} in {self.seconds:.9g} seconds')
+      print(f"completed {self.name or id(self)} in {self.seconds:.9g} seconds")
 
     if meta:
       self.meta.update(meta)
@@ -65,6 +69,7 @@ class Task(ContextDecorator):
   def __repr__(self):
     return f"Task({self.name or id(self)}, seconds={self.seconds:.9g}, sync={self.sync})"
 
+
 class Timer:
   def __init__(self, name=None, log=False):
     self.tasks = []
@@ -75,10 +80,13 @@ class Timer:
 
   def start(self, name, sync=True, **meta):
     task = self.task(name, sync=sync, **meta)
-    if self.log: print('Started', name)
+    if self.log:
+      print("Started", name)
 
     if task in self.active_tasks:
-      raise ValueError(f'Nesting tasks is not allowed, "{name}" was already started and not finished')
+      raise ValueError(
+        f'Nesting tasks is not allowed, "{name}" was already started and not finished'
+      )
     self.active_tasks[name] = task
 
   def end(self, name, sync=True, **meta):
@@ -88,7 +96,7 @@ class Timer:
     task.end(sync=sync, meta=meta)
 
     if self.log:
-      print('Ended', task.name, ', took', task.seconds, 'seconds')
+      print("Ended", task.name, ", took", task.seconds, "seconds")
 
   def task(self, name, sync=True, **meta):
     task = Task.begin(name=name, meta=meta, sync=sync)
@@ -104,4 +112,3 @@ class Timer:
 
   def plot(self):
     plot_timeline(self.tasks)
-
diff --git a/yann/viz/__init__.py b/yann/viz/__init__.py
index fbfa416..02f47f3 100644
--- a/yann/viz/__init__.py
+++ b/yann/viz/__init__.py
@@ -1,42 +1,49 @@
-from .plot import plot_line, plot_pred_scores, plot_rocs, \
-  plot_confusion_matrix, plot_cooccurrences
 from .image import show_images
-
+from .plot import (
+  plot_confusion_matrix,
+  plot_cooccurrences,
+  plot_line,
+  plot_pred_scores,
+  plot_rocs,
+)
 
 
 class Plotter:
   def __call__(self, *args, **kwargs):
     pass
 
+
 class Shower:
   def __call__(self, x, format=None, **kwargs):
     if isinstance(x, (list, tuple)):
       if isinstance(x[0], str):
-        if x[0].lower().endswith(('.jpg', '.jpeg', '.png')):
+        if x[0].lower().endswith((".jpg", ".jpeg", ".png")):
           return self.images(x, **kwargs)
 
       try:
         from PIL import Image
+
         if isinstance(x[0], Image.Image):
           return self.images(x, **kwargs)
       except:
         pass
     if isinstance(x, str):
-      if x.lower().endswith(('.jpg', '.jpeg', '.png')):
+      if x.lower().endswith((".jpg", ".jpeg", ".png")):
         return self.images(x, **kwargs)
 
     try:
       from PIL import Image
+
       if isinstance(x, Image.Image):
         return self.images(x, **kwargs)
     except:
       pass
 
     import torch
+
     if isinstance(x, torch.Tensor):
       return self.tensor(x)
 
-
     return x
 
   def images(self, *args, **kwargs):
@@ -44,7 +51,9 @@ class Shower:
 
   def tensor(self, t, *args, **kwargs):
     from .html import tensor
+
     return tensor(t, *args, **kwargs).display()
 
+
 show = Shower()
-plot = Plotter()
\ No newline at end of file
+plot = Plotter()
diff --git a/yann/viz/activations.py b/yann/viz/activations.py
index 1cdae0f..efa8ab4 100644
--- a/yann/viz/activations.py
+++ b/yann/viz/activations.py
@@ -1,18 +1,23 @@
 from scipy.misc import imresize
 
 
-
-def draw_mask(img, mask, blend=.5, cmap=None, interp='cubic'):
+def draw_mask(img, mask, blend=0.5, cmap=None, interp="cubic"):
   if not cmap:
     import matplotlib.pylab as plt
-    cmap = plt.get_cmap('jet')
+
+    cmap = plt.get_cmap("jet")
   if isinstance(cmap, str):
     import matplotlib.pylab as plt
+
     cmap = plt.get_cmap(cmap)
 
   if mask.shape[:2] != img.shape[:2]:
     mask = imresize(mask, img.shape[:2], interp=interp)
-  return (cmap(mask)[:,:,:3] * 255 * blend + img * (1-blend)).round().astype('uint8')
+  return (
+    (cmap(mask)[:, :, :3] * 255 * blend + img * (1 - blend))
+    .round()
+    .astype("uint8")
+  )
 
 
 def class_activation_maps(features, weights, classes=None, normalize=True):
@@ -25,8 +30,8 @@ def class_activation_maps(features, weights, classes=None, normalize=True):
     class_maps = {}
     for c in classes:
       blended_channels = (
-          weights[c] @ sample.reshape(num_channels, rows * cols)).reshape(
-        rows, cols)
+        weights[c] @ sample.reshape(num_channels, rows * cols)
+      ).reshape(rows, cols)
       if normalize:
         x = blended_channels - blended_channels.min()
         x = x / x.max() * 255
@@ -54,4 +59,4 @@ def class_activation_maps(features, weights, classes=None, normalize=True):
 #     pass
 #
 #   def show(self):
-#     pass
\ No newline at end of file
+#     pass
diff --git a/yann/viz/html.py b/yann/viz/html.py
index 457f8e9..dd75b3e 100644
--- a/yann/viz/html.py
+++ b/yann/viz/html.py
@@ -1,5 +1,6 @@
 from abc import ABCMeta
 
+
 class styles(dict):
   def __init__(self, *args, **props):
     super().__init__()
@@ -7,18 +8,17 @@ class styles(dict):
       if isinstance(args[0], dict):
         self.update(args[0])
     if isinstance(args[0], str):
-      for l in args[0].split(';'):
+      for l in args[0].split(";"):
         l = l.strip()
         if l:
-          k, v = l.split(':')
+          k, v = l.split(":")
 
-          self[k.replace('-', '_').strip()] = v.strip()
+          self[k.replace("-", "_").strip()] = v.strip()
 
     self.update(props)
 
   def __str__(self):
-    return ' '.join(f"{k.replace('_', '-')}: {v};"
-                    for k, v in self.items())
+    return " ".join(f"{k.replace('_', '-')}: {v};" for k, v in self.items())
 
 
 class Node:
@@ -45,13 +45,15 @@ class Node:
   def html(self):
     if self.CHILDREN:
       return f"""
-         <{self.name} {' '.join(
-        f'{k}="{v}"' for k, v in self.props.items())}  style="{self.style}"/>
+         <{self.name} {
+        " ".join(f'{k}="{v}"' for k, v in self.props.items())
+      }  style="{self.style}"/>
       """
 
     return f"""
-    <{self.name} {' '.join(
-      f'{k}="{v}"' for k, v in self.props.items())} style="{self.style}">
+    <{self.name} {
+      " ".join(f'{k}="{v}"' for k, v in self.props.items())
+    } style="{self.style}">
        {self.format_children()}
     </{self.name}>
     """
@@ -62,13 +64,15 @@ class Node:
 
   def render(self):
     from IPython.core.display import HTML
+
     return HTML(self.html())
 
   def format_children(self):
-    return ' '.join(str(c) for c in self.children)
+    return " ".join(str(c) for c in self.children)
 
   def display(self):
     from IPython.core.display import display
+
     self._display_handle = display(self.render(), display_id=True)
 
   def update(self):
@@ -78,10 +82,10 @@ class Node:
 
 def prop(name):
   def g(self):
-    return getattr(self, f'_{name}')
+    return getattr(self, f"_{name}")
 
   def s(self, val):
-    setattr(self, f'_{name}', val)
+    setattr(self, f"_{name}", val)
     self.update()
 
   return property(g, s)
@@ -89,7 +93,7 @@ def prop(name):
 
 class ReactiveNodeMeta(ABCMeta):
   def __new__(mcls, name, bases, namespace):
-    annotations = namespace.get('__annotations__', {})
+    annotations = namespace.get("__annotations__", {})
 
     props = set()
     prop_defaults = {}
@@ -99,18 +103,19 @@ class ReactiveNodeMeta(ABCMeta):
           prop_defaults[name] = namespace[name]
         namespace[name] = prop(name)
         props.add(name)
-    if '_props' in namespace:
-      namespace['_props'].update(props)
+    if "_props" in namespace:
+      namespace["_props"].update(props)
     else:
-      namespace['_props'] = props
+      namespace["_props"] = props
 
-    if '_default_props' in namespace:
-      namespace['_default_props'].update(prop_defaults)
+    if "_default_props" in namespace:
+      namespace["_default_props"].update(prop_defaults)
     else:
-      namespace['_default_props'] = prop_defaults
+      namespace["_default_props"] = prop_defaults
 
     return super().__new__(mcls, name, bases, namespace)
 
+
 class ReactiveMixin(metaclass=ReactiveNodeMeta):
   _props: set
   _default_props: dict
@@ -131,23 +136,44 @@ class EmptyNode(Node):
   CHILDREN = True
 
 
-class div(Node): pass
+class div(Node):
+  pass
+
+
+class span(Node):
+  pass
+
+
+class img(EmptyNode):
+  pass
+
+
+class p(Node):
+  pass
+
 
+class h1(Node):
+  pass
 
-class span(Node): pass
 
+class h2(Node):
+  pass
 
-class img(EmptyNode): pass
-class p(Node): pass
-class h1(Node): pass
-class h2(Node): pass
-class h3(Node): pass
-class h4(Node): pass
-class progress(EmptyNode): pass
+
+class h3(Node):
+  pass
+
+
+class h4(Node):
+  pass
+
+
+class progress(EmptyNode):
+  pass
 
 
 class matplotfig(img):
-  NAME = 'img'
+  NAME = "img"
 
   def __init__(self, figure, live=False, style=None, **props):
     super(matplotfig, self).__init__(style=style, **props)
@@ -157,64 +183,73 @@ class matplotfig(img):
 
     if not self.live:
       from .plot import figure_to_base64
-      self.props['src'] = figure_to_base64(figure, data_encoded=True)
 
+      self.props["src"] = figure_to_base64(figure, data_encoded=True)
 
   def html(self):
     if self.live:
       from .plot import figure_to_base64
-      self.props['src'] = figure_to_base64(self.figure, data_encoded=True)
-    if 'src' not in self.props:
+
+      self.props["src"] = figure_to_base64(self.figure, data_encoded=True)
+    if "src" not in self.props:
       from .plot import figure_to_base64
-      self.props['src'] = figure_to_base64(self.figure, data_encoded=True)
 
-    return super(matplotfig, self).html()
+      self.props["src"] = figure_to_base64(self.figure, data_encoded=True)
 
+    return super(matplotfig, self).html()
 
 
 def _cell(val, size=25):
   return div(
     style=(
-      f'width: {size}px; height: {size}px;'
-      f' display: inline-block;'
-      f' background-color: rgba({50 + val}, {20 + val * .8}, {80 + val * .6}, 1);'
-      f' margin: 0; border: 1px solid rgba(0,0,0, .05)'
-    ))
+      f"width: {size}px; height: {size}px;"
+      f" display: inline-block;"
+      f" background-color: rgba({50 + val}, {20 + val * 0.8}, {80 + val * 0.6}, 1);"
+      f" margin: 0; border: 1px solid rgba(0,0,0, .05)"
+    )
+  )
 
 
 def _row(*args):
-  return div(*args, style='margin: 0; padding: 0; font-size:0; white-space: nowrap;')
+  return div(
+    *args, style="margin: 0; padding: 0; font-size:0; white-space: nowrap;"
+  )
 
 
-def tensor(t, cell_size=15, scaled=None, min=None, max=None, max_elements=50000):
+def tensor(
+  t, cell_size=15, scaled=None, min=None, max=None, max_elements=50000
+):
   if t.numel() > max_elements:
-    raise ValueError('tensor too large')
+    raise ValueError("tensor too large")
   if scaled is None:
     max = t.max() if max is None else max
     min = t.min() if min is None else min
-    scaled = (t.float() - min) / (max-min) * 255
+    scaled = (t.float() - min) / (max - min) * 255
 
   if t.ndim == 1:
     return div(
       f"shape: {tuple(t.shape)}",
-      _row(*(_cell(v, size=cell_size) for v in scaled))
+      _row(*(_cell(v, size=cell_size) for v in scaled)),
     )
 
   if t.ndim == 2:
     return div(
       f"shape: {tuple(t.shape)}",
-      *[
-        _row(*(_cell(v, size=cell_size) for v in row)) for row in scaled
-      ]
+      *[_row(*(_cell(v, size=cell_size) for v in row)) for row in scaled],
     )
 
   if t.ndim >= 3:
     return div(
       f"shape: {tuple(t.shape)}",
       div(
-        *(div(style='border: 1px solid #CCC; padding: 5px; margin: 3px; display: inline-block; border-radius: 6px')(
-          f"index: {n}", tensor(x, scaled=x, cell_size=cell_size, min=min, max=max)
-        ) for n, x in enumerate(scaled)
+        *(
+          div(
+            style="border: 1px solid #CCC; padding: 5px; margin: 3px; display: inline-block; border-radius: 6px"
+          )(
+            f"index: {n}",
+            tensor(x, scaled=x, cell_size=cell_size, min=min, max=max),
+          )
+          for n, x in enumerate(scaled)
         )
-      )
-    )
\ No newline at end of file
+      ),
+    )
diff --git a/yann/viz/image.py b/yann/viz/image.py
index ef911ed..501846a 100644
--- a/yann/viz/image.py
+++ b/yann/viz/image.py
@@ -1,25 +1,27 @@
+import base64
 from io import BytesIO
-from PIL import Image
 from itertools import zip_longest
-import base64
+
+from PIL import Image
 
 
 def base64str(img):
   s = BytesIO()
-  img.save(s, format='JPEG')
-  return (
-      "data:image/jpeg;base64,"
-      + base64.b64encode(s.getvalue()).decode("utf-8")
+  img.save(s, format="JPEG")
+  return "data:image/jpeg;base64," + base64.b64encode(s.getvalue()).decode(
+    "utf-8"
   )
 
 
 def show_images(paths, labels=None, urls=None, w=400, h=400):
-  from IPython.core.display import display, HTML
   from pathlib import Path
 
+  from IPython.core.display import HTML, display
+
   if isinstance(paths, (str, Path)):
-    if '*' in paths:
+    if "*" in paths:
       from glob import glob
+
       paths = glob(paths)
     else:
       paths = [paths]
@@ -32,10 +34,13 @@ def show_images(paths, labels=None, urls=None, w=400, h=400):
   else:
     items = zip_longest(paths, labels or [])
 
-
   tags = []
   for x, l in items:
-    if urls or (urls is not False and isinstance(x, (str, Path)) and str(x).startswith('http')):
+    if urls or (
+      urls is not False
+      and isinstance(x, (str, Path))
+      and str(x).startswith("http")
+    ):
       src = str(x)
     else:
       img = Image.open(x) if isinstance(x, str) else x
@@ -44,23 +49,23 @@ def show_images(paths, labels=None, urls=None, w=400, h=400):
 
     if l:
       tags.append(
-        f'''
+        f"""
         <div style="display: inline-block; padding: 3px">
           <img 
             style="max-width: {w}px; max-height: {h}px; margin: 3px;"
              src={src} />
-          <p>{l if isinstance(l, str) else ', '.join(l)}</p>
+          <p>{l if isinstance(l, str) else ", ".join(l)}</p>
         </div>
-        '''
+        """
       )
     else:
       tags.append(
-        f'''
+        f"""
         <div style="display: inline-block; padding: 3px">
           <img 
             style="max-width: {w}px; max-height: {h}px; margin: 3px;"
              src={src} />
         </div>
-        '''
+        """
       )
-  return display(HTML(f'<div>{"".join(tags)}</div>'))
\ No newline at end of file
+  return display(HTML(f"<div>{''.join(tags)}</div>"))
diff --git a/yann/viz/plot.py b/yann/viz/plot.py
index fcd0f70..eba6354 100644
--- a/yann/viz/plot.py
+++ b/yann/viz/plot.py
@@ -1,42 +1,39 @@
-from matplotlib import pylab as plt
-import numpy as np
+import base64
 import datetime
+import io
+import itertools
 import pathlib
-from sklearn.metrics import roc_curve, auc, confusion_matrix
-import matplotlib.dates as mdates
-from matplotlib.collections import PolyCollection
 from collections import OrderedDict
-
-import io
-import base64
 from urllib.parse import quote
 
-import itertools
+import numpy as np
 
 from .. import to_numpy
 from ..metrics import moving_average
 
 
 def plot_line(
-    y,
-    x=None,
-    figsize=None,
-    window=1,
-    xlim=None,
-    ylim=None,
-    line_width=2,
-    stroke='-',
-    title=None,
-    xlabel=None,
-    ylabel=None,
-    xscale=None,
-    yscale=None,
-    show=True,
-    save=False,
-    legend=True,
-    name=None,
-    grid=True
+  y,
+  x=None,
+  figsize=None,
+  window=1,
+  xlim=None,
+  ylim=None,
+  line_width=2,
+  stroke="-",
+  title=None,
+  xlabel=None,
+  ylabel=None,
+  xscale=None,
+  yscale=None,
+  show=True,
+  save=False,
+  legend=True,
+  name=None,
+  grid=True,
 ):
+  from matplotlib import pylab as plt
+
   fig = figsize and plt.figure(figsize=figsize)
 
   if xlim:
@@ -70,7 +67,7 @@ def plot_line(
     plt.grid(True)
 
   if legend:
-    plt.legend(loc='best')
+    plt.legend(loc="best")
 
   if save and show:
     raise ValueError("Can't show and save at the same time")
@@ -78,8 +75,9 @@ def plot_line(
     plt.show()
   if save:
     plt.gcf().savefig(
-      save if isinstance(save, (str, pathlib.Path)) else
-      f"{title or ylabel or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg"
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f"{title or ylabel or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg"
     )
     plt.gcf().clear()
     if fig:
@@ -87,68 +85,108 @@ def plot_line(
 
   return plt.gcf()
 
+
 def plot_pred_scores(
-    preds,
-    targets,
-    classes=None,
-    logscale=True,
-    figsize=(12, 6),
-    show=True,
-    save=False,
-    title=None):
+  preds,
+  targets,
+  classes=None,
+  logscale=True,
+  figsize=(12, 6),
+  show=True,
+  save=False,
+  title=None,
+):
   import seaborn as sns
+  from matplotlib import pylab as plt
 
   preds, targets = to_numpy(preds), to_numpy(targets)
 
   if title:
     plt.title(title)
 
-
   if classes:
-    classes = classes.items() if isinstance(classes, dict) else \
-      ((c, n) for n, c in enumerate(classes))
+    classes = (
+      classes.items()
+      if isinstance(classes, dict)
+      else ((c, n) for n, c in enumerate(classes))
+    )
   else:
     classes = ((n, n) for n in range(preds.shape[1]))
 
-
   for cls, idx in classes:
     f, ax = plt.subplots(figsize=figsize)
     if logscale:
       ax.set(yscale="log")
 
     if len(targets.shape) == 1:
-      sns.distplot(preds[targets != idx, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0, 1]}, ax=ax, color='red',
-                   label='Negative')
-      sns.distplot(preds[targets == idx, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0, 1]}, ax=ax, color='blue',
-                   label='Positive')
+      sns.distplot(
+        preds[targets != idx, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={"range": [0, 1]},
+        ax=ax,
+        color="red",
+        label="Negative",
+      )
+      sns.distplot(
+        preds[targets == idx, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={"range": [0, 1]},
+        ax=ax,
+        color="blue",
+        label="Positive",
+      )
     else:
-      sns.distplot(preds[targets[:, idx] == 0, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0,1]}, ax=ax, color='red', label='Negative')
-      sns.distplot(preds[targets[:, idx] == 1, idx], bins=50, kde=False,
-                   rug=False, hist_kws={"range": [0,1]}, ax=ax, color='blue', label='Positive')
+      sns.distplot(
+        preds[targets[:, idx] == 0, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={"range": [0, 1]},
+        ax=ax,
+        color="red",
+        label="Negative",
+      )
+      sns.distplot(
+        preds[targets[:, idx] == 1, idx],
+        bins=50,
+        kde=False,
+        rug=False,
+        hist_kws={"range": [0, 1]},
+        ax=ax,
+        color="blue",
+        label="Positive",
+      )
     ax.set_title(cls)
-    plt.xlabel('Score')
-    plt.ylabel('Sample Count')
-    plt.legend(loc='best')
+    plt.xlabel("Score")
+    plt.ylabel("Sample Count")
+    plt.legend(loc="best")
 
   if show:
     plt.show()
   if save:
-    plt.savefig(save if isinstance(save, (str, pathlib.Path)) else
-                  f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg")
+    plt.savefig(
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg"
+    )
     plt.gcf().clear()
 
 
 def plot_rocs(
-    preds,
-    targets,
-    classes=None,
-    figsize=(12,12),
-    show=True,
-    save=False,
-    title=None):
+  preds,
+  targets,
+  classes=None,
+  figsize=(12, 12),
+  show=True,
+  save=False,
+  title=None,
+):
+  from matplotlib import pylab as plt
+  from sklearn.metrics import auc, roc_curve
 
   preds, targets = to_numpy(preds), to_numpy(targets)
 
@@ -157,17 +195,19 @@ def plot_rocs(
     plt.title(title)
 
   if classes:
-    classes = classes.items() if isinstance(classes, dict) else \
-      ((c, n) for n, c in enumerate(classes))
+    classes = (
+      classes.items()
+      if isinstance(classes, dict)
+      else ((c, n) for n, c in enumerate(classes))
+    )
   else:
     classes = ((n, n) for n in range(preds.shape[1]))
 
   plt.xlim([0.0, 1.02])
   plt.ylim([0.0, 1.02])
-  plt.xlabel('False Positive Rate')
-  plt.ylabel('True Positive Rate')
-  plt.plot([0, 1], [0, 1], 'k--', lw=2)
-
+  plt.xlabel("False Positive Rate")
+  plt.ylabel("True Positive Rate")
+  plt.plot([0, 1], [0, 1], "k--", lw=2)
 
   vals = {}
   for cls, idx in classes:
@@ -176,42 +216,38 @@ def plot_rocs(
     else:
       fpr, tpr, thresholds = roc_curve(targets[:, idx], preds[:, idx])
     area = auc(fpr, tpr)
-    plt.plot(fpr, tpr, label=f'{cls} (AUC = %0.3f)' % area)
+    plt.plot(fpr, tpr, label=f"{cls} (AUC = %0.3f)" % area)
 
     vals[cls] = (area, fpr, tpr, thresholds)
-  plt.legend(loc='best')
-
+  plt.legend(loc="best")
 
   if show:
     plt.show()
   if save:
-    plt.savefig(save if isinstance(save, (str, pathlib.Path)) else
-                f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg")
+    plt.savefig(
+      save
+      if isinstance(save, (str, pathlib.Path))
+      else f"{title or datetime.datetime.utcnow().strftime('%y-%m-%dT%H%M%S')}.jpg"
+    )
     plt.gcf().clear()
 
   return vals
 
 
 def plot_cooccurrences(counts, classes, figsize=(14, 11)):
-  import seaborn as sns
   import pandas as pd
+  import seaborn as sns
+  from matplotlib import pylab as plt
 
   mask = np.ones_like(counts)
   mask[np.tril_indices_from(mask)] = False
 
-  df_cm = pd.DataFrame(
-    counts,
-    index=list(classes),
-    columns=list(classes))
+  df_cm = pd.DataFrame(counts, index=list(classes), columns=list(classes))
   plt.figure(figsize=figsize)
   plot = sns.heatmap(
-    df_cm,
-    robust=True,
-    annot=True,
-    fmt="d",
-    cmap="YlGnBu",
-    mask=mask)
-  plot.set_title('Class Co-occurrence')
+    df_cm, robust=True, annot=True, fmt="d", cmap="YlGnBu", mask=mask
+  )
+  plot.set_title("Class Co-occurrence")
 
 
 def sorted_indices(matrix, desc=False):
@@ -219,7 +255,7 @@ def sorted_indices(matrix, desc=False):
   return (np.fliplr(inds) if desc else inds)[0]
 
 
-def truncate_confusion_matrix(matrix, thresh=.2, top=None, symmetric=True):
+def truncate_confusion_matrix(matrix, thresh=0.2, top=None, symmetric=True):
   inds = sorted_indices(matrix, desc=True)
   inds = [(r, c) for r, c in inds if r != c]
 
@@ -240,18 +276,29 @@ def truncate_confusion_matrix(matrix, thresh=.2, top=None, symmetric=True):
 
 
 def plot_confusion_matrix(
-    preds, targets, classes, figsize=(16, 16),
-    thresh=None, top=None, normalize=False, symmetric=True):
+  preds,
+  targets,
+  classes,
+  figsize=(16, 16),
+  thresh=None,
+  top=None,
+  normalize=False,
+  symmetric=True,
+):
+  from matplotlib import pylab as plt
+  from sklearn.metrics import confusion_matrix
+
   preds, targets = to_numpy(preds), to_numpy(targets)
 
   cm = confusion_matrix(targets, preds)
 
   if normalize:
-    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
+    cm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]
 
   if thresh or top:
     cm, rows, cols = truncate_confusion_matrix(
-      cm, thresh=thresh, top=top, symmetric=symmetric)
+      cm, thresh=thresh, top=top, symmetric=symmetric
+    )
   else:
     rows, cols = list(range(len(classes))), list(range(len(classes)))
 
@@ -259,39 +306,49 @@ def plot_confusion_matrix(
     return cm
 
   fig = plt.figure(figsize=figsize)
-  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
-  plt.title('Confusion matrix')
+  plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
+  plt.title("Confusion matrix")
   plt.colorbar()
   plt.xticks(np.arange(len(cols)), [classes[i] for i in cols], rotation=75)
   plt.yticks(np.arange(len(rows)), [classes[i] for i in rows])
 
-  fmt = '.2f' if normalize else 'd'
-  th = cm.max() / 2.
+  fmt = ".2f" if normalize else "d"
+  th = cm.max() / 2.0
   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
-    plt.text(j, i, format(cm[i, j], fmt),
-             horizontalalignment="center",
-             color="white" if cm[i, j] > th else "black")
+    plt.text(
+      j,
+      i,
+      format(cm[i, j], fmt),
+      horizontalalignment="center",
+      color="white" if cm[i, j] > th else "black",
+    )
 
   plt.tight_layout()
-  plt.ylabel('True label')
-  plt.xlabel('Predicted label')
+  plt.ylabel("True label")
+  plt.xlabel("Predicted label")
   plt.show()
 
   return cm
 
 
 def plot_timeline(tasks, figsize=None):
+  import matplotlib.dates as mdates
+  from matplotlib import pylab as plt
+  from matplotlib.collections import PolyCollection
+
   inds = OrderedDict((c, n) for n, c in enumerate(set(t.name for t in tasks)))
   color_map = {c: f"C{n}" for c, n in inds.items()}
 
   verts = []
   colors = []
   for task in tasks:
-    v = [(mdates.date2num(task.start_time), inds[task.name] - .4),
-         (mdates.date2num(task.start_time), inds[task.name] + .4),
-         (mdates.date2num(task.end_time), inds[task.name] + .4),
-         (mdates.date2num(task.end_time), inds[task.name] - .4),
-         (mdates.date2num(task.start_time), inds[task.name] - .4)]
+    v = [
+      (mdates.date2num(task.start_time), inds[task.name] - 0.4),
+      (mdates.date2num(task.start_time), inds[task.name] + 0.4),
+      (mdates.date2num(task.end_time), inds[task.name] + 0.4),
+      (mdates.date2num(task.end_time), inds[task.name] - 0.4),
+      (mdates.date2num(task.start_time), inds[task.name] - 0.4),
+    ]
     verts.append(v)
     colors.append(color_map[task.name])
 
@@ -312,8 +369,9 @@ def plot_timeline(tasks, figsize=None):
   plt.show()
 
 
+def figure_to_base64(fig, format="png", data_encoded=True, close=False):
+  from matplotlib import pylab as plt
 
-def figure_to_base64(fig, format='png', data_encoded=True, close=False):
   buf = io.BytesIO()
   fig.savefig(buf, format=format)
   buf.seek(0)
diff --git a/yann/viz/viewer.py b/yann/viz/viewer.py
index 9fc57dd..fd0e500 100644
--- a/yann/viz/viewer.py
+++ b/yann/viz/viewer.py
@@ -15,4 +15,4 @@ class PandasViewer(Viewer):
 
 
 class DatasetViewer(Viewer):
-  pass
\ No newline at end of file
+  pass
diff --git a/yann/viz/widgets.py b/yann/viz/widgets.py
index 948e68a..2e65996 100644
--- a/yann/viz/widgets.py
+++ b/yann/viz/widgets.py
@@ -1,11 +1,11 @@
-from .html import div, prop, ReactiveMixin, Node
+from .html import Node, ReactiveMixin, div, prop
 
 
 class ProgressBar(ReactiveMixin, Node):
   value: prop = 0
   max: prop = 100
-  color: prop = 'lightgreen'
-  background: prop = 'lightgray'
+  color: prop = "lightgreen"
+  background: prop = "lightgray"
 
   def html(self):
     return (
@@ -19,9 +19,7 @@ class ProgressBar(ReactiveMixin, Node):
         padding: 5px 10px;
         """
       )(
-        div(style='position:relative; z-index: 1;')(
-          *self.children
-        ),
+        div(style="position:relative; z-index: 1;")(*self.children),
         div(
           style=f"""
           background-color: {self.color};
@@ -31,6 +29,7 @@ class ProgressBar(ReactiveMixin, Node):
           left: 0;
           right: {100 - self.value / self.max * 100}%;
           z-index: 0;
-        """)
-        )
-    ).html()
\ No newline at end of file
+        """
+        ),
+      )
+    ).html()