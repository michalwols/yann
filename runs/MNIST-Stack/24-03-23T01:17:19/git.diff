diff --git a/examples/train_mnist.py b/examples/train_mnist.py
index 26f6d94..8218a26 100644
--- a/examples/train_mnist.py
+++ b/examples/train_mnist.py
@@ -18,61 +18,64 @@ class Params(HyperParams):
 
   seed = 1
 
-# parse command line arguments
-params = Params.from_command()
-params.validate()
-
-print(params)
-
-# set random, numpy and pytorch seeds in one call
-yann.seed(params.seed)
-
-
-lenet = Stack(
-  Infer(nn.Conv2d, 10, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
-
-  Infer(nn.Conv2d, 20, kernel_size=5),
-  nn.MaxPool2d(2),
-  nn.ReLU(inplace=True),
-
-  Flatten(),
-
-  Infer(nn.Linear, 50),
-  nn.ReLU(inplace=True),
-  Infer(nn.Linear, 10),
-  activation=nn.LogSoftmax(dim=1)
-)
-
-# run a forward pass to infer input shapes using `Infer` modules
-lenet(torch.rand(1, 1, 28, 28))
-
-# use the registry to resolve optimizer name to an optimizer class
-optimizer = yann.resolve.optimizer(
-  params.optimizer,
-  yann.trainable(lenet.parameters()),
-  momentum=params.momentum,
-  lr=params.learning_rate
-)
-
-train = Trainer(
-  model=lenet,
-  optimizer=optimizer,
-  dataset=params.dataset,
-  batch_size=params.batch_size,
-  transform=transforms.Compose([
-    transforms.ToTensor(),
-    transforms.Normalize((0.1307,), (0.3081,))
-  ]),
-  loss='nll_loss',
-  metrics=('accuracy',)
-)
-
-train(params.epochs)
-
-# save checkpoint
-train.checkpoint()
-
-# plot the loss curve
-train.history.plot()
\ No newline at end of file
+if __name__ == '__main__':
+  # parse command line arguments
+  params = Params.from_command()
+  params.validate()
+
+  print(params)
+
+  # set random, numpy and pytorch seeds in one call
+  yann.seed(params.seed)
+
+
+  lenet = Stack(
+    Infer(nn.Conv2d, 10, kernel_size=5),
+    nn.MaxPool2d(2),
+    nn.ReLU(inplace=True),
+
+    Infer(nn.Conv2d, 20, kernel_size=5),
+    nn.MaxPool2d(2),
+    nn.ReLU(inplace=True),
+
+    Flatten(),
+
+    Infer(nn.Linear, 50),
+    nn.ReLU(inplace=True),
+    Infer(nn.Linear, 10),
+    activation=nn.LogSoftmax(dim=1)
+  )
+
+  # run a forward pass to infer input shapes using `Infer` modules
+  lenet(torch.rand(1, 1, 28, 28))
+
+  # use the registry to resolve optimizer name to an optimizer class
+  optimizer = yann.resolve.optimizer(
+    params.optimizer,
+    yann.trainable(lenet.parameters()),
+    momentum=params.momentum,
+    lr=params.learning_rate
+  )
+
+  train = Trainer(
+    model=lenet,
+    optimizer=optimizer,
+    dataset=params.dataset,
+    batch_size=params.batch_size,
+    transform=transforms.Compose([
+      transforms.ToTensor(),
+      transforms.Normalize((0.1307,), (0.3081,))
+    ]),
+    loss='nll_loss',
+    metrics=('accuracy',),
+    callbacks=True,
+    amp=False
+  )
+
+  train(params.epochs)
+
+  # save checkpoint
+  train.checkpoint()
+
+  # plot the loss curve
+  train.history.plot()
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index 56d239d..10795dc 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -4,5 +4,5 @@ scipy
 matplotlib
 requests
 scikit-learn
-torch==1.2.0
-torchvision==0.4.0
+torch
+torchvision
diff --git a/yann/callbacks/logging.py b/yann/callbacks/logging.py
index 4532c58..67ad419 100644
--- a/yann/callbacks/logging.py
+++ b/yann/callbacks/logging.py
@@ -2,7 +2,7 @@ import sys
 import logging
 
 from .base import Callback
-
+from yann.utils.tensor import describe
 
 class Logger(Callback):
   dist_placement = 0
@@ -35,11 +35,11 @@ class Logger(Callback):
     if index % self.batch_freq == 0:
       if not self.logged_batch_shapes:
         try:
-          self.log("\nBatch inputs shape:", tuple(inputs.size()),
-                   "\nBatch targets shape:", tuple(targets.size()),
-                   "\nBatch outputs shape:", tuple(outputs.size()), '\n')
-        except:
-          pass
+          self.log("\ninputs:", describe(inputs),
+                   "\ntargets:", describe(targets),
+                   "\noutputs:", describe(outputs), '\n')
+        except Exception as e:
+          raise e
         self.logged_batch_shapes = True
 
       if self.batch_string:
diff --git a/yann/init.py b/yann/init.py
index 31d88b1..a1f2615 100644
--- a/yann/init.py
+++ b/yann/init.py
@@ -1,6 +1,6 @@
 from torch import nn
 from torch.nn import init
-
+import math
 
 def kaiming(model: nn.Module):
   for module in model.modules():
@@ -20,3 +20,8 @@ def kaiming(model: nn.Module):
 
 
 msr = kaiming
+
+
+def linear_zero_bias(linear: nn.Module, num_classes):
+  init.zeros_(linear.weight)
+  init.constant_(linear.bias, -math.log(num_classes))
\ No newline at end of file
diff --git a/yann/train/trainer.py b/yann/train/trainer.py
index 82028df..3326f68 100644
--- a/yann/train/trainer.py
+++ b/yann/train/trainer.py
@@ -8,6 +8,7 @@ from typing import Optional, Callable, Union, Dict, Mapping, Sequence
 
 import torch
 import torch.nn
+from torch import autocast
 from torch.cuda.amp import autocast, GradScaler
 from torch.optim.optimizer import Optimizer
 from torch.utils.data import Sampler, DataLoader
@@ -1177,7 +1178,7 @@ DATASET
 VALIDATION DATASET
 =======
 
-{self.val_loader.dataset}
+{self.val_loader.dataset if self.val_loader is not None else None}
 
 LOADER
 ======
diff --git a/yann/typedefs.py b/yann/typedefs.py
index ffab150..03bf79c 100644
--- a/yann/typedefs.py
+++ b/yann/typedefs.py
@@ -12,7 +12,6 @@ LogProbabilities = torch.FloatTensor
 Batch = typing.NewType('Batch', torch.Tensor)
 
 ClassIndices = torch.LongTensor
-OneHot = torch.FloatTensor
 MultiLabelOneHot = OneHot
 
 ImageTensor = torch.ShortTensor
diff --git a/yann/utils/tensor.py b/yann/utils/tensor.py
index 5f78b86..9729ecc 100644
--- a/yann/utils/tensor.py
+++ b/yann/utils/tensor.py
@@ -24,7 +24,7 @@ def one_hot(targets: torch.Tensor, num_classes=None, device=None, dtype=None, no
 
 
 def show_hist(hist):
-  chars = '_▁▂▃▄▅▆▇█'
+  chars = ' ▁▂▃▄▅▆▇█'
   top = max(hist)
   step = (top / float(len(chars) - 1)) or 1
   return ''.join(chars[int(round(count / step))] for count in hist)
@@ -33,7 +33,7 @@ def show_hist(hist):
 def describe(tensor: torch.Tensor, bins=10) -> str:
   try:
     stats = (
-      f"μ={tensor.mean():.4f}  σ={tensor.std():.4f}\n"
+      f"mean: {tensor.mean():.4f}  std: {tensor.std():.4f} "
     )
   except:
     stats = ''
@@ -46,14 +46,11 @@ def describe(tensor: torch.Tensor, bins=10) -> str:
     )
   except:
     hist = ''
-  return (
-
-    f"{tuple(tensor.shape)} "
-    f"{tensor.dtype} "
-    f"{tensor.device}"
-    f"{' grad' if tensor.requires_grad else ''} "
-    f"({tensor.numel() * tensor.element_size() / (1e6):,.5f} MB)\n"
-    f"{stats}"
-    f"{hist}"
-    f"min: {tensor.min():.4f}  max: {tensor.max():.4f}  sum: {tensor.sum():.4f}\n\n"
-  )
\ No newline at end of file
+  return f"""
+shape: {tuple(tensor.shape)} dtype: {tensor.dtype} device: {tensor.device} grad: {tensor.requires_grad}
+size: {tensor.numel() * tensor.element_size() / (1e6):,.5f} MB
+{stats}min: {tensor.min():.4f}  max: {tensor.max():.4f}  sum: {tensor.sum():.4f}
+{hist}
+
+{tensor}
+  """
\ No newline at end of file